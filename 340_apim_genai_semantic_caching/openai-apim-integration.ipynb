{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Management Semantic Caching for LLMs\n",
    "\n",
    "The azure-openai-semantic-cache-lookup policy conducts a cache lookup of responses on Azure OpenAI Chat Completion API and Completion API requests from a pre-configured external cache. It operates by comparing the vector proximity of the prompt to prior requests and using a specific similarity score threshold. Caching responses helps reduce bandwidth and processing demands on the backend Azure OpenAI API, thus reducing latency perceived by API consumers.\n",
    "\n",
    "![](images/architecture.png)\n",
    "\n",
    "[View policy configuration](policy.xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 1. Create deployment using Terraform\n",
    "\n",
    "This lab uses Terraform to declaratively define all the resources that will be deployed. Change the [variables.tf](variables.tf) directly to try different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! terraform init\n",
    "! terraform apply -auto-approve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following resources will be created:\n",
    "\n",
    "![](images/resources.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 2. Get the deployment outputs\n",
    "\n",
    "We are now at the stage where we only need to retrieve the gateway URL and the subscription before we are ready for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apim_resource_gateway_url = ! terraform output -raw apim_resource_gateway_url\n",
    "apim_resource_gateway_url = apim_resource_gateway_url.n\n",
    "print(\"üëâüèª APIM Resource Gateway URL: \", apim_resource_gateway_url)\n",
    "\n",
    "apim_subscription_key = ! terraform output -raw apim_subscription_key\n",
    "apim_subscription_key = apim_subscription_key.n\n",
    "print(\"üëâüèª APIM Subscription Key: \", apim_subscription_key)\n",
    "\n",
    "aca_redis_url = ! terraform output -raw aca_redis_url\n",
    "aca_redis_url = aca_redis_url.n\n",
    "print(\"üëâüèª ACA Redis URL: \", aca_redis_url)\n",
    "\n",
    "openai_api_version = \"2024-10-21\"\n",
    "openai_model_name = \"gpt-4o\"\n",
    "openai_deployment_name = \"gpt-4o\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download required modules to test redis cache instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis\n",
    "\n",
    "client = redis.Redis(\n",
    "    host=aca_redis_url, \n",
    "    port=6379, \n",
    "    # password='@Aa123456789', \n",
    "    decode_responses=True)\n",
    "\n",
    "print(\"Ping Redis: \", client.ping())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### üß™ Make multiple calls using the Azure OpenAI Python SDK\n",
    "\n",
    "The code below contains a list of questions that will be randomly selected and sent as prompts to the OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import time, random\n",
    "\n",
    "runs = 10\n",
    "questions = [\"Can you tell me the time, please?\", \n",
    "             \"Would you be so kind as to inform me of the current time, if possible?\", \n",
    "             \"Could you please inform me of the current time?\", \n",
    "             \"Could you kindly inform me of the current time, please?\"]\n",
    "api_runs = []  # Response Times for each run\n",
    "for i in range(runs):\n",
    "    random_question = random.choice(questions)\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic unhelpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": random_question}\n",
    "    ]\n",
    "    client = AzureOpenAI(azure_endpoint=apim_resource_gateway_url, api_key=apim_subscription_key, api_version=openai_api_version)\n",
    "    start_time = time.time()\n",
    "    response = client.chat.completions.create(model=openai_model_name, messages=messages)\n",
    "    response_time = time.time() - start_time\n",
    "    print(\"‚ñ∂Ô∏è Run:\", i+1, f\"duration: {response_time:.2f} seconds\")\n",
    "    print(\"‚ùî \", random_question)\n",
    "    print(\"üí¨ \", response.choices[0].message.content)\n",
    "    api_runs.append(response_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='plot'></a>\n",
    "### üîç Analyze Semantic Caching performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = [15, 5]\n",
    "df = pd.DataFrame(api_runs, columns=['Response Time'])\n",
    "df['Run'] = range(1, len(df) + 1)\n",
    "df.plot(kind='bar', x='Run', y='Response Time', legend=False)\n",
    "plt.title('Semantic Caching Performance')\n",
    "plt.xlabel('Runs')\n",
    "plt.ylabel('Response Time (s)')\n",
    "plt.xticks(df['Run'], rotation=0)  # Set x-axis ticks to be the run numbers\n",
    "\n",
    "average = df['Response Time'].mean()\n",
    "plt.axhline(y=average, color='r', linestyle='--', label=f'Average: {average:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
