{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coordinate Agent Collaboration using Agent Group Chat\n",
    "\n",
    "In this sample, we will explore how to use AgentGroupChat to coordinate collboration of two different agents working to review and rewrite user provided content. Each agent is assigned a distinct role:\n",
    "\n",
    "* Reviewer: Reviews and provides direction to Writer.\n",
    "* Writer: Updates user content based on Reviewer input.\n",
    "\n",
    "The approach will be broken down step-by-step to high-light the key parts of the coding process.\n",
    "\n",
    "\n",
    "## Creating the required Azure resources\n",
    "\n",
    "You can run the powershell script `create-azure-ai-resources.ps1` to create the following resources:\n",
    "* Azure AI Services with Hub and Project\n",
    "* GPT-4o model\n",
    "\n",
    "## Installing the required packages\n",
    "\n",
    "Import Semantic Kernel SDK from pypi.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U semantic-kernel --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the current version of Semantic Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import __version__\n",
    "\n",
    "__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the environment variables to connect to the LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "if os.path.exists(\".env\"):\n",
    "    load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a chat completion service\n",
    "\n",
    ">Note: The AzureChatCompletion service also supports Microsoft Entra authentication. If you don't provide an API key, the service will attempt to authenticate using the Entra token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "\n",
    "# Add Azure OpenAI chat completion\n",
    "chat_completion = AzureChatCompletion(\n",
    "    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try chat with LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.ai.open_ai import AzureChatPromptExecutionSettings\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "\n",
    "execution_settings = AzureChatPromptExecutionSettings()\n",
    "\n",
    "system_message = \"You are a helpful assistant that can answer questions and provide information.\"\n",
    "\n",
    "chat_history = ChatHistory(system_message=system_message)\n",
    "\n",
    "chat_history.add_user_message(\"Hello, how are you?\")\n",
    "\n",
    "response = await chat_completion.get_chat_message_content(\n",
    "    chat_history=chat_history,\n",
    "    settings=execution_settings,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the kernel\n",
    "\n",
    "Prior to creating any ChatCompletionAgent, the configuration settings, plugins, and Kernel must be initialized.\n",
    "\n",
    "Initialize the kernel object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "\n",
    "kernel = Kernel()\n",
    "\n",
    "kernel.add_service(service=chat_completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Definition\n",
    "\n",
    "We will declare the agent names as `Reviewer` and `Writer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVIEWER_NAME = \"Reviewer\"\n",
    "WRITER_NAME = \"Writer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the reviewer agent\n",
    "\n",
    "Here the Reviewer is given the role of responding to user input, providing direction to the Writer agent, and verifying result of the Writer agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "\n",
    "agent_reviewer = ChatCompletionAgent(\n",
    "        kernel=kernel,\n",
    "        name=REVIEWER_NAME,\n",
    "        instructions=\"\"\"\n",
    "Your responsibility is to review and identify how to improve user provided content.\n",
    "If the user has provided input or direction for content already provided, specify how to address this input.\n",
    "Never directly perform the correction or provide an example.\n",
    "Once the content has been updated in a subsequent response, review it again until it is satisfactory.\n",
    "\n",
    "RULES:\n",
    "- Only identify suggestions that are specific and actionable.\n",
    "- Verify previous suggestions have been addressed.\n",
    "- Never repeat previous suggestions.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the writer agent\n",
    "\n",
    "The Writer agent is similiar. It is given a single-purpose task, follow direction and rewrite the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_writer = ChatCompletionAgent(\n",
    "        kernel=kernel,\n",
    "        name=WRITER_NAME,\n",
    "        instructions=\"\"\"\n",
    "Your sole responsibility is to rewrite content according to review suggestions.\n",
    "- Always apply all review directions.\n",
    "- Always revise the content in its entirety without explanation.\n",
    "- Never address the user.\n",
    "\"\"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Definition\n",
    "\n",
    "Defining the `AgentGroupChat` requires considering the strategies for selecting the Agent turn and determining when to exit the Chat loop. For both of these considerations, we will define a Kernel Prompt Function.\n",
    "\n",
    "The first to reason over Agent selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.functions import KernelFunctionFromPrompt\n",
    "\n",
    "selection_function = KernelFunctionFromPrompt(\n",
    "    function_name=\"selection\", \n",
    "    prompt=f\"\"\"\n",
    "Examine the provided RESPONSE and choose the next participant.\n",
    "State only the name of the chosen participant without explanation.\n",
    "Never choose the participant named in the RESPONSE.\n",
    "\n",
    "Choose only from these participants:\n",
    "- {REVIEWER_NAME}\n",
    "- {WRITER_NAME}\n",
    "\n",
    "Rules:\n",
    "- If RESPONSE is user input, it is {REVIEWER_NAME}'s turn.\n",
    "- If RESPONSE is by {REVIEWER_NAME}, it is {WRITER_NAME}'s turn.\n",
    "- If RESPONSE is by {WRITER_NAME}, it is {REVIEWER_NAME}'s turn.\n",
    "\n",
    "RESPONSE:\n",
    "{{{{$lastmessage}}}}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second will evaluate when to exit the Chat loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termination_keyword = \"yes\"\n",
    "\n",
    "termination_function = KernelFunctionFromPrompt(\n",
    "    function_name=\"termination\", \n",
    "    prompt=f\"\"\"\n",
    "Examine the RESPONSE and determine whether the content has been deemed satisfactory.\n",
    "If the content is satisfactory, respond with a single word without explanation: {termination_keyword}.\n",
    "If specific suggestions are being provided, it is not satisfactory.\n",
    "If no correction is suggested, it is satisfactory.\n",
    "\n",
    "RESPONSE:\n",
    "{{{{$lastmessage}}}}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these Strategies will only require knowledge of the most recent Chat message. This will reduce token usage and help improve performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.contents import ChatHistoryTruncationReducer\n",
    "\n",
    "history_reducer = ChatHistoryTruncationReducer(target_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready to bring everything together in our `AgentGroupChat` definition.\n",
    "\n",
    "Creating `AgentGroupChat` involves:\n",
    "\n",
    "1. Include both agents in the constructor.\n",
    "2. Define a `KernelFunctionSelectionStrategy` using the previously defined `KernelFunction` and Kernel instance.\n",
    "3. Define a `KernelFunctionTerminationStrategy` using the previously defined `KernelFunction` and Kernel instance.\n",
    "\n",
    "Notice that each strategy is responsible for parsing the `KernelFunction` result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.agents import AgentGroupChat\n",
    "from semantic_kernel.agents.strategies import (\n",
    "    KernelFunctionSelectionStrategy,\n",
    "    KernelFunctionTerminationStrategy,\n",
    ")\n",
    "\n",
    "groupChat = AgentGroupChat(\n",
    "    agents=[agent_reviewer, agent_writer],\n",
    "    selection_strategy=KernelFunctionSelectionStrategy(\n",
    "        initial_agent=agent_reviewer,\n",
    "        function=selection_function,\n",
    "        kernel=kernel,\n",
    "        result_parser=lambda result: str(result.value[0]).strip() if result.value[0] is not None else WRITER_NAME,\n",
    "        history_variable_name=\"lastmessage\",\n",
    "        history_reducer=history_reducer,\n",
    "    ),\n",
    "    termination_strategy=KernelFunctionTerminationStrategy(\n",
    "        agents=[agent_reviewer],\n",
    "        function=termination_function,\n",
    "        kernel=kernel,\n",
    "        result_parser=lambda result: termination_keyword in str(result.value[0]).lower(),\n",
    "        history_variable_name=\"lastmessage\",\n",
    "        maximum_iterations=10,\n",
    "        history_reducer=history_reducer,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lastmessage `history_variable_name` corresponds with the `KernelFunctionSelectionStrategy` and the `KernelFunctionTerminationStrategy` prompt that was defined above. This is where the last message is placed when rendering the prompt.\n",
    "\n",
    "## The Chat Loop\n",
    "\n",
    "At last, we are able to coordinate the interaction between the user and the AgentGroupChat. Start by creating creating an empty loop.\n",
    "\n",
    ">Note: Unlike the other examples, no external history or thread is managed. AgentGroupChat manages the conversation history internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupChat.is_complete = False\n",
    "await groupChat.reset()\n",
    "\n",
    "user_input = \"Rozes are red, violetz are blue.\"\n",
    "user_input = \"\"\"\n",
    "Semantic Kernel (SK) is an open-source SDK that enables developers to build and orchestrate \n",
    "complex AI workflows that involve natural language processing (NLP) and machine learning models.\n",
    "It provies a flexible platform for integrating AI capabilities such as semantic search, \n",
    "text summarization, and dialogue systems into applications. \n",
    "With SK, you can easily combine different AI services and models, define their relationships, \n",
    "and orchestrate interactions between them.\n",
    "\"\"\"\n",
    "\n",
    "# Add the current user_input to the chat\n",
    "await groupChat.add_chat_message(message=user_input)\n",
    "\n",
    "try:\n",
    "    async for response in groupChat.invoke():\n",
    "        if response is None or not response.name:\n",
    "            continue\n",
    "        print()\n",
    "        print(f\"# {response.name.upper()}:\\n{response.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during chat invocation: {e}\")\n",
    "\n",
    "groupChat.is_complete = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also interact with the chat by providing text input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_complete: bool = False\n",
    "groupChat.is_complete = False\n",
    "await groupChat.reset()\n",
    "\n",
    "while not is_complete:\n",
    "    print()\n",
    "    user_input = input(\"User > \").strip()\n",
    "    if not user_input:\n",
    "        continue\n",
    "\n",
    "    if user_input.lower() == \"exit\":\n",
    "        is_complete = True\n",
    "        break\n",
    "\n",
    "    if user_input.lower() == \"reset\":\n",
    "        await groupChat.reset()\n",
    "        print(\"[Conversation has been reset]\")\n",
    "        continue\n",
    "\n",
    "    # Try to grab files from the script's current directory\n",
    "    if user_input.startswith(\"@\") and len(user_input) > 1:\n",
    "        file_name = user_input[1:]\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        file_path = os.path.join(script_dir, file_name)\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"Unable to access file: {file_path}\")\n",
    "                continue\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                user_input = file.read()\n",
    "        except Exception:\n",
    "            print(f\"Unable to access file: {file_path}\")\n",
    "            continue\n",
    "\n",
    "    # Add the current user_input to the chat\n",
    "    await groupChat.add_chat_message(message=user_input)\n",
    "\n",
    "    try:\n",
    "        async for response in groupChat.invoke():\n",
    "            if response is None or not response.name:\n",
    "                continue\n",
    "            print()\n",
    "            print(f\"# {response.name.upper()}:\\n{response.content}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during chat invocation: {e}\")\n",
    "\n",
    "    # Reset the chat's complete flag for the new conversation round.\n",
    "    groupChat.is_complete = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's capture user input within the previous loop. In this case:\n",
    "\n",
    "* Empty input will be ignored.\n",
    "* The term exit will signal that the conversation is complete.\n",
    "* The term reset will clear the AgentGroupChat history.\n",
    "* Any term starting with @ will be treated as a file-path whose content will be provided as input.\n",
    "\n",
    "Valid input will be added to the `AgentGroupChat` as a User message.\n",
    "\n",
    "To initate the Agent collaboration in response to user input and display the Agent responses, invoke the AgentGroupChat; however, first be sure to reset the Completion state from any prior invocation.\n",
    "\n",
    ">Note: Service failures are being caught and displayed to avoid crashing the conversation loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More resources\n",
    "\n",
    "https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/examples/example-agent-collaboration?pivots=programming-language-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
