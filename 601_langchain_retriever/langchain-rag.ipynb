{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a semantic search engine\n",
    "\n",
    "This tutorial will familiarize you with LangChain's document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data-- from (vector) databases and other sources-- for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG (see our RAG tutorial here).\n",
    "\n",
    "Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You will need to provision the following Azure resources:\n",
    "* Azure OpenAI with two models: GPT-4o and Embedding model.\n",
    "* Azure AI Search.\n",
    "You can run the terraform template from folder `../400_azure_ai_foundry` to create all of these resources by simply running the following commands.\n",
    "\n",
    "```sh\n",
    "terraform init\n",
    "terraform plan -out tfplan\n",
    "terraform apply tfplan\n",
    "```\n",
    "\n",
    "Then you will need to setup the values for the environment variables in file `.env`.\n",
    "\n",
    "The following resources should be created.\n",
    "\n",
    "![](images/resources.png)\n",
    "\n",
    "## Documents and Document Loaders\n",
    "\n",
    "LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n",
    "\n",
    "* page_content: a string representing the content;\n",
    "* metadata: a dict containing arbitrary metadata;\n",
    "* id: (optional) a string identifier for the document.\n",
    "\n",
    "The metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document.\n",
    "\n",
    "This tutorial requires the langchain-community and pypdf packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-community pypdf --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading documents\n",
    "\n",
    "Let's load a PDF into a sequence of Document objects. We can consult the LangChain documentation for available PDF document loaders. Let's select PyPDFLoader, which is fairly lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./example_data/azure-for-architects.pdf\" #nke-10k-2023.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyPDFLoader loads one Document object per PDF page. For each, we can easily access:\n",
    "\n",
    "* The string content of the page;\n",
    "* Metadata containing the file name and page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 | Cost management for Azure solutions\n",
      "It is important for companies to keep a close watch on Azure consumption and usage. \n",
      "Although they can create policies to set organizational standards and conventions, \n",
      "there is also a need to keep track of billing and consumption data. Moreover, they \n",
      "should employ best practices for consuming Azure resources so that the return is \n",
      "maximized. For this, architects need to know about Azure resources and features, their \n",
      "corresponding costs, and the cost/benefit analysis of features and solutions.\n",
      "In this chapter, we will cover the following topics:\n",
      "• Azure offer details\n",
      "• Billing\n",
      "• Invoicing\n",
      "• Usage and quotas\n",
      "• Usage and Billing APIs\n",
      "• Azure pricing calculator\n",
      "• Best practices for cost optimization\n",
      "Let's go ahead and discuss each of these points.\n",
      "Azure offer details\n",
      "Azure has different offers that you can purchase. So far, we have discussed pay-as-\n",
      "you-go, but there are others, such as Enterprise Agreements (EAs), Azure Sponsorship, \n",
      "and Azure in CSP. We will cover each of these as they are very important for billing:\n",
      "• Pay-as-you-go: This is a commonly known offering, where customers pay based \n",
      "on the consumption and the rates are available in the public-facing documentation \n",
      "of Azure. Customers receive an invoice every month for usage from Microsoft and \n",
      "they can pay via credit card or an invoice payment method.\n",
      "• EAs: EAs entail a monetary commitment with Microsoft, which means that \n",
      "organizations sign an agreement with Microsoft and promise that they will use \n",
      "x amount of Azure resources. If the usage goes above the agreed amount, the \n",
      "customer will receive an overage invoice. Customers can create multiple accounts \n",
      "under an EA and have multiple subscriptions inside these accounts. There are two \n",
      "types of EA: Direct EA and Indirect EA. Direct EA customers have a direct billing \n",
      "relationship with Microsoft; on the other hand, with Indirect EA, the billing is \n",
      "managed by a partner. EA customers will get better offers and discounts because \n",
      "of the commitment they make with Microsoft. The EA is managed via a portal \n",
      "called the EA portal (https:/ /ea.azure.com), and you need to have enrolment \n",
      "privileges to access this portal.\n",
      "\n",
      "{'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.0 (Windows)', 'creationdate': '2021-06-17T13:34:27+05:30', 'author': 'Ritesh Modi', 'moddate': '2021-06-17T14:21:14+05:30', 'subject': 'Create secure, scalable, high-availability applications on the cloud', 'title': 'Azure for Architects, Third Edition', 'trapped': '/False', 'source': './example_data/azure-for-architects.pdf', 'total_pages': 701, 'page': 200, 'page_label': '168'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"{docs[200].page_content}\\n\")\n",
    "print(docs[200].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting text by characters\n",
    "\n",
    "For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve `Document` objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\n",
    "\n",
    "We can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
    "\n",
    "We set `add_start_index=True` so that the character index where each split Document starts within the initial Document is preserved as metadata attribute “start_index”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits:  1385\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=0, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(\"Number of splits: \", len(all_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View a sample split or chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='increased, automation has found increased focus and attention. The primary reason \n",
      "is that cloud technology's agility and flexibility provide an opportunity to provision, \n",
      "deprovision, and manage these resources on the fly in a tiny fraction of the time it \n",
      "used to take. Along with this flexibility and agility come the requirements to be more \n",
      "predictable and consistent with the cloud because it has become easy for organizations \n",
      "to create resources.\n",
      "Provisioning\n",
      "• Data centers\n",
      "• PaaS services\n",
      "• Application \n",
      "deployments\n",
      "• Environment \n",
      "and application \n",
      "Operations\n",
      "• Backups and \n",
      "restores\n",
      "• Monitoring\n",
      "• Performance\n",
      "• Disaster \n",
      "recovery\n",
      "• Availability\n",
      "• Scalability\n",
      "• Stop-start-\n",
      "pause-resume\n",
      "Management\n",
      "• Access \n",
      "management\n",
      "• Resource \n",
      "locking\n",
      "• Policies\n",
      "• Tagging\n",
      "• Cost\n",
      "• releases\n",
      "Deprovisioning\n",
      "• Delete and \n",
      "tear down \n",
      "resources\n",
      "• Soft delete \n",
      "of application \n",
      "• Transfer' metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.0 (Windows)', 'creationdate': '2021-06-17T13:34:27+05:30', 'author': 'Ritesh Modi', 'moddate': '2021-06-17T14:21:14+05:30', 'subject': 'Create secure, scalable, high-availability applications on the cloud', 'title': 'Azure for Architects, Third Edition', 'trapped': '/False', 'source': './example_data/azure-for-architects.pdf', 'total_pages': 701, 'page': 136, 'page_label': '104', 'start_index': 961}\n",
      "---------------\n",
      "page_content='Azure Automation | 105\n",
      "Microsoft has a great tool for IT automation known as System Center Orchestrator. It is \n",
      "a great tool for automation for on-premises and cloud environments, but it is a product \n",
      "and not a service. It should be licensed and deployed on servers, and then runbooks can \n",
      "be executed to effect changes on cloud and on-premises environments.\n",
      "Microsoft realized that an automation solution was required that could be provided \n",
      "to customers as a service rather than bought and deployed as a product. Enter Azure \n",
      "Automation.\n",
      "Azure Automation\n",
      "Azure provides a service called Azure Automation, which is an essential service for the \n",
      "automation of processes, activities, and tasks not only on Azure but also on-premises \n",
      "as well. Using Azure Automation, organizations can automate their processes and tasks \n",
      "related to processing, tear-down, operations, and the management of their resources \n",
      "across the cloud, IT environments, platforms, and languages. In Figure 4.2, we can see' metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.0 (Windows)', 'creationdate': '2021-06-17T13:34:27+05:30', 'author': 'Ritesh Modi', 'moddate': '2021-06-17T14:21:14+05:30', 'subject': 'Create secure, scalable, high-availability applications on the cloud', 'title': 'Azure for Architects, Third Edition', 'trapped': '/False', 'source': './example_data/azure-for-architects.pdf', 'total_pages': 701, 'page': 137, 'page_label': '105', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "# print a sample chunk\n",
    "print(all_splits[309])\n",
    "print(\"---------------\")\n",
    "print(all_splits[310])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings\n",
    "\n",
    "`Vector search` is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\n",
    "\n",
    "`LangChain` supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\hodellai\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-openai --quiet\n",
    "%pip install python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if os.path.exists(\".env\"):\n",
    "    load_dotenv(override=True)\n",
    "\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_EMBEDDING_API_VERSION\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated vectors of length 3072\n",
      "\n",
      "[0.010515968315303326, -0.01905210129916668, -0.017833741381764412, 0.022737640887498856, 0.008901641704142094, -0.006392581854015589, -0.011467811651527882, 0.024169212207198143, -0.0178185123950243, 0.02282901667058468]\n",
      "[-0.002384420484304428, -0.014230713248252869, -0.017530273646116257, 0.012981644831597805, 0.021096978336572647, -0.03303893655538559, -0.021400220692157745, 0.027956021949648857, -0.031335003674030304, 0.007270587608218193]\n"
     ]
    }
   ],
   "source": [
    "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
    "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
    "\n",
    "print(f\"Generated vectors of length {len(vector_1)}\\n\")\n",
    "print(vector_1[:10])\n",
    "print(vector_2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\n",
    "\n",
    "## Vector stores for Azure AI Search\n",
    "\n",
    "`LangChain VectorStore` objects contain methods for adding text and `Document` objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\n",
    "\n",
    "`LangChain` includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as `Postgres`) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet azure-search-documents\n",
    "%pip install --upgrade --quiet azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings, AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_EMBEDDING_API_VERSION\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector store instance\n",
    "\n",
    "Create instance of the `AzureSearch` class using the embeddings from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "# Specify additional properties for the Azure client such as the following https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/core/azure-core/README.md#configurations\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"],\n",
    "    azure_search_key=os.environ[\"AZURE_SEARCH_SERVICE_ADMIN_KEY\"],\n",
    "    index_name=os.environ[\"AZURE_SEARCH_SERVICE_INDEX\"],\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    # Configure max retries for the Azure client\n",
    "    additional_search_client_options={\"retry_total\": 3},\n",
    "    relevance_score_fn=\"cosine\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert text and embeddings into vector store\n",
    "\n",
    "This step loads, chunks, and vectorizes the sample document, and then indexes the content into a search index on `Azure AI Search`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "azure.search.documents._generated.operations._documents_operations.DocumentsOperations.index() got multiple values for keyword argument 'error_map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRequestEntityTooLargeError\u001b[0m                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\hodellai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\azure\\search\\documents\\_search_client.py:655\u001b[0m, in \u001b[0;36mSearchClient._index_documents_actions\u001b[1;34m(self, actions, **kwargs)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     batch_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(List[IndexingResult], batch_response\u001b[38;5;241m.\u001b[39mresults)\n",
      "File \u001b[1;32mc:\\Users\\hodellai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:105\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hodellai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\azure\\search\\documents\\_generated\\operations\\_documents_operations.py:1200\u001b[0m, in \u001b[0;36mDocumentsOperations.index\u001b[1;34m(self, batch, request_options, **kwargs)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m207\u001b[39m]:\n\u001b[1;32m-> 1200\u001b[0m     \u001b[43mmap_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1201\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n",
      "File \u001b[1;32mc:\\Users\\hodellai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\azure\\core\\exceptions.py:163\u001b[0m, in \u001b[0;36mmap_error\u001b[1;34m(status_code, response, error_map)\u001b[0m\n\u001b[0;32m    162\u001b[0m error \u001b[38;5;241m=\u001b[39m error_type(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[1;31mRequestEntityTooLargeError\u001b[0m: Operation returned an invalid status 'Request Entity Too Large'\nContent: The page was not displayed because the request entity is too large.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_splits\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hodellai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:286\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    285\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m )\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "File \u001b[1;32mc:\\Users\\hodellai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\vectorstores\\azuresearch.py:481\u001b[0m, in \u001b[0;36mAzureSearch.add_texts\u001b[1;34m(self, texts, metadatas, keys, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m keys) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(embeddings)):\n\u001b[0;32m    479\u001b[0m     keys \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hodellai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\vectorstores\\azuresearch.py:555\u001b[0m, in \u001b[0;36mAzureSearch.add_embeddings\u001b[1;34m(self, text_embeddings, metadatas, keys)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# Upload data in batches\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m MAX_UPLOAD_BATCH_SIZE:\n\u001b[1;32m--> 555\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;66;03m# Check if all documents were successfully uploaded\u001b[39;00m\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(r\u001b[38;5;241m.\u001b[39msucceeded \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m response):\n",
      "File \u001b[1;32mc:\\Users\\hodellai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\azure\\search\\documents\\_search_client.py:548\u001b[0m, in \u001b[0;36mSearchClient.upload_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m batch\u001b[38;5;241m.\u001b[39madd_upload_actions(documents)\n\u001b[0;32m    547\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_client_headers(kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 548\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(List[IndexingResult], results)\n",
      "File \u001b[1;32mc:\\Users\\hodellai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:105\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\hodellai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\azure\\search\\documents\\_search_client.py:647\u001b[0m, in \u001b[0;36mSearchClient.index_documents\u001b[1;34m(self, batch, **kwargs)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;129m@distributed_trace\u001b[39m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mindex_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: IndexDocumentsBatch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[IndexingResult]:\n\u001b[0;32m    638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Specify a document operations to perform as a batch.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m    :param batch: A batch of document operations to perform.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;124;03m    :raises ~azure.search.documents.RequestEntityTooLargeError\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_documents_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hodellai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\azure\\search\\documents\\_search_client.py:661\u001b[0m, in \u001b[0;36mSearchClient._index_documents_actions\u001b[1;34m(self, actions, **kwargs)\u001b[0m\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    660\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mlen\u001b[39m(actions) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 661\u001b[0m batch_response_first_half \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_documents_actions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_response_first_half:\n\u001b[0;32m    665\u001b[0m     result_first_half \u001b[38;5;241m=\u001b[39m batch_response_first_half\n",
      "File \u001b[1;32mc:\\Users\\hodellai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\azure\\search\\documents\\_search_client.py:655\u001b[0m, in \u001b[0;36mSearchClient._index_documents_actions\u001b[1;34m(self, actions, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m batch \u001b[38;5;241m=\u001b[39m IndexBatch(actions\u001b[38;5;241m=\u001b[39mactions)\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 655\u001b[0m     batch_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mdocuments\u001b[38;5;241m.\u001b[39mindex(batch\u001b[38;5;241m=\u001b[39mbatch, error_map\u001b[38;5;241m=\u001b[39merror_map, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(List[IndexingResult], batch_response\u001b[38;5;241m.\u001b[39mresults)\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RequestEntityTooLargeError:\n",
      "\u001b[1;31mTypeError\u001b[0m: azure.search.documents._generated.operations._documents_operations.DocumentsOperations.index() got multiple values for keyword argument 'error_map'"
     ]
    }
   ],
   "source": [
    "vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying:\n",
    "\n",
    "* Synchronously and asynchronously;\n",
    "* By string query and by vector;\n",
    "* With and without returning similarity scores;\n",
    "* By similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a vector similarity search with relevance scores\n",
    "\n",
    "Execute a pure vector similarity search using the `similarity_search_with_relevance_scores()` method. Queries that don't meet the threshold requirements are exluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "docs_and_scores = vector_store.similarity_search_with_relevance_scores(\n",
    "    query=\"Who are the authors of the book Azure for Architects ?\",\n",
    "    k=4,\n",
    "    score_threshold=0.70,\n",
    ")\n",
    "\n",
    "pprint(docs_and_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a hybrid search\n",
    "\n",
    "`Hybrid search` combines keyword and semantic similarity, marrying the benefits of both approaches.\n",
    "Execute hybrid search using the `search_type` or `hybrid_search()` method. Vector and nonvector text fields are queried in parallel, results are merged, and top matches of the unified result set are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a hybrid search using the search_type parameter\n",
    "docs = vector_store.similarity_search(\n",
    "    query=\"Who are the authors of the book Azure for Architects\",\n",
    "    k=3,\n",
    "    search_type=\"hybrid\",\n",
    ")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval and Generation using LangGraph\n",
    "\n",
    "Now let’s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n",
    "\n",
    "For generation, we will use the chat model selected at the start of the tutorial.\n",
    "\n",
    "We’ll use a prompt for `RAG` that is checked into the `LangChain` prompt hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain import hub\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "question = \"Who are the authors of the book Azure for Architects and when it was published ?\"\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(question)\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "prompt = prompt.invoke({\"question\": question, \"context\": docs_content})\n",
    "answer = llm.invoke(prompt)\n",
    "\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain import hub\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    ")\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "response = graph.invoke({\"question\": \"Who are the authors of the book Azure for Architects and when it was published ?\"})\n",
    "\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph also comes with built-in utilities for visualizing the control flow of your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
