{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG using LangChain\n",
    "\n",
    "This tutorial will familiarize you with LangChain's document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data-- from (vector) databases and other sources-- for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG (see our RAG tutorial here).\n",
    "\n",
    "Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You will need to provision the following Azure resources:\n",
    "* Azure OpenAI with two models: GPT-4o and Embedding model.\n",
    "* Azure AI Search.\n",
    "You can run the terraform template from folder `../400_azure_ai_foundry` to create all of these resources by simply running the following commands.\n",
    "\n",
    "```sh\n",
    "terraform init\n",
    "terraform plan -out tfplan\n",
    "terraform apply tfplan\n",
    "```\n",
    "\n",
    "Then you will need to setup the values for the environment variables in file `.env`.\n",
    "\n",
    "The following resources should be created.\n",
    "\n",
    "![](images/resources.png)\n",
    "\n",
    "## Documents and Document Loaders\n",
    "\n",
    "LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:\n",
    "\n",
    "* page_content: a string representing the content;\n",
    "* metadata: a dict containing arbitrary metadata;\n",
    "* id: (optional) a string identifier for the document.\n",
    "\n",
    "The metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document.\n",
    "\n",
    "This tutorial requires the langchain-community and pypdf packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-community pypdf --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading documents\n",
    "\n",
    "Let's load a PDF into a sequence of Document objects. We can consult the LangChain documentation for available PDF document loaders. Let's select PyPDFLoader, which is fairly lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"./example_data/azure-for-architects.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyPDFLoader loads one Document object per PDF page. For each, we can easily access:\n",
    "\n",
    "* The string content of the page;\n",
    "* Metadata containing the file name and page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{docs[200].page_content}\\n\")\n",
    "print(docs[200].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Splitting/chunking text by characters\n",
    "\n",
    "For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve `Document` objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\n",
    "\n",
    "We can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
    "\n",
    "We set `add_start_index=True` so that the character index where each split Document starts within the initial Document is preserved as metadata attribute “start_index”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits:  1385\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=0, \n",
    "    add_start_index=True\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(\"Number of splits: \", len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Splitting/chunking text based on Semantic similarity\n",
    "\n",
    "If embeddings are sufficiently far apart, chunks are split.\n",
    "\n",
    "At a high level, this splits into sentences, then groups into groups of 3 sentences, and then merges one that are similar in the embedding space.\n",
    "\n",
    "This chunker works by determining when to \"break\" apart sentences. This is done by looking for differences in embeddings between any two sentences. When that difference is past some threshold, then they are split.\n",
    "\n",
    "There are a few ways to determine what that threshold is, which are controlled by the breakpoint_threshold_type kwarg.\n",
    "\n",
    ">Note: if the resulting chunk sizes are too small/big, the additional kwargs breakpoint_threshold_amount and min_chunk_size can be used for adjustments.\n",
    "\n",
    "More details: https://python.langchain.com/docs/how_to/semantic-chunker/\n",
    "\n",
    "Install Dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet langchain_experimental langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To instantiate a SemanticChunker, we must specify an embedding model. Below we will use OpenAIEmbeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "# embeddings was defined earlier\n",
    "text_splitter = SemanticChunker(embeddings)\n",
    "\n",
    "# concatenate the documents\n",
    "pages = [d.page_content for d in docs]\n",
    "len(pages)\n",
    "\n",
    "chunks = text_splitter.create_documents(pages)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ritesh Modi, Jack Lee, and Rithin Skaria\n",
      "Create secure, scalable, high-availability \n",
      "applications on the cloud\n",
      "Azure for Architects\n",
      "Third Edition\n",
      "---------------\n",
      "Azure for Architects Third Edition\n",
      "Copyright © 2020 Packt Publishing\n",
      "All rights reserved. No part of this book may be reproduced, stored in a retrieval system, \n",
      "or transmitted in any form or by any means, without the prior written permission of the \n",
      "publisher, except in the case of brief quotations embedded in critical articles or reviews.\n",
      "Every effort has been made in the preparation of this book to ensure the accuracy of \n",
      "the information presented. However, the information contained in this book is sold \n",
      "without warranty, either express or implied. Neither the authors, nor Packt Publishing, \n",
      "and its dealers and distributors will be held liable for any damages caused or alleged to \n",
      "be caused directly or indirectly by this book.\n",
      "Packt Publishing has endeavored to provide trademark information about all of the \n",
      "companies and products mentioned in this book by the appropriate use of capitals. \n",
      "However, Packt Publishing cannot guarantee the accuracy of this information.\n",
      "---------------\n",
      "Authors: Ritesh Modi, Jack Lee, and Rithin Skaria\n",
      "Technical Reviewers: Melony Qin and Sanjeev Kumar\n",
      "Managing Editors: Aditya Datar and Afzal Shaikh\n",
      "Acquisitions Editor: Shrilekha Inani\n",
      "Production Editors: Ganesh Bhadwalkar and Deepak Chavan\n",
      "Editorial Board: Vishal Bodwani, Ben Renow-Clarke, Edward Doxey, Joanne Lovell,  \n",
      "Arijit Sarkar, and Dominic Shakeshaft\n",
      "First Edition: October 2017\n",
      "Second Edition: January 2019\n",
      "Third Edition: June 2020\n",
      "Production Reference: 3260620\n",
      "ISBN: 978-1-83921-586-5\n",
      "Published by Packt Publishing Ltd.\n",
      "Livery Place, 35 Livery Street\n",
      "Birmingham B3 2PB, UK\n",
      "---------------\n",
      "startHere(Azure);\n",
      "Learn Azure. Experiment with more \n",
      "than 100 services. \n",
      "●  Free services  \n",
      "●  $200 credit  \n",
      "●  Free training\n",
      "Try Azure for free >\n",
      "Get help with your project.\n",
      "Talk to a sales specialist >      \n",
      "Get hands-on \n",
      "in the cloud\n",
      "Try Azure for free >\n",
      "Talk to a sales specialist >\n",
      "---------------\n",
      "Table of Contents\n",
      "Preface   i\n",
      "Chapter 1: Getting started with Azure   1\n",
      "Cloud computing  ..................................................................................................... 2\n",
      "The advantages of cloud computing  ........................................................................... 3\n",
      "Why cloud computing?  .................................................................................................. 3\n",
      "Deployment paradigms in Azure  ................................................................................. 5\n",
      "Understanding Azure  .............................................................................................  6\n",
      "Azure as an intelligent cloud  .................................................................................  8\n",
      "Azure Resource Manager  ....................................................................................... 8\n",
      "---------------\n",
      "The ARM architecture  .................................................................................................... 9\n",
      "Why ARM?  ....................................................................................................................... 9\n",
      "ARM advantages  .......................................................................................................... 10\n",
      "ARM concepts  ............................................................................................................... 11\n",
      "Virtualization  ......................................................................................................... 14\n",
      "Containers  .............................................................................................................  15\n",
      "Docker  .................................................................................................................... 17\n",
      "Interacting with the intelligent cloud  ................................................................. 17\n",
      "---------------\n",
      "The Azure portal  ........................................................................................................... 17\n",
      "PowerShell  .................................................................................................................... 18\n",
      "The Azure CLI  ................................................................................................................ 18\n",
      "The Azure REST API  ...................................................................................................... 19\n",
      "ARM templates  ............................................................................................................. 19\n",
      "Summary  ................................................................................................................  20\n",
      "---------------\n",
      "Chapter 2: Azure solution availability, scalability,  \n",
      "and monitoring   23\n",
      "High availability  .....................................................................................................  24\n",
      "Azure high availability  ..........................................................................................  25\n",
      "Concepts  ....................................................................................................................... 26\n",
      "Load balancing  ............................................................................................................. 29\n",
      "VM high availability  ..................................................................................................... 30\n",
      "Compute high availability  ........................................................................................... 30\n",
      "High-availability platforms  ......................................................................................... 32\n",
      "---------------\n",
      "Load balancers in Azure  .............................................................................................. 32\n",
      "The Azure Application Gateway  ................................................................................. 35\n",
      "Azure Traffic Manager  ................................................................................................. 36\n",
      "Azure Front Door  ......................................................................................................... 38\n",
      "Architectural considerations for high availability  ............................................  38\n",
      "High availability within Azure regions  ....................................................................... 39\n",
      "High availability across Azure regions  ...................................................................... 40\n",
      "Scalability  ............................................................................................................... 42\n",
      "---------------\n",
      "Scalability versus performance  .................................................................................. 43\n",
      "Azure scalability  ........................................................................................................... 44\n",
      "PaaS scalability  ............................................................................................................. 46\n",
      "IaaS scalability  .............................................................................................................. 49\n",
      "VM scale sets  .........................................................................................................  50\n",
      "VMSS architecture  ........................................................................................................ 51\n",
      "VMSS scaling  ................................................................................................................. 51\n",
      "---------------\n",
      "Upgrades and maintenance  ................................................................................  54\n",
      "Application updates  ..................................................................................................... 56\n",
      "Guest updates  .............................................................................................................. 56\n",
      "Image updates  .............................................................................................................. 56\n",
      "Best practices of scaling for VMSSes  ......................................................................... 57\n",
      "Monitoring  ............................................................................................................. 58\n",
      "Azure monitoring  ......................................................................................................... 59\n",
      "Azure activity logs  ........................................................................................................ 59\n",
      "---------------\n",
      "Azure diagnostic logs  ................................................................................................... 60\n",
      "Azure application logs  ................................................................................................. 60\n",
      "Guest and host OS logs  ............................................................................................... 60\n",
      "Azure Monitor  .............................................................................................................. 61\n",
      "Azure Application Insights  .......................................................................................... 61\n",
      "Azure Log Analytics  ...................................................................................................... 61\n",
      "Solutions  ....................................................................................................................... 62\n",
      "---------------\n",
      "Alerts  ............................................................................................................................. 63\n",
      "Summary  ................................................................................................................  67\n",
      "Chapter 3: Design pattern – Networks, storage, messaging,  \n",
      "and events   69\n",
      "Azure Availability Zones and Regions  ................................................................  70\n",
      "Availability of resources  .............................................................................................. 70\n",
      "Data and privacy compliance  ..................................................................................... 70\n",
      "Application performance  ............................................................................................ 71\n",
      "Cost of running applications  ...................................................................................... 71\n",
      "---------------\n",
      "Virtual networks  ...................................................................................................  71\n",
      "Architectural considerations for virtual networks  .................................................. 72\n",
      "Benefits of virtual networks  ....................................................................................... 76\n",
      "Virtual network design  .........................................................................................  76\n",
      "Connecting to resources within the same region and subscription  ..................... 77\n",
      "Connecting to resources within the same region in another subscription  .......... 77\n",
      "Connecting to resources in different regions in another subscription  ................. 79\n",
      "Connecting to on-premises datacenters  ................................................................... 80\n",
      "Storage  ...................................................................................................................  84\n",
      "---------------\n",
      "Storage categories  ....................................................................................................... 84\n",
      "Storage types  ................................................................................................................ 84\n",
      "Storage features  ........................................................................................................... 85\n",
      "Architectural considerations for storage accounts  ................................................. 86\n",
      "Cloud design patterns  .......................................................................................... 88\n",
      "Messaging patterns  ..................................................................................................... 89\n",
      "Performance and scalability patterns  ....................................................................... 93\n",
      "Summary  ..............................................................................................................  101\n",
      "---------------\n",
      "Chapter 4: Automating architecture on Azure   103\n",
      "Automation  .......................................................................................................... 104\n",
      "Azure Automation  ............................................................................................... 105\n",
      "Azure Automation architecture  ........................................................................ 105\n",
      "Process automation  ..................................................................................................  107\n",
      "Configuration management  ....................................................................................  108\n",
      "Update management  ...............................................................................................  109\n",
      "---------------\n",
      "Concepts related to Azure Automation  ........................................................... 109\n",
      "Runbook   .................................................................................................................... 109\n",
      "Run As accounts  ........................................................................................................ 110\n",
      "Jobs   ............................................................................................................................  111\n",
      "Assets  .........................................................................................................................  112\n",
      "Credentials  ................................................................................................................. 112\n",
      "Certificates  ................................................................................................................. 113\n",
      "Creating a service principal using certificate credentials  .................................... 115\n",
      "---------------\n",
      "Connections  ...............................................................................................................  116\n",
      "Runbook authoring and execution  ................................................................... 118\n",
      "Parent and child runbooks  ......................................................................................  119\n",
      "Creating a runbook  ................................................................................................... 120\n",
      "Using Az modules  ................................................................................................ 122\n",
      "Webhooks  ............................................................................................................  125\n",
      "Invoking a webhook  .................................................................................................  127\n",
      "Invoking a runbook from Azure Monitor  ...............................................................  129\n",
      "---------------\n",
      "Hybrid Workers  .........................................................................................................  134\n",
      "Azure Automation State Configuration  ...........................................................  136\n",
      "Azure Automation pricing  .................................................................................. 141\n",
      "Comparison with serverless automation  ........................................................  141\n",
      "Summary  ..............................................................................................................  142\n",
      "---------------\n",
      "Chapter 5: Designing policies, locks, and tags for  \n",
      "Azure deployments   145\n",
      "Azure management groups  ...............................................................................  146\n",
      "Azure tags  ............................................................................................................ 147\n",
      "Tags with PowerShell ................................................................................................  150\n",
      "Tags with Azure Resource Manager templates  ..................................................... 150\n",
      "Tagging resource groups versus resources  ...........................................................  151\n",
      "Azure Policy  ......................................................................................................... 152\n",
      "Built-in policies  ..........................................................................................................  153\n",
      "---------------\n",
      "Policy language  .........................................................................................................  153\n",
      "Allowed fields  ............................................................................................................  156\n",
      "Azure locks  ........................................................................................................... 156\n",
      "Azure RBAC  .......................................................................................................... 158\n",
      "Custom roles ..............................................................................................................  161\n",
      "How are locks different from RBAC?  ......................................................................  162\n",
      "Azure Blueprints  ................................................................................................. 162\n",
      "An example of implementing Azure governance features  ............................ 163\n",
      "---------------\n",
      "Background  ................................................................................................................  163\n",
      "RBAC for Company Inc  .............................................................................................  163\n",
      "Azure Policy  ...............................................................................................................  164\n",
      "Azure locks  ................................................................................................................. 165\n",
      "Summary  ..............................................................................................................  165\n",
      "---------------\n",
      "Chapter 6: Cost management for Azure solutions   167\n",
      "Azure offer details  .............................................................................................. 168\n",
      "Understanding billing  ......................................................................................... 169\n",
      "Invoicing  ............................................................................................................... 176\n",
      "The Modern Commerce experience  .......................................................................  177\n",
      "Usage and quotas  ............................................................................................... 179\n",
      "Resource providers and resource types  .......................................................... 180\n",
      "Usage and Billing APIs  ........................................................................................ 182\n",
      "Azure Enterprise Billing APIs  ...................................................................................  182\n",
      "---------------\n",
      "Azure Consumption APIs  .........................................................................................  183\n",
      "Azure Cost Management APIs  .................................................................................  184\n",
      "Azure pricing calculator  ..................................................................................... 184\n",
      "Best practices  ......................................................................................................  187\n",
      "Azure Governance  ....................................................................................................  187\n",
      "Compute best practices  ...........................................................................................  188\n",
      "Storage best practices  .............................................................................................. 189\n",
      "PaaS best practices  ................................................................................................... 190\n",
      "---------------\n",
      "General best practices  .............................................................................................  191\n",
      "Summary  ..............................................................................................................  191\n",
      "---------------\n",
      "Chapter 7: Azure OLTP solutions   193\n",
      "OLTP applications  ............................................................................................... 194\n",
      "Relational databases  ................................................................................................  195\n",
      "Azure cloud services  ........................................................................................... 195\n",
      "Deployment models  ........................................................................................... 196\n",
      "Databases on Azure Virtual Machines  ...................................................................  197\n",
      "Databases hosted as managed services  ................................................................  198\n",
      "Azure SQL Database  ........................................................................................... 198\n",
      "Application features  .................................................................................................  199\n",
      "---------------\n",
      "Security  ......................................................................................................................  204\n",
      "Single Instance  ....................................................................................................  210\n",
      "Elastic pools  ......................................................................................................... 211\n",
      "Managed Instance  ..............................................................................................  213\n",
      "SQL database pricing  .......................................................................................... 215\n",
      "DTU-based pricing  .....................................................................................................  215\n",
      "vCPU-based pricing  ................................................................................................... 217\n",
      "How to choose the appropriate pricing model  .....................................................  218\n",
      "---------------\n",
      "Azure Cosmos DB  ................................................................................................ 219\n",
      "Features  .....................................................................................................................  221\n",
      "Use case scenarios  ....................................................................................................  222\n",
      "Summary  ..............................................................................................................  222\n",
      "---------------\n",
      "Chapter 8: Architecting secure applications on Azure   225\n",
      "Security  ................................................................................................................  226\n",
      "Security life cycle  ......................................................................................................  228\n",
      "Azure security  ............................................................................................................ 230\n",
      "IaaS security  ........................................................................................................ 231\n",
      "Network security groups  .........................................................................................  231\n",
      "Firewalls  .....................................................................................................................  234\n",
      "Application security groups  ..................................................................................... 235\n",
      "---------------\n",
      "Azure Firewall  ............................................................................................................ 236\n",
      "Reducing the attack surface area  ...........................................................................  237\n",
      "Implementing jump servers  ....................................................................................  238\n",
      "Azure Bastion  ............................................................................................................  239\n",
      "Application security  ............................................................................................  239\n",
      "SSL/TLS  .......................................................................................................................  239\n",
      "Managed identities  ...................................................................................................  240\n",
      "Azure Sentinel  ..................................................................................................... 244\n",
      "---------------\n",
      "PaaS security  ....................................................................................................... 245\n",
      "Azure Private Link  .....................................................................................................  245\n",
      "Azure Application Gateway  .....................................................................................  245\n",
      "Azure Front Door  ......................................................................................................  246\n",
      "Azure App Service Environment ..............................................................................  247\n",
      "Log Analytics  ..............................................................................................................  247\n",
      "---------------\n",
      "Azure Storage  ......................................................................................................  248\n",
      "Azure SQL  .............................................................................................................  252\n",
      "Azure Key Vault  ...................................................................................................  256\n",
      "Authentication and authorization using OAuth  .............................................  257\n",
      "Security monitoring and auditing  .....................................................................  265\n",
      "Azure Monitor  ...........................................................................................................  265\n",
      "Azure Security Center  ............................................................................................... 267\n",
      "Summary  ..............................................................................................................  268\n",
      "Chapter 9: Azure Big Data solutions   271\n",
      "---------------\n",
      "Big data  ................................................................................................................  272\n",
      "Process for big data  ..................................................................................................  273\n",
      "Big data tools  .......................................................................................................  274\n",
      "Azure Data Factory   ..................................................................................................  274\n",
      "Azure Data Lake Storage  .......................................................................................... 274\n",
      "Hadoop  ....................................................................................................................... 275\n",
      "Apache Spark   ............................................................................................................ 276\n",
      "---------------\n",
      "Databricks  ..................................................................................................................  276\n",
      "Data integration  .................................................................................................. 276\n",
      "ETL .........................................................................................................................  277\n",
      "A primer on Azure Data Factory  .......................................................................  278\n",
      "A primer on Azure Data Lake Storage   ............................................................. 279\n",
      "---------------\n",
      "Migrating data from Azure Storage to Data Lake Storage Gen2  .................. 280\n",
      "Preparing the source storage account  ...................................................................  280\n",
      "Provisioning a new resource group  ........................................................................ 280\n",
      "Provisioning a storage account  ...............................................................................  281\n",
      "Provisioning the Data Lake Storage Gen2 service  ................................................  283\n",
      "Provisioning Azure Data Factory   ............................................................................  284\n",
      "Repository settings  ...................................................................................................  285\n",
      "Data Factory datasets  ..............................................................................................  287\n",
      "---------------\n",
      "Creating the second dataset  ...................................................................................  289\n",
      "Creating a third dataset  ...........................................................................................  289\n",
      "Creating a pipeline  .................................................................................................... 291\n",
      "Adding one more Copy Data activity  ...................................................................... 293\n",
      "Creating a solution using Databricks  ............................................................... 294\n",
      "Loading data  ..............................................................................................................  297\n",
      "Summary  ..............................................................................................................  303\n",
      "Chapter 10: Serverless in Azure – Working with  \n",
      "Azure Functions   305\n",
      "---------------\n",
      "Serverless  ............................................................................................................. 306\n",
      "The advantages of Azure Functions  .................................................................  306\n",
      "FaaS  ......................................................................................................................  308\n",
      "The Azure Functions runtime  .................................................................................. 308\n",
      "Azure Functions bindings and triggers  ..................................................................  309\n",
      "Azure Functions configuration  ................................................................................ 312\n",
      "Azure Functions cost plans  ......................................................................................  314\n",
      "Azure Functions destination hosts  .........................................................................  316\n",
      "---------------\n",
      "Azure Functions use cases  .......................................................................................  316\n",
      "Types of Azure functions ..........................................................................................  318\n",
      "---------------\n",
      "Creating an event-driven function ....................................................................  318\n",
      "Function Proxies  .................................................................................................  321\n",
      "Durable Functions ...............................................................................................  322\n",
      "Steps for creating a durable function using Visual Studio  ..................................  324\n",
      "Creating a connected architecture with functions  ......................................... 329\n",
      "Azure Event Grid  ................................................................................................. 332\n",
      "Event Grid  ..................................................................................................................  333\n",
      "Resource events  ........................................................................................................  335\n",
      "---------------\n",
      "Custom events ...........................................................................................................  340\n",
      "Summary  ..............................................................................................................  343\n",
      "Chapter 11: Azure solutions using Azure Logic Apps,  \n",
      "Event Grid, and Functions   345\n",
      "Azure Logic Apps    ............................................................................................... 346\n",
      "Activities  ..................................................................................................................... 346\n",
      "Connectors  ................................................................................................................. 346\n",
      "The workings of a logic app  .....................................................................................  347\n",
      "Creating an end-to-end solution using serverless technologies  ................... 355\n",
      "---------------\n",
      "The problem statement  ...........................................................................................  355\n",
      "Solution  ......................................................................................................................  355\n",
      "Architecture  ............................................................................................................... 356\n",
      "Prerequisites  .............................................................................................................  357\n",
      "Implementation  ........................................................................................................  357\n",
      "Testing  ........................................................................................................................  385\n",
      "Summary  ..............................................................................................................  386\n",
      "---------------\n",
      "Chapter 12: Azure Big Data eventing solutions   389\n",
      "Introducing events  ..............................................................................................  390\n",
      "Event streaming  ........................................................................................................  391\n",
      "Event Hubs  ................................................................................................................. 392\n",
      "Event Hubs architecture  .................................................................................... 395\n",
      "Consumer groups  .....................................................................................................  402\n",
      "Throughput  ................................................................................................................ 403\n",
      "A primer on Stream Analytics  ...........................................................................  403\n",
      "---------------\n",
      "The hosting environment  ........................................................................................  407\n",
      "Streaming units  .........................................................................................................  408\n",
      "A sample application using Event Hubs and Stream Analytics  ..................... 408\n",
      "Provisioning a new resource group  .................................................................. 408\n",
      "Creating an Event Hubs namespace  ....................................................................... 409\n",
      "Creating an event hub  ..............................................................................................  410\n",
      "Provisioning a logic app  ...........................................................................................  411\n",
      "Provisioning the storage account  ...........................................................................  413\n",
      "---------------\n",
      "Creating a storage container  ................................................................................... 413\n",
      "Creating Stream Analytics jobs  ...............................................................................  414\n",
      "Running the application  ........................................................................................... 416\n",
      "Summary  ..............................................................................................................  418\n",
      "---------------\n",
      "Chapter 13: Integrating Azure DevOps   421\n",
      "DevOps  ................................................................................................................. 422\n",
      "The essence of DevOps  ...................................................................................... 425\n",
      "DevOps practices  ................................................................................................ 427\n",
      "Configuration management  ....................................................................................  428\n",
      "Configuration management tools  ..........................................................................  429\n",
      "Continuous integration  ............................................................................................  430\n",
      "Continuous deployment  ..........................................................................................  433\n",
      "Continuous delivery  ..................................................................................................  435\n",
      "---------------\n",
      "Continuous learning  ................................................................................................. 435\n",
      "Azure DevOps  ......................................................................................................  436\n",
      "TFVC  ............................................................................................................................  439\n",
      "Git  ................................................................................................................................ 439\n",
      "Preparing for DevOps  .........................................................................................  440\n",
      "Azure DevOps organizations  ...................................................................................  441\n",
      "Provisioning Azure Key Vault  ...................................................................................  442\n",
      "Provisioning a configuration-management server/service  ................................. 442\n",
      "---------------\n",
      "Log Analytics  ..............................................................................................................  443\n",
      "Azure Storage accounts  ...........................................................................................  443\n",
      "Docker and OS images  .............................................................................................  443\n",
      "Management tools  .................................................................................................... 443\n",
      "DevOps for PaaS solutions .................................................................................  444\n",
      "Azure App Service  .....................................................................................................  445\n",
      "Deployment slots  ...................................................................................................... 445\n",
      "Azure SQL  ...................................................................................................................  446\n",
      "---------------\n",
      "The build and release pipelines  ..............................................................................  446\n",
      "---------------\n",
      "DevOps for IaaS  ................................................................................................... 458\n",
      "Azure virtual machines  ............................................................................................  458\n",
      "Azure public load balancers  ....................................................................................  459\n",
      "The build pipeline  .....................................................................................................  459\n",
      "The release pipeline  .................................................................................................  460\n",
      "DevOps with containers  ..................................................................................... 462\n",
      "Containers ..................................................................................................................  462\n",
      "The build pipeline  .....................................................................................................  463\n",
      "---------------\n",
      "The release pipeline  .................................................................................................  463\n",
      "Azure DevOps and Jenkins  ................................................................................. 464\n",
      "Azure Automation  ............................................................................................... 466\n",
      "Provisioning an Azure Automation account  ..........................................................  467\n",
      "Creating a DSC configuration  ..................................................................................  468\n",
      "Importing the DSC configuration  ............................................................................  469\n",
      "Compiling the DSC configuration  ............................................................................  470\n",
      "Assigning configurations to nodes  .........................................................................  470\n",
      "---------------\n",
      "Validation  ................................................................................................................... 471\n",
      "Tools for DevOps  .................................................................................................  471\n",
      "Summary  ..............................................................................................................  473\n",
      "Chapter 14: Architecting Azure Kubernetes solutions   475\n",
      "Introduction to containers  ................................................................................  476\n",
      "Kubernetes fundamentals  .................................................................................  477\n",
      "Kubernetes architecture  ....................................................................................  479\n",
      "Kubernetes clusters  .................................................................................................. 480\n",
      "---------------\n",
      "Kubernetes components ..........................................................................................  481\n",
      "---------------\n",
      "Kubernetes primitives  ........................................................................................  484\n",
      "Pod  ..............................................................................................................................  485\n",
      "Services  ......................................................................................................................  486\n",
      "Deployments  .............................................................................................................  488\n",
      "Replication controller and ReplicaSet  ....................................................................  490\n",
      "ConfigMaps and Secrets  ........................................................................................... 491\n",
      "AKS architecture  ................................................................................................. 492\n",
      "Deploying an AKS cluster  ................................................................................... 493\n",
      "---------------\n",
      "Creating an AKS cluster  ............................................................................................ 493\n",
      "Kubectl  .......................................................................................................................  495\n",
      "Connecting to the cluster .........................................................................................  495\n",
      "AKS networking  ................................................................................................... 500\n",
      "Kubenet  ...................................................................................................................... 501\n",
      "Azure CNI (advanced networking)  ..........................................................................  503\n",
      "Access and identity for AKS  ............................................................................... 504\n",
      "Virtual kubelet  .....................................................................................................  505\n",
      "---------------\n",
      "Virtual nodes  ....................................................................................................... 506\n",
      "Summary  ..............................................................................................................  507\n",
      "Chapter 15: Cross-subscription deployments using  \n",
      "ARM templates   509\n",
      "ARM templates  ....................................................................................................  510\n",
      "Deploying resource groups with ARM templates  ........................................... 513\n",
      "Deploying ARM templates  .......................................................................................  515\n",
      "Deployment of templates using Azure CLI  ............................................................  516\n",
      "Deploying resources across subscriptions and resource groups  ................. 517\n",
      "Another example of cross-subscription and resource group deployments  ......  519\n",
      "---------------\n",
      "Deploying cross-subscription and resource group deployments  \n",
      "using linked templates  ....................................................................................... 522\n",
      "Virtual machine solutions using ARM templates  ............................................  526\n",
      "PaaS solutions using ARM templates  ...............................................................  532\n",
      "Data-related solutions using ARM templates  .................................................  534\n",
      "Creating an IaaS solution on Azure with Active Directory and DNS  ............. 541\n",
      "Summary  ..............................................................................................................  545\n",
      "Chapter 16: ARM template modular design  \n",
      "and implementation   547\n",
      "Problems with the single template approach  ................................................. 548\n",
      "Reduced flexibility in changing templates  ............................................................. 548\n",
      "---------------\n",
      "Troubleshooting large templates  ...........................................................................  548\n",
      "Dependency abuse  ...................................................................................................  549\n",
      "Reduced agility  ..........................................................................................................  549\n",
      "No reusability  ............................................................................................................  549\n",
      "Understanding the Single Responsibility Principle  .........................................  550\n",
      "Faster troubleshooting and debugging  .................................................................. 550\n",
      "Modular templates  ...................................................................................................  550\n",
      "Deployment resources  ............................................................................................. 551\n",
      "---------------\n",
      "Linked templates  ................................................................................................  552\n",
      "Nested templates  ...............................................................................................  554\n",
      "Free-flow configurations  .................................................................................... 556\n",
      "Known configurations  ........................................................................................ 556\n",
      "Understanding copy and copyIndex  ................................................................. 567\n",
      "Securing ARM templates  ....................................................................................  569\n",
      "Using outputs between ARM templates  ..........................................................  570\n",
      "Summary  ..............................................................................................................  573\n",
      "---------------\n",
      "Chapter 17: Designing IoT solutions   575\n",
      "IoT  ......................................................................................................................... 576\n",
      "IoT architecture  ...................................................................................................  577\n",
      "Connectivity  ............................................................................................................... 579\n",
      "Identity  ....................................................................................................................... 581\n",
      "Capture  ....................................................................................................................... 581\n",
      "Ingestion  ....................................................................................................................  581\n",
      "Storage  .......................................................................................................................  582\n",
      "---------------\n",
      "Transformation  .........................................................................................................  582\n",
      "Analytics  ..................................................................................................................... 582\n",
      "Presentation  .............................................................................................................. 583\n",
      "Azure IoT  ..............................................................................................................  584\n",
      "Connectivity  ............................................................................................................... 584\n",
      "Identity  ....................................................................................................................... 585\n",
      "Capture  ....................................................................................................................... 585\n",
      "---------------\n",
      "Ingestion  ....................................................................................................................  585\n",
      "Storage  .......................................................................................................................  586\n",
      "Transformation and analytics  .................................................................................  586\n",
      "Presentation  .............................................................................................................. 587\n",
      "Azure IoT Hub  ...................................................................................................... 588\n",
      "Protocols  ....................................................................................................................  589\n",
      "Device registration ....................................................................................................  589\n",
      "---------------\n",
      "Message management  ............................................................................................. 590\n",
      "Security  ......................................................................................................................  593\n",
      "Scalability  ................................................................................................................... 594\n",
      "Azure IoT Edge  ...........................................................................................................  596\n",
      "High availability  ...................................................................................................  596\n",
      "---------------\n",
      "Azure IoT Central  ................................................................................................ 597\n",
      "Summary  ..............................................................................................................  598\n",
      "Chapter 18: Azure Synapse Analytics for architects   601\n",
      "Azure Synapse Analytics  .................................................................................... 602\n",
      "A common scenario for architects  .................................................................... 603\n",
      "An overview of Azure Synapse Analytics  ......................................................... 603\n",
      "What is workload isolation? .....................................................................................  604\n",
      "Introduction to Synapse workspaces and Synapse Studio  .................................. 605\n",
      "Apache Spark for Synapse  .......................................................................................  607\n",
      "---------------\n",
      "Synapse SQL  ..............................................................................................................  608\n",
      "Synapse pipelines  .....................................................................................................  609\n",
      "Azure Synapse Link for Cosmos DB  ........................................................................ 610\n",
      "Migrating from existing legacy systems to Azure Synapse Analytics  ........... 611\n",
      "Why you should migrate your legacy data warehouse to  \n",
      "Azure Synapse Analytics  ..........................................................................................  611\n",
      "The three-step migration process  ..........................................................................  613\n",
      "The two types of migration strategies  ...................................................................  614\n",
      "Reducing the complexity of your existing legacy data warehouse\n",
      "---------------\n",
      "before migrating  .......................................................................................................  615\n",
      "Converting physical data marts to virtual data marts  ......................................... 615\n",
      "Migrating existing data warehouse schemas to Azure Synapse Analytics  ........ 616\n",
      "Migrating historical data from your legacy data warehouse to  \n",
      "Azure Synapse Analytics  ..........................................................................................  619\n",
      "Migrating existing ETL processes to Azure Synapse Analytics  ............................ 621\n",
      "Re-developing scalable ETL processes using ADF  ................................................. 622\n",
      "Recommendations for migrating queries, BI reports, dashboards,  \n",
      "and other visualizations  ........................................................................................... 622\n",
      "Common migration issues and resolutions ...........................................................  623\n",
      "---------------\n",
      "Common SQL incompatibilities and resolutions  ............................................ 625\n",
      "SQL DDL differences and resolutions  ..................................................................... 626\n",
      "SQL DML differences and resolutions  .................................................................... 627\n",
      "SQL DCL differences and resolutions  .....................................................................  627\n",
      "Extended SQL differences and workarounds  ........................................................  631\n",
      "Security considerations  ......................................................................................  632\n",
      "Data encryption at rest  ............................................................................................  632\n",
      "Data in motion  ..........................................................................................................  632\n",
      "Tools to help migrate to Azure Synapse Analytics  ..........................................  633\n",
      "---------------\n",
      "ADF  ..............................................................................................................................  633\n",
      "Azure Data Warehouse Migration Utility  ............................................................... 634\n",
      "Microsoft Services for Physical Data Transfer  ......................................................  634\n",
      "Microsoft Services for data ingestion  ..................................................................... 635\n",
      "Summary  ..............................................................................................................  636\n",
      "Chapter 19: Architecting intelligent solutions   639\n",
      "The evolution of AI   ............................................................................................. 640\n",
      "Azure AI processes  .............................................................................................. 641\n",
      "---------------\n",
      "Data ingestion  ...........................................................................................................  641\n",
      "Data transformation .................................................................................................  641\n",
      "Analysis  ......................................................................................................................  641\n",
      "Data modeling  ........................................................................................................... 642\n",
      "Validating the model  ................................................................................................  642\n",
      "Deployment  ...............................................................................................................  642\n",
      "Monitoring  .................................................................................................................  642\n",
      "---------------\n",
      "Azure Cognitive Services  ....................................................................................  643\n",
      "Vision  .......................................................................................................................... 644\n",
      "Search  ......................................................................................................................... 644\n",
      "Language  ....................................................................................................................  644\n",
      "Speech  ........................................................................................................................  644\n",
      "Decision  ......................................................................................................................  644\n",
      "Understanding Cognitive Services  ....................................................................  645\n",
      "---------------\n",
      "Consuming Cognitive Services  ................................................................................  646\n",
      "Building an OCR service  ..................................................................................... 646\n",
      "Using PowerShell  ......................................................................................................  649\n",
      "Using C#  .....................................................................................................................  650\n",
      "The development process  ........................................................................................ 652\n",
      "Building a visual features service using the Cognitive Search .NET SDK  .....  655\n",
      "Using PowerShell  ......................................................................................................  655\n",
      "Using .NET  .................................................................................................................. 656\n",
      "---------------\n",
      "Safeguarding the Cognitive Services key  ......................................................... 658\n",
      "Using Azure Functions Proxies  ................................................................................ 658\n",
      "Consuming Cognitive Services  .......................................................................... 659\n",
      "Summary  ..............................................................................................................  659\n",
      "Index   661\n",
      "---------------\n",
      "About\n",
      "This section briefly introduces the authors, the coverage of this book, the technical skills you'll \n",
      "need to get started, and the hardware and software requirements required to architect solutions \n",
      "using Azure.\n",
      "Preface\n",
      ">\n",
      "---------------\n",
      "ii | Preface\n",
      "About Azure for Architects, Third Edition\n",
      "Thanks to its support for high availability, scalability, security, performance, and \n",
      "disaster recovery, Azure has been widely adopted to create and deploy different types \n",
      "of application with ease. Updated for the latest developments, this third edition of Azure \n",
      "for Architects helps you get to grips with the core concepts of designing serverless \n",
      "architecture, including containers, Kubernetes deployments, and big data solutions. \n",
      "You'll learn how to architect solutions such as serverless functions, you'll discover \n",
      "deployment patterns for containers and Kubernetes, and you'll explore large-scale big \n",
      "data processing using Spark and Databricks. As you advance, you'll implement DevOps \n",
      "using Azure DevOps, work with intelligent solutions using Azure Cognitive Services, and \n",
      "integrate security, high availability, and scalability into each solution. Finally, you'll delve\n",
      "---------------\n",
      "into Azure security concepts such as OAuth, OpenConnect, and managed identities.\n",
      "By the end of this book, you'll have gained the confidence to design intelligent Azure \n",
      "solutions based on containers and serverless functions.\n",
      "About the Authors\n",
      "Ritesh Modi is a former Microsoft senior technology evangelist. He has been \n",
      "recognized as a Microsoft Regional Director for his contributions to Microsoft products, \n",
      "services, and communities. He is a cloud architect, a published author, a speaker, \n",
      "and a leader who is popular for his contributions to datacenters, Azure, Kubernetes, \n",
      "blockchain, cognitive services, DevOps, artificial intelligence, and automation. He is the \n",
      "author of eight books.\n",
      "Ritesh has spoken at numerous national and international conferences and is a \n",
      "published author for MSDN magazine. He has more than a decade of experience in \n",
      "building and deploying enterprise solutions for customers, and has more than 25\n",
      "---------------\n",
      "technical certifications. His hobbies are writing books, playing with his daughter, \n",
      "watching movies, and learning new technologies. He currently lives in Hyderabad, India. \n",
      "You can follow him on Twitter at @automationnext.\n",
      "Jack Lee is a senior Azure certified consultant and an Azure practice lead with a \n",
      "passion for software development, cloud, and DevOps innovations. Jack has been \n",
      "recognized as a Microsoft MVP for his contributions to the tech community. He \n",
      "has presented at various user groups and conferences, including the Global Azure \n",
      "Bootcamp at Microsoft Canada. Jack is an experienced mentor and judge at hackathons \n",
      "and is also the president of a user group that focuses on Azure, DevOps, and software \n",
      "development. He is the co-author of Cloud Analytics with Microsoft Azure, published by \n",
      "Packt Publishing. You can follow Jack on Twitter at @jlee_consulting.\n",
      "---------------\n",
      "About Azure for Architects, Third Edition | iii\n",
      "Rithin Skaria is an open source evangelist with over 7 years of experience of managing \n",
      "open source workloads in Azure, AWS, and OpenStack. He is currently working \n",
      "for Microsoft and is a part of several open source community activities conducted \n",
      "within Microsoft. He is a Microsoft Certified Trainer, Linux Foundation Certified \n",
      "Engineer and Administrator, Kubernetes Application Developer and Administrator, \n",
      "and also a Certified OpenStack Administrator. When it comes to Azure, he has four \n",
      "certifications (solution architecture, Azure administration, DevOps, and security), and \n",
      "he is also certified in Office 365 administration. He has played a vital role in several \n",
      "open source deployments, and the administration and migration of these workloads \n",
      "to the cloud. He also co-authored Linux Administration on Azure, published by Packt \n",
      "Publishing. Connect with him on LinkedIn at @rithin-skaria.\n",
      "About the Reviewers\n",
      "---------------\n",
      "Melony Qin is a woman in STEM. Currently working as a Program Manager at \n",
      "Microsoft, she's a member of the Association for Computing Machinery (ACM) and \n",
      "Project Management Institute (PMI). She has contributed to serverless computing, \n",
      "big data processing, DevOps, artificial intelligence, machine learning, and IoT with \n",
      "Microsoft Azure. She holds all the Azure certifications (both the Apps and Infrastructure \n",
      "and the Data and AI tracks) as well as Certified Kubernetes Administrator (CKA) and \n",
      "Certified Kubernetes Application Developer (CKAD), and is mainly working on her \n",
      "contributions to open-source software (OSS), DevOps, Kubernetes, serverless, big data \n",
      "analytics, and IoT on Microsoft Azure in the community. She's the author and co-author \n",
      "of two books, Microsoft Azure Infrastructure and The Kubernetes Workshop, both \n",
      "published by Packt Publishing. She can be reached out via Twitter at @MelonyQ.\n",
      "Sanjeev Kumar is a Cloud Solution Architect for SAP on Azure at Microsoft. He\n",
      "---------------\n",
      "is currently based in Zurich, Switzerland. He has worked with SAP technology \n",
      "for over 19 years. He has been working with public cloud technologies for about 8 years, \n",
      "the last 2 years of which have been focused on Microsoft Azure. \n",
      "In his SAP on Azure cloud architecture advisory role, Sanjeev Kumar has worked \n",
      "with a number of the world's top financial services and manufacturing companies. His \n",
      "focus areas include cloud architecture and design to help customers migrate their \n",
      "SAP systems to Azure and adopt Azure best practices for SAP deployments, especially \n",
      "by implementing Infrastructure as Code and DevOps. He has also worked in the \n",
      "areas of containerization and microservices using Docker and Azure Kubernetes \n",
      "Service, streaming data processing using Apache Kafka, and full stack application \n",
      "development using Node.js. He has worked on various product development \n",
      "initiatives spanning IaaS, PaaS, and SaaS. He is also interested in the emerging\n",
      "---------------\n",
      "topics of artificial intelligence, machine learning, and large-scale data processing \n",
      "and analytics. He writes on topics related to SAP on Azure, DevOps, and Infrastructure \n",
      "as Code on LinkedIn, where you can find him at @sanjeevkumarprofile.\n",
      "---------------\n",
      "iv | Preface\n",
      "Learning Objectives\n",
      "By the end of this book, you will be able to:\n",
      "• Understand the components of the Azure cloud platform\n",
      "• Use cloud design patterns\n",
      "• Use enterprise security guidelines for your Azure deployment\n",
      "• Design and implement serverless and integration solutions\n",
      "• Build efficient data solutions on Azure\n",
      "• Understand container services on Azure\n",
      "Audience\n",
      "If you are a cloud architect, DevOps engineer, or a developer looking to learn about \n",
      "the key architectural aspects of the Azure cloud platform, this book is for you. A basic \n",
      "understanding of the Azure cloud platform will help you grasp the concepts covered in \n",
      "this book more effectively.\n",
      "Approach\n",
      "This book covers each topic with step-by-step explanations of essential concepts, \n",
      "practical examples, and self-assessment questions. By providing a balance of theory \n",
      "and practical experience of working through engaging projects, this book will help you \n",
      "understand how architects work in the real world.\n",
      "---------------\n",
      "Hardware Requirements\n",
      "For the optimal experience, we recommend the following configuration:\n",
      "• Minimum 4 GB RAM\n",
      "• Minimum 32 GB of free memory\n",
      "---------------\n",
      "About Azure for Architects, Third Edition | v\n",
      "Software Requirements\n",
      "• Visual Studio 2019\n",
      "• Docker for Windows latest version\n",
      "• AZ PowerShell module 1.7 and above\n",
      "• Azure CLI latest version\n",
      "• Azure subscription\n",
      "• Windows Server 2016/2019\n",
      "• Window 10 latest version - 64 bit\n",
      "Conventions\n",
      "Code words in the text, database table names, folder names, filenames, file extensions, \n",
      "pathnames, dummy URLs, and user inputs are shown as follows:\n",
      "The DSC configuration still isn't known to Azure Automation. It's available on some \n",
      "local machines. It should be uploaded to Azure Automation DSC Configurations. \n",
      "Azure Automation provides the Import-AzureRMAutomationDscConfiguration cmdlet to \n",
      "import the configuration to Azure Automation:\n",
      "Import-AzureRmAutomationDscConfiguration -SourcePath \"C:\\DSC\\AA\\DSCfiles\\\n",
      "ConfigureSiteOnIIS.ps1\" -ResourceGroupName \"omsauto\" -AutomationAccountName \n",
      "\"datacenterautomation\" -Published -Verbose\n",
      "Download Resources\n",
      "---------------\n",
      "The code bundle for this book is also hosted on GitHub at: https:/ /github.com/\n",
      "PacktPublishing/Azure-for-Architects-Third-Edition.\n",
      "We also have other code bundles from our rich catalog of books and videos available at \n",
      "https:/ /github.com/PacktPublishing Check them out!\n",
      "---------------\n",
      "Every few years, a technological innovation emerges that permanently changes the \n",
      "entire landscape and ecosystem around it. If we go back in time, the 1970s and 1980s \n",
      "were the time of mainframes. These mainframes were massive, often occupying large \n",
      "rooms, and were solely responsible for almost all computing work. Since the technology \n",
      "was difficult to procure and time-consuming to use, many enterprises used to place \n",
      "orders for mainframes one month in advance before they could have an operational \n",
      "mainframe set up.\n",
      "Then, the early 1990s witnessed a boom in demand for personal computing and the \n",
      "internet. As a result, computers became much smaller in size and comparatively easy \n",
      "to procure for the general public. Consistent innovations on the personal computing \n",
      "and internet fronts eventually changed the entire computer industry. Many people had \n",
      "desktop computers that were capable of running multiple programs and connecting\n",
      "---------------\n",
      "to the internet. The rise of the internet also propagated the rise of client-server \n",
      "deployments. Now there could be centralized servers hosting applications, and services \n",
      "could be reached by anyone who had a connection to the internet anywhere on the \n",
      "globe. This was also a time when server technology gained prominence; Windows NT \n",
      "was released during this time and was soon followed by Windows 2000 and Windows \n",
      "2003 at the turn of the century.\n",
      "Getting started \n",
      "with Azure\n",
      "1\n",
      "---------------\n",
      "2 | Getting started with Azure\n",
      "The most remarkable innovation of the 2000s was the rise and adoption of portable \n",
      "devices, especially smartphones, and with these came a plethora of apps. Apps could \n",
      "connect to centralized servers on the internet and carry out business as usual. Users \n",
      "were no longer dependent on browsers to do this work; all servers were either self-\n",
      "hosted or hosted using a service provider, such as an internet service provider (ISP).\n",
      "Users did not have much control over their servers. Multiple customers and their \n",
      "deployments were part of the same server, even without customers knowing about it.\n",
      "However, something else happened in the middle and latter parts of the first decade \n",
      "of the 2000s. This was the rise of cloud computing, and it again rewrote the entire \n",
      "landscape of the IT industry. Initially, adoption was slow, and people approached it with \n",
      "caution, either because the cloud was in its infancy and still had to mature, or because\n",
      "---------------\n",
      "people had various negative notions about what it was.\n",
      "To gain a better understanding of the disruptive technology, we will cover the following \n",
      "topics in this chapter:\n",
      "• Cloud computing\n",
      "• Infrastructure as a service (IaaS), platform as a service (PaaS), and Software as a \n",
      "service (SaaS)\n",
      "• Understanding Azure\n",
      "• Azure Resource Manager (ARM)\n",
      "• Virtualization, containers, and Docker\n",
      "• Interacting with the intelligent cloud\n",
      "Cloud computing\n",
      "Today, cloud computing is one of the most promising upcoming technologies, and \n",
      "enterprises, no matter how big or small, are adopting it as a part of their IT strategy. It \n",
      "is difficult these days to have any meaningful conversation about an IT strategy without \n",
      "including cloud computing in the overall solution discussions.\n",
      "Cloud computing, or simply the cloud in layman's terms, refers to the availability of \n",
      "resources on the internet. These resources are made available to users on the internet\n",
      "---------------\n",
      "as services. For example, storage is available on-demand through the internet for users \n",
      "to store their files, documents, and more. Here, storage is a service that is offered by a \n",
      "cloud provider.\n",
      "---------------\n",
      "Cloud computing | 3\n",
      "A cloud provider is an enterprise or consortium of companies that provides cloud \n",
      "services to other enterprises and consumers. They host and manage these services \n",
      "on behalf of the user. They are responsible for enabling and maintaining the health of \n",
      "services. There are large datacenters across the globe that have been opened by cloud \n",
      "providers to cater to the IT demands of users.\n",
      "Cloud resources consist of hosting services on on-demand infrastructures, such as \n",
      "computing infrastructures, networks, and storage facilities. This flavor of the cloud is \n",
      "known as IaaS.\n",
      "The advantages of cloud computing\n",
      "Cloud adoption is at an all-time high and is growing because of several advantages, such \n",
      "as these:\n",
      "• Pay-as-you-go model: Customers do not need to purchase hardware and software \n",
      "for cloud resources. There is no capital expenditure for using a cloud resource; \n",
      "customers simply pay for the time that they use or reserve a resource.\n",
      "---------------\n",
      "• Global access: Cloud resources are available globally through the internet. \n",
      "Customers can access their resources on-demand from anywhere.\n",
      "• Unlimited resources: The scaling capability of cloud technology is unlimited; \n",
      "customers can provision as many resources as they want, without any constraints. \n",
      "This is also known as unlimited scalability.\n",
      "• Managed services: The cloud provider provides numerous services that are \n",
      "managed by them for customers. This takes away any technical and financial \n",
      "burden from the customer.\n",
      "Why cloud computing?\n",
      "To understand the need for cloud computing, we must understand the industry's \n",
      "perspective.\n",
      "---------------\n",
      "4 | Getting started with Azure\n",
      "Flexibility and agility\n",
      "Instead of creating a large monolithic application using a big-bang approach \n",
      "deployment methodology, today, applications comprise smaller services using the \n",
      "microservices paradigm. Microservices help to create services in an independent \n",
      "and autonomous manner that can be evolved in isolation without bringing the entire \n",
      "application down. They offer large amounts of flexibility and agility in bringing changes \n",
      "to production in a faster and better way. There are many microservices that come \n",
      "together to create an application and provide integrated solutions for customers. These \n",
      "microservices should be discoverable and have well-defined endpoints for integration. \n",
      "The number of integrations with the microservices approach is very high compared \n",
      "to traditional monolithic applications. These integrations add complexity in both the \n",
      "development and deployment of applications. \n",
      "Speed, standardization, and consistency\n",
      "---------------\n",
      "It follows that the methodology for deployments should also undergo changes to adapt \n",
      "to the needs of these services, that is, frequent changes and frequent deployments. \n",
      "For frequent changes and deployments, it is important to use processes that help in \n",
      "bringing about these changes in a predictable and consistent manner. Automated agile \n",
      "processes should be used such that smaller changes can be deployed and tested in \n",
      "isolation.\n",
      "Staying relevant\n",
      "Finally, deployment targets should be redefined. Not only should deployment targets \n",
      "be easily creatable within seconds, but also the environment built should be consistent \n",
      "across versions, with appropriate binaries, runtimes, frameworks, and configuration. \n",
      "Virtual machines were used with monolithic applications but microservices need more \n",
      "agility, flexibility, and a more lightweight option than virtual machines. Container \n",
      "technology is the preferred mechanism for deployment targets for these services, and\n",
      "---------------\n",
      "we will cover more about that later in this chapter.\n",
      "Scalability\n",
      "Some important tenets of using microservices are that they have an unlimited scaling \n",
      "capability in isolation, global high availability, disaster recovery with a near-zero \n",
      "recovery point, and time objectives. These qualities of microservices necessitate \n",
      "infrastructure that can scale in an unlimited fashion. There should not be any resource \n",
      "constraints. While this is the case, it is also important that an organization does not pay \n",
      "for resources up front when they are not utilized.\n",
      "---------------\n",
      "Cloud computing | 5\n",
      "Cost-effectiveness\n",
      "Paying for resources that are being consumed and using them optimally by increasing \n",
      "and decreasing the resource counts and capacity automatically is the fundamental \n",
      "tenet of cloud computing. These emerging application requirements demand the cloud \n",
      "as the preferred platform to scale easily, be highly available, be disaster-resistant, bring \n",
      "in changes easily, and achieve predictable and consistent automated deployments in a \n",
      "cost-effective manner.\n",
      "Deployment paradigms in Azure\n",
      "There are three different deployment patterns that are available in Azure; they are as \n",
      "follows:\n",
      "• IaaS\n",
      "• PaaS\n",
      "• SaaS\n",
      "The difference between these three deployment patterns is the level of control that \n",
      "is exercised by customers via Azure. Figure 1.1 displays the different levels of control \n",
      "within each of these deployment patterns:\n",
      "Figure 1.1: Cloud services—IaaS, PaaS, and SaaS\n",
      "IaaS PaaS SaaS\n",
      "Applications Applications Applications\n",
      "Data Data Data\n",
      "Runtime Runtime Runtime\n",
      "---------------\n",
      "Middleware Middleware Middleware\n",
      "OS OS OS\n",
      "Virtualization Virtualization Virtualization\n",
      "Servers Servers Servers\n",
      "Storage Storage Storage\n",
      "Networking Networking Networking\n",
      "Managed by Consumer Managed by Vendor\n",
      "---------------\n",
      "6 | Getting started with Azure\n",
      "It is clear from Figure 1.1 that customers have more control when using IaaS \n",
      "deployments, and this level of control continually decreases as we progress from PaaS \n",
      "to SaaS deployments. \n",
      "IaaS\n",
      "IaaS is a type of deployment model that allows customers to provision their own \n",
      "infrastructure on Azure. Azure provides several infrastructure resources and customers \n",
      "can provision them on-demand. Customers are responsible for maintaining and \n",
      "governing their own infrastructure. Azure will ensure the maintenance of the physical \n",
      "infrastructure on which these virtual infrastructure resources are hosted. Under \n",
      "this approach, customers require active management and operations in the Azure \n",
      "environment.\n",
      "PaaS\n",
      "PaaS takes away infrastructure deployment and control from the customer. This is a \n",
      "higher-level abstraction compared to IaaS. In this approach, customers bring their own \n",
      "application, code, and data, and deploy them on the Azure-provided platform. These\n",
      "---------------\n",
      "platforms are managed and governed by Azure and customers are solely responsible for \n",
      "their applications. Customers perform activities related to their application deployment \n",
      "only. This model provides faster and easier options for the deployment of applications \n",
      "compared to IaaS.\n",
      "SaaS\n",
      "SaaS is a higher-level abstraction compared to PaaS. In this approach, software and its \n",
      "services are available for customer consumption. Customers only bring their data into \n",
      "these services—they do not have any control over these services. Now that we have a \n",
      "basic understanding of service types in Azure, let's get into the details of Azure and \n",
      "understand it from the ground up.\n",
      "Understanding Azure\n",
      "Azure provides all the benefits of the cloud while remaining open and flexible. Azure \n",
      "supports a wide variety of operating systems, languages, tools, platforms, utilities, \n",
      "and frameworks. For example, it supports Linux and Windows, SQL Server, MySQL,\n",
      "---------------\n",
      "and PostgreSQL. It supports most of the programming languages, including C#, \n",
      "Python, Java, Node.js, and Bash. It supports NoSQL databases, such as MongoDB and \n",
      "Cosmos DB, and it also supports continuous integration tools, such as Jenkins and \n",
      "Azure DevOps Services (formerly Visual Studio Team Services (VSTS)). The whole \n",
      "idea behind this ecosystem is to enable customers to have the freedom to choose their \n",
      "own language, platform, operating system, database, storage, and tools and utilities. \n",
      "Customers should not be constrained from a technology perspective; instead, they \n",
      "should be able to build and focus on their business solution, and Azure provides them \n",
      "with a world-class technology stack that they can use.\n",
      "---------------\n",
      "Understanding Azure | 7\n",
      "Azure is very much compatible with the customer's choice of technology stack. \n",
      "For example, Azure supports all popular (open-source and commercial) database \n",
      "environments. Azure provides Azure SQL, MySQL, and Postgres PaaS services. It \n",
      "provides the Hadoop ecosystem and offers HDInsight, a 100% Apache Hadoop–based \n",
      "PaaS. It also provides a Hadoop on Linux virtual machine (VM) implementation for \n",
      "customers who prefer the IaaS approach. Azure also provides the Redis Cache service \n",
      "and supports other popular database environments, such as Cassandra, Couchbase, and \n",
      "Oracle as an IaaS implementation.\n",
      "The number of services is increasing by the day in Azure and the most up-to-date list of \n",
      "services can be found at https:/ /azure.microsoft.com/services.\n",
      "Azure also provides a unique cloud computing paradigm known as the hybrid cloud. The \n",
      "hybrid cloud refers to a deployment strategy in which a subset of services is deployed\n",
      "---------------\n",
      "on a public cloud, while other services are deployed on an on-premises private cloud \n",
      "or datacenter. There is a virtual private network (VPN) connection between the public \n",
      "and private clouds. Azure offers customers the flexibility to divide and deploy their \n",
      "workload on both the public cloud and an on-premises datacenter. \n",
      "Azure has datacenters across the globe and combines these datacenters into regions. \n",
      "Each region has multiple datacenters to ensure that recovery from disasters is quick \n",
      "and efficient. At the time of writing, there are 58 regions across the globe. This provides \n",
      "customers with the flexibility to deploy their services in their choice of location. They \n",
      "can also combine these regions to deploy a solution that is disaster-resistant and \n",
      "deployed near their customer base.\n",
      "Note\n",
      "In China and Germany, the Azure Cloud Services are separate for general use \n",
      "and for governmental use. This means that the cloud services are maintained in \n",
      "separate datacenters.\n",
      "---------------\n",
      "8 | Getting started with Azure\n",
      "Azure as an intelligent cloud\n",
      "Azure provides infrastructure and services to ingest billions of transactions using \n",
      "hyper-scale processing. It provides petabytes of storage for data, and it provides a host \n",
      "of interconnected services that can pass data among themselves. With such capabilities \n",
      "in place, data can be processed to generate meaningful knowledge and insights. There \n",
      "are multiple types of insights that can be generated through data analysis, which are as \n",
      "follows:\n",
      "• Descriptive: This type of analysis provides details about what is happening or has \n",
      "happened in the past.\n",
      "• Predictive: This type of analysis provides details about what is going to happen in \n",
      "the future.\n",
      "• Prescriptive: This type of analysis provides details about what should be done to \n",
      "either enhance or prevent current or future events.\n",
      "• Cognitive: This type of analysis actually executes actions that are determined by \n",
      "prescriptive analytics in an automated manner.\n",
      "---------------\n",
      "While deriving insights from data is good, it is equally important to act on them. Azure \n",
      "provides a rich platform to ingest large volumes of data, process and transform it, \n",
      "store and generate insights from it, and display it on real-time dashboards. It is also \n",
      "possible to take action on these insights automatically. These services are available to \n",
      "every customer of Azure and they provide a rich ecosystem in which customers can \n",
      "create solutions. Enterprises are creating numerous applications and services that are \n",
      "completely disrupting industries because of the easy availability of these intelligent \n",
      "services from Azure, which are combined to create meaningful value for end customers. \n",
      "Azure ensures that services that are commercially not viable to implement for small and \n",
      "medium companies can now be readily consumed and deployed in a few minutes.\n",
      "Azure Resource Manager\n",
      "Azure Resource Manager (ARM) is the technology platform and orchestration service\n",
      "---------------\n",
      "from Microsoft that ties up all the components that were discussed earlier. It brings \n",
      "Azure's resource providers, resources, and resource groups together to form a cohesive \n",
      "cloud platform. It makes Azure services available as subscriptions, resource types \n",
      "available to resource groups, and resource and resource APIs accessible to the portal \n",
      "and other clients, and it authenticates access to these resources. It also enables features \n",
      "such as tagging, authentication, role-based access control (RBAC), resource locking, \n",
      "and policy enforcement for subscriptions and their resource groups. It also provides \n",
      "deployment and management features using the Azure portal, Azure PowerShell, and \n",
      "command-line interface (CLI) tools.\n",
      "---------------\n",
      "Azure Resource Manager | 9\n",
      "The ARM architecture\n",
      "The architecture of ARM and its components is shown in Figure 1.2. As we can see, an \n",
      "Azure Subscription comprises multiple resource groups. Each resource group contains \n",
      "resource instances that are created from resource types that are available in the \n",
      "resource provider:\n",
      "Figure 1.2: The ARM architecture\n",
      "Why ARM?\n",
      "Prior to ARM, the framework used by Azure was known as Azure Service Manager \n",
      "(ASM). It is important to have a small introduction to it so that we can get a clear \n",
      "understanding of the emergence of ARM and the slow and steady deprecation of ASM. \n",
      "Resource Group\n",
      "Microsoft Azure\n",
      "LOB App\n",
      "Azure Subscription\n",
      "laas Workload\n",
      "REST API Endpoints\n",
      "Resource Group\n",
      "Resource Providers\n",
      "Azure Resource Manager (ARM)\n",
      "Azure\n",
      "Azure App\n",
      "Service\n",
      "Azure \n",
      "Storage\n",
      "Azure SQL\n",
      "Database\n",
      "Azure VM\n",
      "Azure MySQL \n",
      "ClearDB \n",
      "Database\n",
      "Azure \n",
      "Virtual \n",
      "Network\n",
      "Azure \n",
      "Storage\n",
      "---------------\n",
      "10 | Getting started with Azure\n",
      "Limitations of ASM\n",
      "ASM has inherent constraints. For example, ASM deployments are slow and blocking—\n",
      "operations are blocked if an earlier operation is already in progress. Some of the \n",
      "limitations of ASM are as follows:\n",
      "• Parallelism: Parallelism is a challenge in ASM. It is not possible to execute multiple \n",
      "transactions successfully in parallel. The operations in ASM are linear and so they \n",
      "are executed one after another. If multiple transactions are executed at the same \n",
      "time, there will either be parallel operation errors or the transactions will get \n",
      "blocked.\n",
      "• Resources: Resources in ASM are provisioned and managed in isolation of each \n",
      "other; there is no relation between ASM resources. Grouping services and \n",
      "resources or configuring them together is not possible.\n",
      "• Cloud services: Cloud services are the units of deployment in ASM. They are \n",
      "reliant on affinity groups and are not scalable due to their design and architecture.\n",
      "---------------\n",
      "Granular and discrete roles and permissions cannot be assigned to resources in ASM. \n",
      "Customers are either service administrators or co-administrators in the subscription. \n",
      "They either get full control over resources or do not have access to them at all. ASM \n",
      "provides no deployment support. Either deployments are done manually, or we need to \n",
      "resort to writing procedural scripts in .NET or PowerShell. ASM APIs are not consistent \n",
      "between resources.\n",
      "ARM advantages\n",
      "ARM provides distinct advantages and benefits over ASM, which are as follows:\n",
      "• Grouping: ARM allows the grouping of resources together in a logical container. \n",
      "These resources can be managed together and go through a common life cycle as \n",
      "a group. This makes it easier to identify related and dependent resources.\n",
      "• Common life cycles: Resources in a group have the same life cycle. These \n",
      "resources can evolve and be managed together as a unit.\n",
      "• RBAC: Granular roles and permissions can be assigned to resources providing\n",
      "---------------\n",
      "discrete access to customers. Customers can also have only those rights that are \n",
      "assigned to them.\n",
      "---------------\n",
      "Azure Resource Manager | 11\n",
      "• Deployment support: ARM provides deployment support in terms of templates, \n",
      "enabling DevOps and infrastructure as code (IaC). These deployments are faster, \n",
      "consistent, and predictable.\n",
      "• Superior technology: The cost and billing of resources can be managed as a unit. \n",
      "Each resource group can provide its usage and cost information.\n",
      "• Manageability: ARM provides advanced features, such as security, monitoring, \n",
      "auditing, and tagging, for better manageability of resources. Resources can be \n",
      "queried based on tags. Tags also provide cost and billing information for resources \n",
      "that are tagged similarly.\n",
      "• Migration: Migration and updating resources is easier within and across resource \n",
      "groups.\n",
      "ARM concepts\n",
      "With ARM, everything in Azure is a resource. Examples of resources are VMs, network \n",
      "interfaces, public IP addresses, storage accounts, and virtual networks. ARM is \n",
      "based on concepts that are related to resource providers and resource consumers.\n",
      "---------------\n",
      "Azure provides resources and services through multiple resource providers that are \n",
      "consumed and deployed in groups.\n",
      "Resource providers\n",
      "These are services that are responsible for providing resource types through ARM. \n",
      "The top-level concept in ARM is the resource provider. These providers are containers \n",
      "for resource types. Resource types are grouped into resource providers. They are \n",
      "responsible for deploying and managing resources. For example, a VM resource \n",
      "type is provided by a resource provider called Microsoft.Compute/virtualMachines \n",
      "resource. Representational state transfer (REST) API operations are versioned to \n",
      "distinguish between them. The version naming is based on the dates on which they \n",
      "are released by Microsoft. It is necessary for a related resource provider to be available \n",
      "to a subscription to deploy a resource. Not all resource providers are available to a \n",
      "subscription out of the box. If a resource is not available to a subscription, then we\n",
      "---------------\n",
      "need to check whether the required resource provider is available in each region. If it is \n",
      "available, the customer can explicitly register for the subscription.\n",
      "---------------\n",
      "12 | Getting started with Azure\n",
      "Resource types\n",
      "Resource types are an actual resource specification defining the resource's public API \n",
      "interface and implementation. They implement the working and operations supported \n",
      "by the resource. Similar to resource providers, resource types also evolve over time \n",
      "in terms of their internal implementation, and there are multiple versions of their \n",
      "schemas and public API interfaces. The version names are based on the dates that they \n",
      "are released by Microsoft as a preview or general availability (GA). The resource types \n",
      "become available as a subscription after a resource provider is registered to them. Also, \n",
      "not every resource type is available in every Azure region. The availability of a resource \n",
      "is dependent on the availability and registration of a resource provider in an Azure \n",
      "region and must support the API version needed for provisioning it.\n",
      "Resource groups\n",
      "Resource groups are units of deployment in ARM. They are containers grouping\n",
      "---------------\n",
      "multiple resource instances in a security and management boundary. A resource group \n",
      "is uniquely named in a subscription. Resources can be provisioned on different Azure \n",
      "regions and yet belong to the same resource group. Resource groups provide additional \n",
      "services to all the resources within them. Resource groups provide metadata services, \n",
      "such as tagging, which enables the categorization of resources; the policy-based \n",
      "management of resources; RBAC; the protection of resources from accidental deletion \n",
      "or updates; and more. As mentioned before, they have a security boundary, and users \n",
      "that don't have access to a resource group cannot access resources contained within it. \n",
      "Every resource instance needs to be part of a resource group; otherwise, it cannot be \n",
      "deployed.\n",
      "Resources and resource instances\n",
      "Resources are created from resource types and are an instance of a resource type. An \n",
      "instance can be unique globally or at a resource group level. The uniqueness is defined\n",
      "---------------\n",
      "by both the name of the resource and its type. If we compare this with object-oriented \n",
      "programming constructs, resource instances can be seen as objects and resource \n",
      "types can be seen as classes. The services are consumed through the operations that \n",
      "are supported and implemented by resource instances. The resource type defines \n",
      "properties and each instance should configure mandatory properties during the \n",
      "provisioning of an instance. Some are mandatory properties, while others are optional. \n",
      "They inherit the security and access configuration from their parent resource group. \n",
      "These inherited permissions and role assignments can be overridden for each resource. \n",
      "A resource can be locked in such a way that some of its operations can be blocked \n",
      "and not made available to roles, users, and groups even though they have access to it. \n",
      "Resources can be tagged for easy discoverability and manageability.\n",
      "---------------\n",
      "Azure Resource Manager | 13\n",
      "ARM features\n",
      "Here are some of the main features that are provided by ARM:\n",
      "• RBAC: Azure Active Directory (Azure AD) authenticates users to provide access to \n",
      "subscriptions, resource groups, and resources. ARM implements OAuth and RBAC \n",
      "within the platform, enabling authorization and access control for resources, \n",
      "resource groups, and subscriptions based on roles assigned to a user or group. \n",
      "A permission defines access to the operations in a resource. These permissions \n",
      "can allow or deny access to the resource. A role definition is a collection of these \n",
      "permissions. Roles map Azure AD users and groups to particular permissions. \n",
      "Roles are subsequently assigned to a scope; this can be an individual, a collection \n",
      "of resources, a resource group, or the subscription. The Azure AD identities (users, \n",
      "groups, and service principals) that are added to a role gain access to the resource\n",
      "---------------\n",
      "according to the permissions defined in the role. ARM provides multiple out-of-\n",
      "the-box roles. It provides system roles, such as the owner, contributor, and \n",
      "reader. It also provides resource-based roles, such as SQL DB contributor and VM \n",
      "contributor. ARM also allows the creation of custom roles.\n",
      "• Tags: Tags are name-value pairs that add additional information and metadata \n",
      "to resources. Both resources and resource groups can be tagged with multiple \n",
      "tags. Tags help in the categorization of resources for better discoverability and \n",
      "manageability. Resources can be quickly searched for and easily identified. Billing \n",
      "and cost information can also be fetched for resources that have the same tags. \n",
      "While this feature is provided by ARM, an IT administrator defines its usage and \n",
      "taxonomy with regard to resources and resource groups. Taxonomy and tags, for \n",
      "example, can relate to departments, resource usage, location, projects, or any\n",
      "---------------\n",
      "other criteria that are deemed fit from a cost, usage, billing, or search perspective. \n",
      "These tags can then be applied to resources. Tags that are defined at the resource \n",
      "group level are not inherited by their resources.\n",
      "• Policies: Another security feature that is provided by ARM is custom policies. \n",
      "Custom policies can be created to control access to resources. Policies are defined \n",
      "as conventions and rules, and they must be adhered to while interacting with \n",
      "resources and resource groups. The policy definition contains an explicit denial of \n",
      "actions on resources or access to resources. By default, every access is allowed if \n",
      "it is not mentioned in the policy definition. These policy definitions are assigned \n",
      "to the resource, resource group, and subscription scope. It is important to note \n",
      "that these policies are not replacements or substitutes for RBAC. In fact, they \n",
      "complement and work together with RBAC. Policies are evaluated after a user is\n",
      "---------------\n",
      "authenticated by Azure AD and authorized by the RBAC service. ARM provides \n",
      "a JSON-based policy definition language for defining policies. Some examples \n",
      "of policy definitions are that a policy must tag every provisioned resource, and \n",
      "resources can only be provisioned to specific Azure regions.\n",
      "---------------\n",
      "14 | Getting started with Azure\n",
      "• Locks: Subscriptions, resource groups, and resources can be locked to prevent \n",
      "accidental deletions or updates by an authenticated user. Locks applied at higher \n",
      "levels flow downstream to the child resources. Alternatively, locks that are applied \n",
      "at the subscription level lock every resource group and the resources within it.\n",
      "• Multi-region: Azure provides multiple regions for provisioning and hosting \n",
      "resources. ARM allows resources to be provisioned at different locations while still \n",
      "residing within the same resource group. A resource group can contain resources \n",
      "from different regions.\n",
      "• Idempotent: This feature ensures predictability, standardization, and consistency \n",
      "in resource deployment by ensuring that every deployment will result in the same \n",
      "state of resources and configuration, no matter the number of times it is executed.\n",
      "• Extensible: ARM provides an extensible architecture to allow the creation and\n",
      "---------------\n",
      "plugging in of new resource providers and resource types on the platform.\n",
      "Virtualization\n",
      "Virtualization was a breakthrough innovation that completely changed the way that \n",
      "physical servers were looked at. It refers to the abstraction of a physical object into a \n",
      "logical object.\n",
      "The virtualization of physical servers led to virtual servers known as VMs. These \n",
      "VMs consume and share the physical CPU, memory, storage, and other hardware \n",
      "of the physical server on which they are hosted. This enables the faster and easier \n",
      "provisioning of application environments on-demand, providing high availability and \n",
      "scalability with reduced cost. One physical server is enough to host multiple VMs, with \n",
      "each VM containing its own operating system and hosting services on it.\n",
      "There was no longer any need to buy additional physical servers for deploying new \n",
      "applications and services. The existing physical servers were sufficient to host more\n",
      "---------------\n",
      "VMs. Furthermore, as part of rationalization, many physical servers were consolidated \n",
      "into a few with the help of virtualization.\n",
      "Each VM contains the entire operating system, and each VM is completely isolated \n",
      "from other VMs, including the physical hosts. Although a VM uses the hardware that is \n",
      "provided by the host physical server, it has full control over its assigned resources and \n",
      "its environment. These VMs can be hosted on a network such as a physical server with \n",
      "its own identity.\n",
      "Azure can create Linux and Windows VMs in a few minutes. Microsoft provides its own \n",
      "images, along with images from its partners and the community; users can also provide \n",
      "their own images. VMs are created using these images.\n",
      "---------------\n",
      "Containers | 15\n",
      "Containers\n",
      "Containers are also a virtualization technology; however, they do not virtualize a server. \n",
      "Instead, a container is operating system–level virtualization. What this means is that \n",
      "containers share the operating system kernel (which is provided by the host) among \n",
      "themselves along with the host. Multiple containers running on a host (physical or \n",
      "virtual) share the host operating system kernel. Containers ensure that they reuse the \n",
      "host kernel instead of each having a dedicated kernel to themselves.\n",
      "Containers are completely isolated from their host or from other containers running on \n",
      "the host. Windows containers use Windows storage filter drivers and session isolation \n",
      "to isolate operating system services such as the file system, registry, processes, and \n",
      "networks. The same is true even for Linux containers running on Linux hosts. Linux \n",
      "containers use the Linux namespace, control groups, and union file system to virtualize \n",
      "the host operating system.\n",
      "---------------\n",
      "The container appears as if it has a completely new and untouched operating system \n",
      "and resources. This arrangement provides lots of benefits, such as the following:\n",
      "• Containers are fast to provision and take less time to provision compared to \n",
      "virtual machines. Most of the operating system services in a container are \n",
      "provided by the host operating system.\n",
      "• Containers are lightweight and require fewer computing resources than VMs. The \n",
      "operating system resource overhead is no longer required with containers.\n",
      "• Containers are much smaller than VMs.\n",
      "• Containers can help solve problems related to managing multiple application \n",
      "dependencies in an intuitive, automated, and simple manner.\n",
      "• Containers provide infrastructure in order to define all application dependencies \n",
      "in a single place.\n",
      "---------------\n",
      "16 | Getting started with Azure\n",
      "Containers are an inherent feature of Windows Server 2016 and Windows 10; however, \n",
      "they are managed and accessed using a Docker client and a Docker daemon. Containers \n",
      "can be created on Azure with a Windows Server 2016 SKU as an image. Each container \n",
      "has a single main process that must be running for the container to exist. A container \n",
      "will stop when this process ends. Additionally, a container can either run in interactive \n",
      "mode or in detached mode like a service:\n",
      "Figure 1.3: Container architecture\n",
      "Figure 1.3 shows all the technical layers that enable containers. The bottom-most layer \n",
      "provides the core infrastructure in terms of network, storage, load balancers, and \n",
      "network cards. At the top of the infrastructure is the compute layer, consisting of either \n",
      "a physical server or both physical and virtual servers on top of a physical server. This \n",
      "layer contains the operating system with the ability to host containers. The operating\n",
      "---------------\n",
      "system provides the execution driver that the layers above use to call the kernel code \n",
      "and objects to execute containers. Microsoft created Host Container System Shim \n",
      "(HCSShim) for managing and creating containers and uses Windows storage filter \n",
      "drivers for image and file management.\n",
      "Container environment isolation is enabled for the Windows session. Windows Server \n",
      "2016 and Nano Server provide the operating system, enable the container features, \n",
      "and execute the user-level Docker client and Docker Engine. Docker Engine uses the \n",
      "services of HCSShim, storage filter drivers, and sessions to spawn multiple containers \n",
      "on the server, with each containing a service, application, or database.\n",
      "---------------\n",
      "Docker | 17\n",
      "Docker\n",
      "Docker provides management features to Windows containers. It comprises the \n",
      "following two executables:\n",
      "• The Docker daemon\n",
      "• The Docker client\n",
      "The Docker daemon is the workhorse for managing containers. It is a Windows service \n",
      "responsible for managing all activities on the host that are related to containers. The \n",
      "Docker client interacts with the Docker daemon and is responsible for capturing inputs \n",
      "and sending them across to the Docker daemon. The Docker daemon provides the \n",
      "runtime, libraries, graph drivers, and engine to create, manage, and monitor containers \n",
      "and images on the host server. It also has the ability to create custom images that are \n",
      "used for building and shipping applications to multiple environments.\n",
      "Interacting with the intelligent cloud\n",
      "Azure provides multiple ways to connect, automate, and interact with the intelligent \n",
      "cloud. All these methods require users to be authenticated with valid credentials before\n",
      "---------------\n",
      "they can be used. The different ways to connect to Azure are as follows:\n",
      "• The Azure portal\n",
      "• PowerShell\n",
      "• The Azure CLI\n",
      "• The Azure REST API\n",
      "The Azure portal\n",
      "The Azure portal is a great place to get started. With the Azure portal, users can log \n",
      "in and start creating and managing Azure resources manually. The portal provides \n",
      "an intuitive and user-friendly user interface through the browser. The Azure portal \n",
      "provides an easy way to navigate to resources using blades. The blades display all the \n",
      "properties of a resource, including its logs, cost, relationship with other resources, tags, \n",
      "security options, and more. An entire cloud deployment can be managed through the \n",
      "portal.\n",
      "---------------\n",
      "18 | Getting started with Azure\n",
      "PowerShell\n",
      "PowerShell is an object-based command-line shell and scripting language that \n",
      "is used for the administration, configuration, and management of infrastructure \n",
      "and environments. It is built on top of .NET Framework and provides automation \n",
      "capabilities. PowerShell has truly become a first-class citizen among IT administrators \n",
      "and automation developers for managing and controlling the Windows environment. \n",
      "Today, almost every Windows environment and many Linux environments can be \n",
      "managed by PowerShell. In fact, almost every aspect of Azure can also be managed \n",
      "by PowerShell. Azure provides rich support for PowerShell. It provides a PowerShell \n",
      "module for each resource provider containing hundreds of cmdlets. Users can use these \n",
      "cmdlets in their scripts to automate interaction with Azure. The Azure PowerShell \n",
      "module is available through the web platform installer and through the PowerShell\n",
      "---------------\n",
      "Gallery. Windows Server 2016 and Windows 10 provide package management and \n",
      "PowerShellGet modules for the quick and easy downloading and installation of \n",
      "PowerShell modules from the PowerShell Gallery. The PowerShellGet module provides \n",
      "the Install-Module cmdlet for downloading and installing modules on the system.\n",
      "Installing a module is a simple act of copying the module files at well-defined module \n",
      "locations, which can be done as follows:\n",
      "Import-module PowerShellGet\n",
      "Install-Module -Name az -verbose\n",
      "The Import-module command imports a module and its related functions within the \n",
      "current execution scope and Install-Module helps in installing modules.\n",
      "The Azure CLI\n",
      "Azure also provides Azure CLI 2.0, which can be deployed on Linux, Windows, and \n",
      "macOS operating systems. Azure CLI 2.0 is Azure's new command-line utility for \n",
      "managing Azure resources. Azure CLI 2.0 is optimized for managing and administering\n",
      "---------------\n",
      "Azure resources from the command line, and for building automation scripts that work \n",
      "against ARM. The CLI can be used to execute commands using the Bash shell or the \n",
      "Windows command line. The Azure CLI is very famous among non-Windows users as it \n",
      "allows you to talk to Azure on Linux and macOS. The steps for installing Azure CLI 2.0 \n",
      "are available at https:/ /docs.microsoft.com/cli/azure/install-azure-cli?view=azure-cli-\n",
      "latest.\n",
      "---------------\n",
      "Interacting with the intelligent cloud | 19\n",
      "The Azure REST API\n",
      "All Azure resources are exposed to users through REST endpoints. REST APIs are \n",
      "service endpoints that implement HTTP operations (or methods) by providing create, \n",
      "retrieve, update, or delete (CRUD) access to the service's resources. Users can \n",
      "consume these APIs to create and manage resources. In fact, the CLI and PowerShell \n",
      "mechanisms use these REST APIs internally to interact with resources on Azure.\n",
      "ARM templates\n",
      "In an earlier section, we looked at deployment features such as multi-service, multi-\n",
      "region, extensible, and idempotent features that are provided by ARM. ARM templates \n",
      "are the primary means of provisioning resources in ARM. ARM templates provide \n",
      "implementation support for ARM's deployment features.\n",
      "ARM templates provide a declarative model through which resources, their \n",
      "configuration, scripts, and extensions are specified. ARM templates are based on the\n",
      "---------------\n",
      "JavaScript Object Notation (JSON) format. They use JSON syntax and conventions to \n",
      "declare and configure resources. JSON files are text-based, user-friendly, and easily \n",
      "readable files.\n",
      "They can be stored in a source code repository and have version control. They are also \n",
      "a means to represent IaC that can be used to provision resources in an Azure resource \n",
      "group again and again, predictably and uniformly. A template needs a resource group \n",
      "for deployment. It can only be deployed to a resource group, and the resource group \n",
      "should exist before executing a template deployment. A template is not capable of \n",
      "creating a resource group.\n",
      "Templates provide the flexibility to be generic and modular in their design and \n",
      "implementation. Templates provide the ability to accept parameters from users, declare \n",
      "internal variables, define dependencies between resources, link resources within \n",
      "the same resource group or different resource groups, and execute other templates.\n",
      "---------------\n",
      "They also provide scripting language type expressions and functions that make them \n",
      "dynamic and customizable at runtime.\n",
      "---------------\n",
      "20 | Getting started with Azure\n",
      "Deployments\n",
      "PowerShell allows the following two modes for the deployment of templates:\n",
      "• Incremental: Incremental deployment adds resources declared in the template \n",
      "that don't exist in a resource group, leaves resources unchanged in a resource \n",
      "group that is not part of a template definition, and leaves resources unchanged \n",
      "in a resource group that exists in both the template and resource group with the \n",
      "same configuration state.\n",
      "• Complete: Complete deployment, on the other hand, adds resources declared in a \n",
      "template to the resource group, deletes resources that do not exist in the template \n",
      "from the resource group, and leaves resources unchanged that exist in both the \n",
      "resource group and template with the same configuration state.\n",
      "Summary\n",
      "The cloud is a relatively new paradigm and is still in its nascent stage. There will be \n",
      "a lot of innovation and capabilities added over time. Azure is one of the top cloud\n",
      "---------------\n",
      "providers today and it provides rich capabilities through IaaS, PaaS, SaaS, and hybrid \n",
      "deployments. In fact, Azure Stack, which is an implementation of the private cloud from \n",
      "Microsoft, will be released soon. This will have the same features available on a private \n",
      "cloud as on the public cloud. They both will, in fact, connect and work seamlessly and \n",
      "transparently together.\n",
      "It is very easy to get started with Azure, but developers and architects can also fall \n",
      "into a trap if they do not design and architect their solutions appropriately. This book \n",
      "is an attempt to provide guidance and directions for architecting solutions the right \n",
      "way, using appropriate services and resources. Every service on Azure is a resource. It \n",
      "is important to understand how these resources are organized and managed in Azure. \n",
      "This chapter provided context around ARM and groups—which are the core frameworks \n",
      "that provide the building blocks for resources. ARM offers a set of services to resources\n",
      "---------------\n",
      "that help provide uniformity, standardization, and consistency in managing them. The \n",
      "services, such as RBAC, tags, policies, and locks, are available to every resource provider \n",
      "and resource. Azure also provides rich automation features to automate and interact \n",
      "with resources. Tools such as PowerShell, ARM templates, and the Azure CLI can be \n",
      "incorporated as part of release pipelines, continuous deployment, and delivery. Users \n",
      "can connect to Azure from heterogeneous environments using these automation tools.\n",
      "The next chapter will discuss some of the important architectural concerns that help to \n",
      "solve common cloud-based deployment problems and ensure applications are secure, \n",
      "available, scalable, and maintainable in the long run.\n",
      "---------------\n",
      "Architectural concerns, such as high availability and scalability, are some of the \n",
      "highest‑priority items for any architect. This is common across many projects and \n",
      "solutions. However, this becomes even more important when deploying applications \n",
      "to the cloud because of the complexity involved. Most of the time, the complexity \n",
      "does not come from the application, but from the choices available in terms of similar \n",
      "resources on the cloud. The other complex issue that arises from the cloud is the \n",
      "constant availability of new features. These new features can almost make an architect's \n",
      "decisions completely redundant in hindsight.\n",
      "In this chapter, we will look at an architect's perspective in terms of deploying highly \n",
      "available and scalable applications on Azure.\n",
      "Azure solution \n",
      "availability, scalability, \n",
      "and monitoring\n",
      "2\n",
      "---------------\n",
      "24 | Azure solution availability, scalability, and monitoring\n",
      "Azure is a mature platform that provides a number of options for implementing high \n",
      "availability and scalability at multiple levels. It is vital for an architect to know about \n",
      "them, including the differences between them and the costs involved, and finally, be in \n",
      "a position to choose an appropriate solution that meets the best solution requirements. \n",
      "There is no one solution for everything, but there is a good one for each project.\n",
      "Running applications and systems that are available to users for consumption whenever \n",
      "they need them is one of the topmost priorities for organizations. They want their \n",
      "applications to be operational and functional, and to continue to be available to their \n",
      "customers even when some untoward events occur. High availability is the primary \n",
      "theme of this chapter. Keeping the lights on is the common metaphor that is used for\n",
      "---------------\n",
      "high availability. Achieving high availability for applications is not an easy task, and \n",
      "organizations have to spend considerable time, energy, resources, and money in doing \n",
      "so. Additionally, there is still the risk that an organization's implementation will not \n",
      "produce the desired results. Azure provides a lot of high‑availability features for virtual \n",
      "machines (VMs) and the Platform as a Service (PaaS) service. In this chapter, we will go \n",
      "through the architectural and design features that are provided by Azure to ensure high \n",
      "availability for running applications and services.\n",
      "In this chapter, we will cover the following topics:\n",
      "• High availability\n",
      "• Azure high availability\n",
      "• Architectural considerations for high availability\n",
      "• Scalability\n",
      "• Upgrades and maintenance\n",
      "High availability\n",
      "High availability forms one of the core non‑functional technical requirements for any \n",
      "business‑critical service and its deployment. High availability refers to the feature\n",
      "---------------\n",
      "of a service or application that keeps it operational on a continuous basis; it does \n",
      "so by meeting or surpassing its promised service level agreement (SLA). Users are \n",
      "promised a certain SLA based on the service type. The service should be available \n",
      "for consumption based on its SLA. For example, an SLA can define 99% availability \n",
      "for an application for the entire year. This means that it should be available for \n",
      "consumption by users for 361.35 days. If it fails to remain available for this period, that \n",
      "constitutes a breach of the SLA. Most mission-critical applications define their high-\n",
      "availability SLA as 99.999% for a year. This means the application should be up, running, \n",
      "and available throughout the year, but it can only be down and unavailable for 5.2 hours. \n",
      "If the downtime goes beyond that, you are eligible for credit, which will be calculated \n",
      "based on the total uptime percentage.\n",
      "---------------\n",
      "Azure high availability | 25\n",
      "It is important to note here that high availability is defined in terms of time (yearly, \n",
      "monthly, weekly, or a combination of these).\n",
      "A service or application is made up of multiple components and these components are \n",
      "deployed on separate tiers and layers. Moreover, a service or application is deployed \n",
      "on an operating system (OS) and hosted on a physical machine or VM. It consumes \n",
      "network and storage services for various purposes. It might even be dependent \n",
      "on external systems. For these services or applications to be highly available, it \n",
      "is important that networks, storage, OSes, VMs or physical machines, and each \n",
      "component of the application is designed with the SLA and high availability in mind. A \n",
      "definite application life cycle process is used to ensure that high availability should be \n",
      "baked in from the start of application planning until its introduction to operations. This\n",
      "---------------\n",
      "also involves introducing redundancy. Redundant resources should be included in the \n",
      "overall application and deployment architecture to ensure that if one resource goes \n",
      "down, another takes over and serves the requests of the customer.\n",
      "Some of the major factors affecting the high availability of an application are as follows:\n",
      "• Planned maintenance\n",
      "• Unplanned maintenance\n",
      "• Application deployment architecture\n",
      "We will be looking into each of these factors in the following sections. Let's take a closer \n",
      "look at how high availability is ensured for deployments in Azure.\n",
      "Azure high availability\n",
      "Achieving high availability and meeting high SLA requirements is tough. Azure provides \n",
      "lots of features that enable high availability for applications, from the host and guest OS \n",
      "to applications using its PaaS. Architects can use these features to get high availability \n",
      "in their applications using configuration instead of building these features from scratch \n",
      "or depending on third‑party tools.\n",
      "---------------\n",
      "In this section, we will look at the features and capabilities provided by Azure to make \n",
      "applications highly available. Before we get into the architectural and configuration \n",
      "details, it is important to understand concepts related to Azure's high availability.\n",
      "---------------\n",
      "26 | Azure solution availability, scalability, and monitoring\n",
      "Concepts\n",
      "The fundamental concepts provided by Azure to attain high availability are as follows:\n",
      "• Availability sets\n",
      "• The fault domain\n",
      "• The update domain\n",
      "• Availability zones\n",
      "As you know, it's very important that we design solutions to be highly available. The \n",
      "workloads might be mission‑critical and require highly available architecture. We will \n",
      "take a closer look at each of the concepts of high availability in Azure now. Let's start \n",
      "with availability sets.\n",
      "Availability sets\n",
      "High availability in Azure is primarily achieved through redundancy. Redundancy \n",
      "means that there is more than one resource instance of the same type that takes \n",
      "control in the event of a primary resource failure. However, just having more similar \n",
      "resources does not make them highly available. For example, there could be multiple \n",
      "VMs provisioned within a subscription, but simply having multiple VMs does not make\n",
      "---------------\n",
      "them highly available. Azure provides a resource known as an availability set, and having \n",
      "multiple VMs associated with it makes them highly available. A minimum of two VMs \n",
      "should be hosted within the availability set to make them highly available. All VMs in the \n",
      "availability set become highly available because they are placed on separate physical \n",
      "racks in the Azure datacenter. During updates, these VMs are updated one at a time, \n",
      "instead of all at the same time. Availability sets provide a fault domain and an update \n",
      "domain to achieve this, and we will discuss this more in the next section. In short, \n",
      "availability sets provide redundancy at the datacenter level, similar to locally redundant \n",
      "storage.\n",
      "It is important to note that availability sets provide high availability within a datacenter. \n",
      "If an entire datacenter is down, then the availability of the application will be impacted. \n",
      "To ensure that applications are still available when a datacenter goes down, Azure has\n",
      "---------------\n",
      "introduced a new feature known as availability zones, which we will learn about shortly.\n",
      "If you recall the list of fundamental concepts, the next one in the list is the fault domain. \n",
      "The fault domain is often denoted by the acronym FD. In the next section, we will \n",
      "discuss what the FD is and how it is relevant while designing highly available solutions.\n",
      "---------------\n",
      "Azure high availability | 27\n",
      "The fault domain\n",
      "Fault domains (FDs) represent a group of VMs that share a common power source \n",
      "and network switch. When a VM is provisioned and assigned to an availability \n",
      "set, it is hosted within an FD. Each availability set has either two or three FDs by \n",
      "default, depending on the Azure region. Some regions provide two, while others \n",
      "provide three FDs in an availability set. FDs are non-configurable by users. \n",
      "When multiple VMs are created, they are placed on separate FDs. If the number of VMs \n",
      "is more than the FDs, the additional VMs are placed on existing FDs. For example, if \n",
      "there are five VMs, there will be FDs hosted on more than one VM. \n",
      "FDs are related to physical racks in the Azure datacenter. FDs provide high availability \n",
      "in the case of unplanned downtime due to hardware, power, and network failure. Since \n",
      "each VM is placed on a different rack with different hardware, a different power supply,\n",
      "---------------\n",
      "and a different network, other VMs continue running if a rack snaps off.\n",
      "The next one in the list is the update domain.\n",
      "The update domain\n",
      "An FD takes care of unplanned downtime, while an update domain handles downtime \n",
      "from planned maintenance. Each VM is also assigned an update domain and all the \n",
      "VMs within that update domain will reboot together. There can be as many as 20 \n",
      "update domains in a single availability set. Update domains are non-configurable by \n",
      "users. When multiple VMs are created, they are placed on separate update domains. \n",
      "If more than 20 VMs are provisioned on an availability set, they are placed in a \n",
      "round‑robin fashion on these update domains. Update domains take care of planned \n",
      "maintenance. From Service Health in the Azure portal, you can check the planned \n",
      "maintenance details and set alerts.\n",
      "In the next section, we will be covering availability zones.\n",
      "Availability zones\n",
      "This is a relatively new concept introduced by Azure and is very similar to zone\n",
      "---------------\n",
      "redundancy for storage accounts. Availability zones provide high availability within a \n",
      "region by placing VM instances on separate datacenters within the region. Availability \n",
      "zones are applicable to many resources in Azure, including VMs, managed disks, VM \n",
      "scale sets, and load balancers. The complete list of resources that are supported by \n",
      "availability zones can be found at https:/ /docs.microsoft.com/azure/availability‑\n",
      "zones/az‑overview#services‑that‑support‑availability‑zones. Being unable to \n",
      "configure availability across zones was a gap in Azure for a long time, and it was \n",
      "eventually fixed with the introduction of availability zones.\n",
      "---------------\n",
      "28 | Azure solution availability, scalability, and monitoring\n",
      "Each Azure region comprises multiple datacenters equipped with independent power, \n",
      "cooling, and networking. Some regions have more datacenters, while others have less. \n",
      "These datacenters within the region are known as zones. To ensure resiliency, there's a \n",
      "minimum of three separate zones in all enabled regions. Deploying VMs in an availability \n",
      "zone ensures that these VMs are in different datacenters and are on different racks and \n",
      "networks. These datacenters in a region relate to high‑speed networks and there is no \n",
      "lag in communication between these VMs. Figure 2.1 shows how availability zones are \n",
      "set up in a region:\n",
      "Figure 2.1: Availability zones in a region\n",
      "You can find more information about availability zones at https:/ /docs.microsoft.com/\n",
      "azure/availability‑zones/az‑overview.\n",
      "Zone‑redundant services replicate your applications and data across availability zones \n",
      "to protect from single points of failure.\n",
      "---------------\n",
      "Azure Region\n",
      "Availability Zone 1A vailability Zone 2\n",
      "Availability Zone 3\n",
      "---------------\n",
      "Azure high availability | 29\n",
      "If an application needs higher availability and you want to ensure that it is available \n",
      "even if an entire Azure region is down, the next rung of the ladder for availability is the \n",
      "Traffic Manager feature, which will be discussed later in this chapter. Let's now move \n",
      "on to understanding Azure's take on load balancing for VMs.\n",
      "Load balancing\n",
      "Load balancing, as the name suggests, refers to the process of balancing a load among \n",
      "VMs and applications. With one VM, there is no need for a load balancer because the \n",
      "entire load is on a single VM and there is no other VM to share the load. However, with \n",
      "multiple VMs containing the same application and service, it is possible to distribute the \n",
      "load among them through load balancing. Azure provides a few resources to enable load \n",
      "balancing:\n",
      "• Load balancers: The Azure load balancer helps to design solutions with high \n",
      "availability. Within the Transmission Control Protocol (TCP) stack, it is a layer\n",
      "---------------\n",
      "4 transport‑level load balancer. This is a layer 4 load balancer that distributes \n",
      "incoming traffic among healthy instances of services that are defined in a load-\n",
      "balanced set. Level 4 load balancers work at the transport level and have network‑\n",
      "level information, such as an IP address and port, to decide the target for the \n",
      "incoming request. Load balancers are discussed in more detail later in this chapter.\n",
      "• Application gateways: An Azure Application Gateway delivers high availability to \n",
      "your applications. They are layer 7 load balancers that distribute the incoming \n",
      "traffic among healthy instances of services. Level 7 load balancers can work at \n",
      "the application level and have application‑level information, such as cookies, \n",
      "HTTP, HTTPS, and sessions for the incoming request. Application gateways are \n",
      "discussed in more detail later in this chapter. Application gateways are also used \n",
      "when deploying Azure Kubernetes Service, specifically for scenarios in which\n",
      "---------------\n",
      "ingress traffic from the internet should be routed to the Kubernetes services in \n",
      "the cluster.\n",
      "• Azure Front Door: Azure Front Door is very similar to application gateways; \n",
      "however, it does not work at the region or datacenter level. Instead, it helps \n",
      "in routing requests across regions globally. It has the same feature set as that \n",
      "provided by application gateways, but at the global level. It also provides a web \n",
      "application firewall for the filtering of requests and provides other security-\n",
      "related protection. It provides session affinity, TLS termination, and URL-based \n",
      "routing as some of its features.\n",
      "• Traffic Manager: Traffic Manager helps in the routing of requests at the global \n",
      "level across multiple regions based on the health and availability of regional \n",
      "endpoints. It supports doing so using DNS redirect entries. It is highly resilient and \n",
      "has no service impact during region failures as well.\n",
      "---------------\n",
      "30 | Azure solution availability, scalability, and monitoring\n",
      "Since we've explored the methods and services that can be used to achieve load \n",
      "balancing, we'll go ahead and discuss how to make VMs highly available.\n",
      "VM high availability\n",
      "VMs provide compute capabilities. They provide processing power and hosting for \n",
      "applications and services. If an application is deployed on a single VM and that machine \n",
      "is down, then the application will not be available. If the application is composed \n",
      "of multiple tiers and each tier is deployed in its own single instance of a VM, even \n",
      "downtime for a single instance of VM can render the entire application unavailable. \n",
      "Azure tries to make even single VM instances highly available for 99.9% of the time, \n",
      "particularly if these single‑instance VMs use premium storage for their disks. Azure \n",
      "provides a higher SLA for those VMs that are grouped together in an availability set. It\n",
      "---------------\n",
      "provides a 99.95% SLA for VMs that are part of an availability set with two or more VMs. \n",
      "The SLA is 99.99% if VMs are placed in availability zones. In the next section, we will be \n",
      "discussing high availability for compute resources.\n",
      "Compute high availability\n",
      "Applications demanding high availability should be deployed on multiple VMs in the \n",
      "same availability set. If applications are composed of multiple tiers, then each tier \n",
      "should have a group of VMs on their dedicated availability set. In short, if there are \n",
      "three tiers of an application, there should be three availability sets and a minimum of six \n",
      "VMs (two in each availability set) to make the entire application highly available.\n",
      "So, how does Azure provide an SLA and high availability to VMs in an availability set \n",
      "with multiple VMs in each availability set? This is the question that might come to mind \n",
      "for you.\n",
      "Here, the use of concepts that we considered before comes into play—that is, the fault\n",
      "---------------\n",
      "and update domains. When Azure sees multiple VMs in an availability set, it places those \n",
      "VMs on a separate FD. In other words, these VMs are placed on separate physical racks \n",
      "instead of the same rack. This ensures that at least one VM continues to be available \n",
      "even if there is a power, hardware, or rack failure. There are two or three FDs in an \n",
      "availability set and, depending on the number of VMs in an availability set, the VMs are \n",
      "placed in separate FDs or repeated in a round‑robin fashion. This ensures that high \n",
      "availability is not impacted because of the failure of the rack.\n",
      "Azure also places these VMs on a separate update domain. In other words, Azure tags \n",
      "these VMs internally in such a way that these VMs are patched and updated one after \n",
      "another, such that any reboot in an update domain does not affect the availability of the \n",
      "application. This ensures that high availability is not impacted because of the VM and\n",
      "---------------\n",
      "host maintenance. It is important to note that Azure is not responsible for OS‑level and \n",
      "application maintenance.\n",
      "---------------\n",
      "Azure high availability | 31\n",
      "With the placement of VMs in separate fault and update domains, Azure ensures that all \n",
      "VMs are never down at the same time and that they are alive and available for serving \n",
      "requests, even though they might be undergoing maintenance or facing physical \n",
      "downtime challenges:\n",
      "Figure 2.2: VM distribution across fault and update domains\n",
      "Figure 2.2 shows four VMs (two have Internet Information Services (IIS) and the \n",
      "other two have SQL Server installed on them). Both the IIS and SQL VMs are part of \n",
      "availability sets. The IIS and SQL VMs are in separate FDs and different racks in the \n",
      "datacenter. They are also in separate update domains.\n",
      "Figure 2.3 shows the relationship between fault and update domains: \n",
      "Figure 2.3: Layout of update domains and FDs in an availability set\n",
      "UD 0\n",
      "FD 0 FD 2\n",
      "UD 1\n",
      "FD 1\n",
      "UD 2\n",
      "---------------\n",
      "32 | Azure solution availability, scalability, and monitoring\n",
      "So far, we have discussed achieving high availability for compute resources. In the next \n",
      "section, you will learn how high availability can be implemented for PaaS.\n",
      "High-availability platforms\n",
      "Azure has provided a lot of new features to ensure high availability for PaaS. Some of \n",
      "them are listed here:\n",
      "• Containers in app services\n",
      "• Azure Container Instances groups\n",
      "• Azure Kubernetes Service\n",
      "• Other container orchestrators, such as DC/OS and Swarm\n",
      "Another important platform that brings high availability is Service Fabric. Both Service \n",
      "Fabric and container orchestrators that include Kubernetes ensure that the desired \n",
      "number of application instances are always up and running in an environment. What \n",
      "this means is that even if one of the instances goes down in the environment, the \n",
      "orchestrator will know about it by means of active monitoring and will spin up a\n",
      "---------------\n",
      "new instance on a different node, thereby maintaining the ideal and desired number \n",
      "of instances. It does this without any manual or automated interference from the \n",
      "administrator.\n",
      "While Service Fabric allows any type of application to become highly available, \n",
      "orchestrators such as Kubernetes, DC/OS, and Swarm are specific to containers. \n",
      "Also, it is important to understand that these platforms provide features that help in \n",
      "rolling updates, rather than a big bank update that might affect the availability of the \n",
      "application.\n",
      "When we were discussing high availability for VMs, we took a brief look at what load \n",
      "balancing is. Let's take a closer look at it to better understand how it works in Azure.\n",
      "Load balancers in Azure\n",
      "Azure provides two resources that have the functionality of a load balancer. It provides \n",
      "a level 4 load balancer, which works at the transport layer within the TCP OSI stack, and\n",
      "---------------\n",
      "a level 7 load balancer (application gateway), which works at the application and session \n",
      "levels.\n",
      "Although both application gateways and load balancers provide the basic features of \n",
      "balancing a load, they serve different purposes. There are a number of use cases in \n",
      "which it makes more sense to deploy an application gateway than a load balancer.\n",
      "An application gateway provides the following features that are not available with Azure \n",
      "load balancers:\n",
      "---------------\n",
      "Azure high availability | 33\n",
      "• Web application firewall: This is an additional firewall on top of the OS firewall \n",
      "and it gives the ability to peek into incoming messages. This helps in identifying \n",
      "and preventing common web‑based attacks, such as SQL injection, cross‑site \n",
      "scripting attacks, and session hijacks.\n",
      "• Cookie-based session affinity: Load balancers distribute incoming traffic to \n",
      "service instances that are healthy and relatively free. A request can be served by \n",
      "any service instance. However, there are applications that need advanced features \n",
      "in which all subsequent requests following the first request should be processed \n",
      "by the same service instance. This is known as cookie-based session affinity. An \n",
      "application gateway provides cookie-based session affinity to keep a user session \n",
      "on the same service instance using cookies.\n",
      "• Secure Sockets Layer (SSL) offload: The encryption and decryption of request\n",
      "---------------\n",
      "and response data is performed by SSL and is generally a costly operation. Web \n",
      "servers should ideally be spending their resources on processing and serving \n",
      "requests, rather than the encryption and decryption of traffic. SSL offload helps in \n",
      "transferring this cryptography process from the web server to the load balancer, \n",
      "thereby providing more resources to web servers serving users. The request from \n",
      "the user is encrypted but gets decrypted at the application gateway instead of \n",
      "the web server. The request from the application gateway to the web server is \n",
      "unencrypted.\n",
      "• End-to-end SSL: While SSL offload is a nice feature for certain applications, there \n",
      "are certain mission‑critical secure applications that need complete SSL encryption \n",
      "and decryption even if traffic passes through load balancers. An application \n",
      "gateway can be configured for end-to-end SSL cryptography as well.\n",
      "• URL-based content routing: Application gateways are also useful for redirecting\n",
      "---------------\n",
      "traffic to different servers based on the URL content of incoming requests. This \n",
      "helps in hosting multiple services alongside other applications.\n",
      "Azure load balancers\n",
      "An Azure load balancer distributes incoming traffic based on the transport-level \n",
      "information that is available to it. It relies on the following features:\n",
      "• An originating IP address\n",
      "• A target IP address\n",
      "• An originating port number\n",
      "• A target port number\n",
      "• A type of protocol—either TCP or HTTP\n",
      "---------------\n",
      "34 | Azure solution availability, scalability, and monitoring\n",
      "An Azure load balancer can be a private load balancer or a public load balancer. A \n",
      "private load balancer can be used to distribute traffic within the internal network. As \n",
      "this is internal, there won't be any public IPs assigned and they cannot be accessed \n",
      "from the internet. A public load balancer has an external public IP attached to it and can \n",
      "be accessed via the internet. In Figure 2.4, you can see how internal (private) and public \n",
      "load balancers are incorporated into a single solution to handle internal and external \n",
      "traffic, respectively:\n",
      "Figure 2.4: Distributing traffic using Azure load balancers\n",
      "In Figure 2.4, you can see that external users are accessing the VMs via the public load \n",
      "balancer, and then the traffic from the VM is distributed across another set of VMs \n",
      "using an internal load balancer.\n",
      "We have done a comparison of how Azure load balancers differ from Application\n",
      "---------------\n",
      "Gateways. In the next section, we will discuss application gateways in more detail.\n",
      "Virtual Network\n",
      "Public Load\n",
      "Balancer\n",
      "Internal Load\n",
      "Balancer\n",
      "Web Tier\n",
      "Subnet\n",
      "Business\n",
      "Tier\n",
      "Subnet\n",
      "---------------\n",
      "Azure high availability | 35\n",
      "The Azure Application Gateway\n",
      "An Azure load balancer helps us to enable solutions at the infrastructure level. However, \n",
      "there are times when using a load balancer requires advanced services and features. \n",
      "These advanced services include SSL termination, sticky sessions, advanced security, \n",
      "and more. An Azure application gateway provides these additional features; the Azure \n",
      "application gateway is a level 7 load balancer that works with the application and \n",
      "session payload in a TCP OSI stack.\n",
      "Application gateways have more information compared to Azure load balancers in order \n",
      "to make decisions on request routing and load balancing between servers. Application \n",
      "gateways are managed by Azure and are highly available.\n",
      "An application gateway sits between the users and the VMs, as shown in Figure 2.5:\n",
      "Figure 2.5: An Azure application gateway\n",
      "Application gateways are a managed service. They use Application Request\n",
      "---------------\n",
      "Routing (ARR) to route requests to different services and endpoints. Creating an \n",
      "application gateway requires a private or public IP address. The application gateway \n",
      "then routes the HTTP/HTTPS traffic to configured endpoints.\n",
      "An application gateway is similar to an Azure load balancer from a configuration \n",
      "perspective, with additional constructs and features. Application gateways can be \n",
      "configured with a front-end IP address, a certificate, a port configuration, a back-end \n",
      "pool, session affinity, and protocol information.\n",
      "Another service that we discussed in relation to high availability for VMs was Azure \n",
      "Traffic Manager. Let's try to understand more about this service in the next section.\n",
      "Application Gateway\n",
      "(AdatumAppGateway)\n",
      "User\n",
      "---------------\n",
      "36 | Azure solution availability, scalability, and monitoring\n",
      "Azure Traffic Manager\n",
      "After gaining a good understanding of both Azure load balancers and application \n",
      "gateways, it's time to get into the details of Traffic Manager. Azure load balancers \n",
      "and application gateways are much‑needed resources for high availability within \n",
      "a datacenter or region; however, to achieve high availability across regions and \n",
      "datacenters, there is a need for another resource, and Traffic Manager helps us in \n",
      "this regard.\n",
      "Traffic Manager helps us to create highly available solutions that span multiple \n",
      "geographies, regions, and datacenters. Traffic Manager is not similar to load balancers. \n",
      "It uses the Domain Name Service (DNS) to redirect requests to an appropriate endpoint \n",
      "determined by the health and configuration of the endpoint. Traffic Manager is not a \n",
      "proxy or a gateway, and it does not see the traffic passing between the client and the\n",
      "---------------\n",
      "service. It simply redirects requests based on the most appropriate endpoints.\n",
      "Azure Traffic Manager helps to control the traffic that is distributed across application \n",
      "endpoints. An endpoint can be termed as any internet‑facing service hosted inside or \n",
      "outside of Azure.\n",
      "Endpoints are internet‑facing, reachable public URLs. Applications are provisioned \n",
      "within multiple geographies and Azure regions. Applications deployed to each region \n",
      "have a unique endpoint referred to by DNS CNAME. These endpoints are mapped to the \n",
      "Traffic Manager endpoint. When a Traffic Manager instance is provisioned, it gets an \n",
      "endpoint by default with a .trafficmanager.net URL extension.\n",
      "When a request arrives at the Traffic Manager URL, it finds the most appropriate \n",
      "endpoint in its list and redirects the request to it. In short, Azure Traffic Manager acts \n",
      "as a global DNS to identify the region that will serve the request.\n",
      "However, how does Traffic Manager know which endpoints to use and redirect client\n",
      "---------------\n",
      "requests to? There are two aspects that Traffic Manager considers to determine the \n",
      "most appropriate endpoint and region.\n",
      "Firstly, Traffic Manager actively monitors the health of all endpoints. It can monitor the \n",
      "health of VMs, cloud services, and app services. If it determines that the health of an \n",
      "application deployed to a region is not suitable for redirecting traffic, it redirects the \n",
      "requests to a healthy endpoint.\n",
      "---------------\n",
      "Azure high availability | 37\n",
      "Secondly, Traffic Manager can be configured with routing information. There are six \n",
      "traffic routing methods available in Traffic Manager, which are as follows:\n",
      "• Priority: This should be used when all traffic should go to a default endpoint, and \n",
      "backups are available in case the primary endpoints are unavailable.\n",
      "• Weighted: This should be used to distribute traffic across endpoints evenly, or \n",
      "according to defined weights.\n",
      "• Performance: This should be used for endpoints in different regions, and users \n",
      "should be redirected to the closest endpoint based on their location. This has a \n",
      "direct impact on network latency.\n",
      "• Geographic: This should be used to redirect users to an endpoint (Azure, external, \n",
      "or nested) based on the nearest geographical location. This can help in adhering \n",
      "to compliance related to data protection, localization, and region-based traffic \n",
      "collection.\n",
      "• Subnet: This is a new routing method and it helps in providing clients with\n",
      "---------------\n",
      "different endpoints based on their IP addresses. In this method, a range of IP \n",
      "addresses are assigned to each endpoint. These IP address ranges are mapped \n",
      "to the client IP address to determine an appropriate returning endpoint. Using \n",
      "this routing method, it is possible to provide different content to different people \n",
      "based on their originating IP address. \n",
      "• Multivalue: This is also a new method added in Azure. In this method, multiple \n",
      "endpoints are returned to the client and any of them can be used. This ensures \n",
      "that if one endpoint is unhealthy, then other endpoints can be used instead. This \n",
      "helps in increasing the overall availability of the solution.\n",
      "It should be noted that after Traffic Manager determines a valid healthy endpoint, \n",
      "clients connect directly to the application. Let's now move on to understand Azure's \n",
      "capabilities in routing user requests globally.\n",
      "In the next section, we will be discussing another service, called Azure Front Door.\n",
      "---------------\n",
      "This service is like Azure Application Gateway; however, there is a small difference that \n",
      "makes this service distinct. Let's go ahead and learn more about Azure Front Door.\n",
      "---------------\n",
      "38 | Azure solution availability, scalability, and monitoring\n",
      "Azure Front Door\n",
      "Azure Front Door is the latest offering in Azure that helps route requests to services \n",
      "at a global level instead of a local region or datacenter level, as in the case of Azure \n",
      "Application Gateway and load balancers. Azure Front Door is like Application Gateway, \n",
      "with the difference being in the scope. It is a layer 7 load balancer that helps in routing \n",
      "requests to the nearest best‑performing service endpoint deployed in multiple regions. \n",
      "It provides features such as TLS termination, session affinity, URL-based routing, \n",
      "and multiple site hosting, along with a web application firewall. It is similar to Traffic \n",
      "Manager in that it is, by default, resilient to entire region failures and it provides \n",
      "routing capabilities. It also conducts endpoint health probes periodically to ensure that \n",
      "requests are routed to healthy endpoints only.\n",
      "It provides four different routing methods:\n",
      "---------------\n",
      "• Latency: Requests will route to endpoints that will have the least latency end to \n",
      "end.\n",
      "• Priority: Requests will route to a primary endpoint and to a secondary endpoint in \n",
      "the case of the failure of the primary.\n",
      "• Weighted: Requests will route based on weights assigned to the endpoints.\n",
      "• Session Affinity: Requests in a session will end up with the same endpoint to make \n",
      "use of session data from prior requests. The original request can end up with any \n",
      "available endpoint.\n",
      "Deployments looking for resilience at the global level should include Azure Front Door \n",
      "in their architecture, alongside application gateways and load balancers. In the next \n",
      "section, you will see some of the architectural considerations that you should account \n",
      "for while designing highly available solutions.\n",
      "Architectural considerations for high availability\n",
      "Azure provides high availability through various means and at various levels. High\n",
      "---------------\n",
      "availability can be at the datacenter level, the region level, or even across Azure. In this \n",
      "section, we will go through some of the architectures for high availability.\n",
      "---------------\n",
      "Architectural considerations for high availability | 39\n",
      "High availability within Azure regions\n",
      "The architecture shown in Figure 2.6 shows a high‑availability deployment within a \n",
      "single Azure region. High availability is designed at the individual resource level. In \n",
      "this architecture, there are multiple VMs at each tier connected through either an \n",
      "application gateway or a load balancer, and they are each part of an availability set. Each \n",
      "tier is associated with an availability set. These VMs are placed on separate fault and \n",
      "update domains. While the web servers are connected to application gateways, the rest \n",
      "of the tiers, such as the application and database tiers, have internal load balancers:\n",
      "Figure 2.6: Designing high availability within a region\n",
      "---------------\n",
      "40 | Azure solution availability, scalability, and monitoring\n",
      "Now that you know how to design highly available solutions in the same region, let's \n",
      "discuss how an architecture that is similar, but spread across Azure regions, can be \n",
      "designed.\n",
      "High availability across Azure regions\n",
      "This architecture shows similar deployments in two different Azure regions. As shown \n",
      "in Figure 2.7, both regions have the same resources deployed. High availability is \n",
      "designed at the individual resource level within these regions. There are multiple VMs \n",
      "at each tier, connected through load balancers, and they are part of an availability set. \n",
      "These VMs are placed on separate fault and update domains. While the web servers \n",
      "are connected to external load balancers, the rest of the tiers, such as the application \n",
      "and database tiers, have internal load balancers. It should be noted that application \n",
      "load balancers can be used for web servers and the application tier (instead of Azure\n",
      "---------------\n",
      "load balancers) if there is a need for advanced services, such as session affinity, SSL \n",
      "termination, advanced security using a web application firewall (WAF), and path‑\n",
      "based routing. The databases in both regions are connected to each other using virtual \n",
      "network peering and gateways. This is helpful in configuring log shipping, SQL Server \n",
      "Always On, and other data synchronization techniques.\n",
      "The endpoints of the load balancers from both regions are used to configure Traffic \n",
      "Manager endpoints, and traffic is routed based on the priority load-balancing method. \n",
      "Traffic Manager helps in routing all requests to the East US region and, after failover, to \n",
      "West Europe in the case of the non-availability of the first region:\n",
      "---------------\n",
      "Architectural considerations for high availability | 41\n",
      "Figure 2.7: Designing high availability across Azure regions\n",
      "In the next section, we will be exploring scalability, which is another advantage of \n",
      "the cloud.\n",
      "---------------\n",
      "42 | Azure solution availability, scalability, and monitoring\n",
      "Scalability\n",
      "Running applications and systems that are available to users for consumption is \n",
      "important for architects of any business‑critical application. However, there is another \n",
      "equally important application feature that is one of the top priorities for architects, and \n",
      "this is the scalability of the application.\n",
      "Imagine a situation in which an application is deployed and obtains great performance \n",
      "and availability with a few users, but both availability and performance decrease as the \n",
      "number of users begin to increase. There are times when an application performs well \n",
      "under a normal load, but suffers a drop in performance with an increase in the number \n",
      "of users. This can happen if there is a sudden increase in the number of users and the \n",
      "environment is not built for such a large number of users.\n",
      "To accommodate such spikes in the number of users, you might provision the hardware\n",
      "---------------\n",
      "and bandwidth for handling spikes. The challenge with this is that the additional \n",
      "capacity is not used for the majority of the year, and so does not provide any return on \n",
      "investment. It is provisioned for use only during the holiday season or sales. I hope that \n",
      "by now you are becoming familiar with the problems that architects are trying to solve. \n",
      "All these problems are related to capacity sizing and the scalability of an application. \n",
      "The focus of this chapter is to understand scalability as an architectural concern and to \n",
      "check out the services that are provided by Azure for implementing scalability.\n",
      "Capacity planning and sizing are a couple of the top priorities for architects and \n",
      "their applications and services. Architects must find a balance between buying and \n",
      "provisioning too many resources and buying and provisioning too few resources. Having \n",
      "too few resources can lead to you not being able to serve all users, resulting in them\n",
      "---------------\n",
      "turning to a competitor. On the other hand, having too many resources can hurt your \n",
      "budget and return on investment because most of the resources will remain unused \n",
      "most of the time. Moreover, the problem is amplified by the varying level of demand at \n",
      "different times. It is almost impossible to predict the number of users of an application \n",
      "over a day, let alone a year. However, it is possible to find an approximate number using \n",
      "past information and continuous monitoring.\n",
      "Scalability refers to the ability to handle a growing number of users and provide them \n",
      "with the same level of performance as when there are fewer users utilizing resources \n",
      "for application deployment, processes, and technology. Scalability might mean serving \n",
      "more requests without a decrease in performance, or it might mean handling larger and \n",
      "more time‑consuming work without any loss of performance in both cases.\n",
      "---------------\n",
      "Scalability | 43\n",
      "Capacity planning and sizing exercises should be undertaken by architects at the \n",
      "very beginning of a project and during the planning phase to provide scalability to \n",
      "applications.\n",
      "Some applications have stable demand patterns, while it is difficult to predict others. \n",
      "Scalability requirements are known for stable‑demand applications, while discerning \n",
      "them can be a more involved process for variable‑demand applications. Autoscaling, a \n",
      "concept that we will review in the next section, should be used for applications whose \n",
      "demands cannot be predicted.\n",
      "People often tend to confuse scalability with performance. In the next section, you will \n",
      "see a quick comparison of these two terms.\n",
      "Scalability versus performance\n",
      "It is quite easy to get confused between scalability and performance when it comes to \n",
      "architectural concerns, because scalability is all about ensuring that irrespective of the\n",
      "---------------\n",
      "number of users consuming the application, all users receive the same predetermined \n",
      "level of performance.\n",
      "Performance relates to ensuring that an application caters to predefined response \n",
      "times and throughput. Scalability refers to having provisions for more resources when \n",
      "needed in order to accommodate more users without sacrificing performance.\n",
      "It is better to understand this using an analogy: the speed of a train directly relates to \n",
      "the performance of a railway network. However, getting more trains to run in parallel at \n",
      "the same or at higher speeds represents the scalability of the railway network.\n",
      "Now that you know what the difference between scalability and performance is, let's \n",
      "discuss how Azure provides scalability.\n",
      "---------------\n",
      "44 | Azure solution availability, scalability, and monitoring\n",
      "Azure scalability\n",
      "In this section, we will look at the features and capabilities provided by Azure to make \n",
      "applications highly available. Before we get into the architecture and configuration \n",
      "details, it is important to understand Azure's high‑availability concepts, in other words, \n",
      "scaling.\n",
      "Scaling refers to either increasing or decreasing the amount of resources that are \n",
      "used to serve requests from users. Scaling can be automatic or manual. Manual scaling \n",
      "requires an administrator to manually initiate the scaling process, while automatic \n",
      "scaling refers to an automatic increase or decrease in resources based on the events \n",
      "available from the environment and ecosystem, such as memory and CPU availability. \n",
      "Resources can be scaled up or down, or out and in, which will be explained later in this \n",
      "section.\n",
      "In addition to rolling updates, the fundamental constructs provided by Azure to achieve\n",
      "---------------\n",
      "high availability are as follows:\n",
      "• Scaling up and down\n",
      "• Scaling out and in\n",
      "• Autoscaling\n",
      "Scaling up\n",
      "Scaling a VM or service up entails the addition of further resources to existing servers, \n",
      "such as CPU, memory, and disks. It aims to increase the capacity of existing physical \n",
      "hardware and resources.\n",
      "Scaling down\n",
      "Scaling a VM or service down entails the removal of existing resources from existing \n",
      "servers, such as CPU, memory, and disks. It aims to decrease the capacity of existing \n",
      "physical and virtual hardware and resources.\n",
      "---------------\n",
      "Scalability | 45\n",
      "Scaling out\n",
      "Scaling out entails adding further hardware, such as additional servers and capacity. \n",
      "This typically involves adding new servers, assigning them IP addresses, deploying \n",
      "applications on them, and making them part of the existing load balancers such that \n",
      "traffic can be routed to them. Scaling out can be automatic or manual as well. However, \n",
      "for better results, automation should be used:\n",
      "Figure 2.8: Scaling out\n",
      "Scaling in\n",
      "Scaling in refers to the process of removing the existing hardware in terms of existing \n",
      "servers and capacity. This typically involves removing existing servers, deallocating \n",
      "their IP addresses, and removing them from the existing load balancer configuration \n",
      "such that traffic cannot be routed to them. Like scaling out, scaling in can be automatic \n",
      "or manual.\n",
      "Autoscaling\n",
      "Autoscaling refers to the process of either scaling up/down or scaling out/in \n",
      "dynamically based on application demand, and this happens using automation.\n",
      "---------------\n",
      "Autoscaling is useful because it ensures that a deployment always consists of an ideal \n",
      "number of server instances. Autoscaling helps in building applications that are fault \n",
      "tolerant. It not only supports scalability, but also makes applications highly available. \n",
      "Finally, it provides the best cost management. Autoscaling makes it possible to have \n",
      "the optimal configuration for server instances based on demand. It helps in not over-\n",
      "provisioning servers, only for them to end up being underutilized, and removes servers \n",
      "that are no longer required after scaling out. \n",
      "So far, we've discussed scalability in Azure. Azure offers scalability options for most of \n",
      "its services. Let's explore scalability for PaaS in Azure in the next section.\n",
      "---------------\n",
      "46 | Azure solution availability, scalability, and monitoring\n",
      "PaaS scalability\n",
      "Azure provides App Service for hosting managed applications. App Service is a PaaS \n",
      "offering from Azure. It provides services for the web and mobile platforms. Behind the \n",
      "web and mobile platforms is a managed infrastructure that is managed by Azure on \n",
      "behalf of its users. Users do not see or manage any infrastructure; however, they have \n",
      "the ability to extend the platform and deploy their applications on top of it. In doing \n",
      "so, architects and developers can concentrate on their business problems instead \n",
      "of worrying about the base platform and infrastructure provisioning, configuration, \n",
      "and troubleshooting. Developers have the flexibility to choose any language, OS, and \n",
      "framework to develop their applications. App Service provides multiple plans and, based \n",
      "on the plans chosen, various degrees of scalability are available. App Service provides \n",
      "the following five plans:\n",
      "---------------\n",
      "• Free: This uses shared infrastructure. It means that multiple applications will \n",
      "be deployed on the same infrastructure from the same or multiple tenants. It \n",
      "provides 1 GB of storage free of charge. However, there is no scaling facility in this \n",
      "plan.\n",
      "• Shared: This also uses shared infrastructure and provides 1 GB of storage free \n",
      "of charge. Additionally, custom domains are also provided as an extra feature. \n",
      "However, there is no scaling facility in this plan.\n",
      "• Basic: This has three different stock keeping units (SKUs): B1, B2, and B3. They \n",
      "each have increasing units of resources available to them in terms of CPU and \n",
      "memory. In short, they provide improved configuration of the VMs backing these \n",
      "services. Additionally, they provide storage, custom domains, and SSL support. \n",
      "The basic plan provides basic features for manual scaling. There is no autoscaling \n",
      "available in this plan. A maximum of three instances can be used to scale out an \n",
      "application.\n",
      "---------------\n",
      "• Standard: This also has three different SKUs: S1, S2, and S3. They each have \n",
      "increasing units of resources available to them in terms of CPU and memory. In \n",
      "short, they provide improved configuration of the VMs backing these services. \n",
      "Additionally, they provide storage, custom domains, and SSL support that is \n",
      "similar to that of the basic plan. This plan also provides a Traffic Manager instance, \n",
      "staging slots, and one daily backup as an additional feature on top of the basic \n",
      "plan. The standard plan provides features for automatic scaling. A maximum of 10 \n",
      "instances can be used to scale out the application.\n",
      "---------------\n",
      "Scalability | 47\n",
      "• Premium: This also has three different SKUs: P1, P2, and P3. They each have \n",
      "increasing units of resources available to them in terms of CPU and memory. In \n",
      "short, they provide improved configuration of the VMs backing these services. \n",
      "Additionally, they provide storage, custom domains, and SSL support that is similar \n",
      "to the basic plan. This plan also provides a Traffic Manager instance, staging \n",
      "slots, and 50 daily backups as an additional feature on top of the basic plan. The \n",
      "standard plan provides features for autoscaling. A maximum of 20 instances can be \n",
      "used to scale out the application.\n",
      "We have explored the scalability tiers available for PaaS services. Now, let's see how \n",
      "scaling can be done in the case of an App Service plan.\n",
      "PaaS – scaling up and down\n",
      "Scaling up and down services that are hosted by App Service is quite simple. The Azure \n",
      "app services Scale Up menu opens a new pane with all plans and their SKUs listed.\n",
      "---------------\n",
      "Choosing a plan and SKU will scale a service up or down, as shown in Figure 2.9:\n",
      "Figure 2.9: Different plans with their SKUs\n",
      "---------------\n",
      "48 | Azure solution availability, scalability, and monitoring\n",
      "PaaS – scaling out and in\n",
      "Scaling out and in services hosted in App Service is also quite simple. The Azure \n",
      "app services Scale Out menu item opens a new pane with scaling configuration options.\n",
      "By default, autoscaling is disabled for both premium and standard plans. It can be \n",
      "enabled using the Scale Out menu item and by clicking on the Enable autoscale button, \n",
      "as shown in Figure 2.10:\n",
      "Figure 2.10: Enabling the autoscale option\n",
      "Manual scaling does not require configuration, but autoscaling helps in configuring with \n",
      "the aid of the following properties:\n",
      "• Mode of scaling: This is based on a performance metric such as CPU or memory \n",
      "usage, or users can simply specify a number of instances for scaling.\n",
      "• When to scale: Multiple rules can be added that determine when to scale out \n",
      "and in. Each rule can determine criteria such as CPU or memory consumption,\n",
      "---------------\n",
      "whether to increase or decrease the number of instances, and how many instances \n",
      "to increase or decrease to at a time. At least one rule for scaling out and one rule \n",
      "for scaling in should be configured. Threshold definitions help in defining the \n",
      "upper and lower limits that should trigger the autoscale—by either increasing or \n",
      "decreasing the number of instances.\n",
      "• How to scale: This specifies how many instances to create or remove in each \n",
      "scale‑out or scale‑in operation:\n",
      "---------------\n",
      "Scalability | 49\n",
      "Figure 2.11: Setting the instance limits\n",
      "This is quite a good feature to enable in any deployment. However, you should enable \n",
      "both scaling out and scaling in together to ensure that your environment is back to \n",
      "normal capacity after scaling out.\n",
      "Since we have covered the scalability in PaaS, let's move on and discuss scalability in \n",
      "IaaS next.\n",
      "IaaS scalability\n",
      "There are users who will want to have complete control over their base infrastructure, \n",
      "platform, and application. They will prefer to consume IaaS solutions rather than PaaS \n",
      "solutions. When such customers create VMs, they are also responsible for capacity \n",
      "sizing and scaling. There is no out-of-the-box configuration for manually scaling or \n",
      "autoscaling VMs. These customers will have to write their own automation scripts, \n",
      "triggers, and rules to achieve autoscaling. With VMs comes the responsibility of \n",
      "maintaining them. The patching, updating, and upgrading of VMs is the responsibility\n",
      "---------------\n",
      "of owners. Architects should think about both planned and unplanned maintenance. \n",
      "How these VMs should be patched, the order, grouping, and other factors must be \n",
      "considered to ensure that neither the scalability nor the availability of an application is \n",
      "compromised. To help alleviate such problems, Azure provides VM scale sets (VMSS) as \n",
      "a solution, which we will discuss next.\n",
      "---------------\n",
      "50 | Azure solution availability, scalability, and monitoring\n",
      "VM scale sets\n",
      "VMSSes are Azure compute resources that you can use to deploy and manage a set \n",
      "of identical VMs. With all VMs configured in the same way, scale sets are designed \n",
      "to support true autoscaling, and no pre‑provisioning of VMs is required. It helps in \n",
      "provisioning multiple identical VMs that are connected to each other through a virtual \n",
      "network and subnet.\n",
      "A VMSS consists of multiple VMs, but they are managed at the VMSS level. All VMs are \n",
      "part of this unit and any changes made are applied to the unit, which, in turn, applies it \n",
      "to those VMs that are using a predetermined algorithm:\n",
      "Figure 2.12: A VM scale set\n",
      "This enables these VMs to be load balanced using an Azure load balancer or an \n",
      "application gateway. The VMs could be either Windows or Linux VMs. They can run \n",
      "automated scripts using a PowerShell extension and they can be managed centrally\n",
      "---------------\n",
      "using a state configuration. They can be monitored as a unit, or individually using Log \n",
      "Analytics.\n",
      "VMSSes can be provisioned from the Azure portal, the Azure CLI, Azure Resource \n",
      "Manager templates, REST APIs, and PowerShell cmdlets. It is possible to invoke REST \n",
      "APIs and the Azure CLI from any platform, environment, or OS, and in any language.\n",
      "Many of Azure's services already use VMSSes as their underlying architecture. Among \n",
      "them are Azure Batch, Azure Service Fabric, and Azure Container Service. Azure \n",
      "Container Service, in turn, provisions Kubernetes and DC/OS on these VMSSes.\n",
      "---------------\n",
      "VM scale sets | 51\n",
      "VMSS architecture\n",
      "VMSSes allow the creation of up to 1,000 VMs in a scale set when using a platform \n",
      "image, and 100 VMs if using a custom image. If the number of VMs is less than 100 in a \n",
      "scale set, they are placed in a single availability set; however, if the number is greater \n",
      "than 100, multiple availability sets are created (known as placement groups), and \n",
      "VMs are distributed among these availability sets. We know from Chapter 1, Getting \n",
      "started with Azure, that VMs in an availability set are placed on separate fault and \n",
      "update domains. Availability sets related to VMSSes have five fault and update domains \n",
      "by default. VMSSes provide a model that holds metadata information for the entire set. \n",
      "Changing this model and applying changes impacts all VM instances. This information \n",
      "includes the maximum and minimum number of VM instances, the OS SKU and version, \n",
      "the current number of VMs, fault and update domains, and more. This is demonstrated \n",
      "in Figure 2.13:\n",
      "---------------\n",
      "Figure 2.13: VMs in an availability set\n",
      "VMSS scaling\n",
      "Scaling refers to increasing or decreasing compute and storage resources. A VMSS is \n",
      "a feature-rich resource that makes scaling easy and efficient. It provides autoscaling, \n",
      "which helps in scaling up or down based on external events and data such as CPU and \n",
      "memory usage. Some of the VMSS scaling features are given here.\n",
      "Horizontal versus vertical scaling\n",
      "Scaling can be horizontal or vertical, or both. Horizontal scaling is another name for \n",
      "scaling out and in, while vertical scaling refers to scaling up and down.\n",
      "---------------\n",
      "52 | Azure solution availability, scalability, and monitoring\n",
      "Capacity\n",
      "VMSSes have a capacity property that determines the number of VMs in a scale set. A \n",
      "VMSS can be deployed with zero as a value for this property. It will not create a single \n",
      "VM; however, if you provision a VMSS by providing a number for the capacity property, \n",
      "that number of VMs are created.\n",
      "Autoscaling\n",
      "The autoscaling of VMs in a VMSS refers to the addition or removal of VM instances \n",
      "based on the configured environment in order to meet the performance and scalability \n",
      "demands of an application. Generally, in the absence of a VMSS, this is achieved using \n",
      "automation scripts and runbooks.\n",
      "VMSSes help in this automation process with the support of configuration. Instead of \n",
      "writing scripts, a VMSS can be configured for autoscaling up and down.\n",
      "Autoscaling uses multiple integrated components to achieve its end goal. Autoscaling \n",
      "entails continuously monitoring VMs and collecting telemetry data about them.\n",
      "---------------\n",
      "This data is stored, combined, and then evaluated against a set of rules to determine \n",
      "whether autoscaling should be triggered. The trigger could be to scale out or scale in. It \n",
      "could also be to scale up or down. \n",
      "The autoscaling mechanism uses diagnostic logs for collecting telemetry data from \n",
      "VMs. These logs are stored in storage accounts as diagnostic metrics. The autoscaling \n",
      "mechanism also uses the Application Insights monitoring service, which reads these \n",
      "metrics, combines them, and stores them in a storage account.\n",
      "Background autoscaling jobs run continually to read Application Insights' storage data, \n",
      "evaluate it based on all the rules configured for autoscaling, and, if any of the rules or \n",
      "combination of rules are met, run the process of autoscaling. The rules can take into \n",
      "consideration the metrics from guest VMs and the host server.\n",
      "The rules defined using the property descriptions are available at https:/ /docs.\n",
      "---------------\n",
      "microsoft.com/azure/virtual‑machine‑scale‑sets/virtual‑machine‑scale‑sets‑\n",
      "autoscale‑overview.\n",
      "---------------\n",
      "VM scale sets | 53\n",
      "The VMSS autoscale architecture is shown in Figure 2.14:\n",
      "Figure 2.14: VMSS autoscale architecture\n",
      "Autoscaling can be configured for scenarios that are more complex than general \n",
      "metrics available from environments. For example, scaling could be based on any of the \n",
      "following:\n",
      "• A specific day\n",
      "• A recurring schedule such as weekends\n",
      "• Weekdays versus weekends\n",
      "• Holidays and one‑off events\n",
      "• Multiple resource metrics\n",
      "---------------\n",
      "54 | Azure solution availability, scalability, and monitoring\n",
      "These can be configured using the schedule property of Application Insights resources, \n",
      "which help in registering rules.\n",
      "Architects should ensure that at least two actions—scale out and scale in—are \n",
      "configured together. Scaling a configuration in or out will not help in achieving the \n",
      "scaling benefits provided by VMSSes.\n",
      "To summarize, we have covered the scalability options in Azure and the detailed \n",
      "scaling features in the case of IaaS and PaaS to meet your business requirements. If \n",
      "you recall the shared responsibility model, you'll remember that platform upgrades and \n",
      "maintenance should be done by the cloud provider. In this case, Microsoft takes care of \n",
      "upgrades and maintenance related to the platform. Let's see how this is achieved in the \n",
      "next section.\n",
      "Upgrades and maintenance\n",
      "After a VMSS and applications are deployed, they need to be actively maintained.\n",
      "---------------\n",
      "Planned maintenance should be conducted periodically to ensure that both the \n",
      "environment and application are up to date with the latest features, from a security and \n",
      "resilience point of view.\n",
      "Upgrades can be associated with applications, the guest VM instance, or the image \n",
      "itself. Upgrades can be quite complex because they should happen without affecting the \n",
      "availability, scalability, and performance of environments and applications. To ensure \n",
      "that updates can take place one instance at a time using rolling upgrade methods, it is \n",
      "important that a VMSS supports and provides capabilities for these advanced scenarios.\n",
      "There is a utility provided by the Azure team to manage updates for VMSSes. It's a \n",
      "Python‑based utility that can be downloaded from https:/ /github.com/gbowerman/\n",
      "vmssdashboard. It makes REST API calls to Azure to manage scale sets. This utility can \n",
      "be used to start, stop, upgrade, and reimage VMs on in an FD or group of VMs, as shown \n",
      "in Figure 2.15:\n",
      "---------------\n",
      "Upgrades and maintenance | 55\n",
      "Figure 2.15: Utility for managing VMSS updates\n",
      "Since you have a basic understanding of upgrade and maintenance, let's see how \n",
      "application updates are done in VMSSes.\n",
      "---------------\n",
      "56 | Azure solution availability, scalability, and monitoring\n",
      "Application updates\n",
      "Application updates in VMSSes should not be executed manually. They must be run as \n",
      "part of the release management and pipelines that use automation. Moreover, an update \n",
      "should happen one application instance at a time and not affect the overall availability \n",
      "and scalability of an application. Configuration management tools, such as Desired \n",
      "State Configuration (DSC), should be deployed to manage application updates. The DSC \n",
      "pull server can be configured with the latest version of the application configuration \n",
      "and it should be applied on a rolling basis to each instance.\n",
      "In the next section, we will focus on how the updates are done on the guest OS.\n",
      "Guest updates\n",
      "Updates to VMs are the responsibility of the administrator. Azure is not responsible \n",
      "for patching guest VMs. Guest updates are in preview mode and users should control\n",
      "---------------\n",
      "patching manually or use custom automation methods, such as runbooks and scripts. \n",
      "However, rolling patch upgrades are in preview mode and can be configured in the \n",
      "Azure Resource Manager template using an upgrade policy, as follows:\n",
      "\"upgradePolicy\": { \n",
      "\"mode\": \"Rolling\", \n",
      "\"automaticOSUpgrade\": \"true\" or \"false\", \n",
      "  \"rollingUpgradePolicy\": { \n",
      "    \"batchInstancePercent\": 20, \n",
      "    \"maxUnhealthyUpgradedInstanceCount\": 0, \n",
      "    \"pauseTimeBetweenBatches\": \"PT0S\" \n",
      "  } \n",
      "}\n",
      "Now that we know how guest updates are managed in Azure, let's see how image \n",
      "updates are accomplished.\n",
      "Image updates\n",
      "A VMSS can update the OS version without any downtime. OS updates involve changing \n",
      "the version or SKU of the OS or changing the URI of a custom image. Updating without \n",
      "downtime means updating VMs one at a time or in groups (such as one FD at a time) \n",
      "rather than all at once. By doing so, any VMs that are not being upgraded can keep \n",
      "running.\n",
      "---------------\n",
      "So far, we have discussed updates and maintenance. Let's now examine what the best \n",
      "practices of scaling for VMSSes are.\n",
      "---------------\n",
      "Upgrades and maintenance | 57\n",
      "Best practices of scaling for VMSSes\n",
      "In this section, we will go through some of the best practices that applications should \n",
      "implement to take advantage of the scaling capability provided by VMSSes.\n",
      "The preference for scaling out\n",
      "Scaling out is a better scaling solution than scaling up. Scaling up or down means \n",
      "resizing VM instances. When a VM is resized, it generally needs to be restarted, which \n",
      "has its own disadvantages. First, there is downtime for the machine. Second, if there \n",
      "are active users connected to the application on that instance, they might face a lack of \n",
      "availability of the application, or they might even lose transactions. Scaling out does not \n",
      "impact existing VMs; rather, it provisions newer machines and adds them to the group.\n",
      "New instances versus dormant instances\n",
      "Scaling new instances can take two broad approaches: creating the new instance \n",
      "from scratch, which requires installing applications, configuring, and testing them;\n",
      "---------------\n",
      "or starting the dormant, sleeping instances when they are needed due to scalability \n",
      "pressure on other servers.\n",
      "Configuring the maximum and minimum number of instances appropriately\n",
      "Setting a value of two for both the minimum and maximum instance counts, with the \n",
      "current instance count being two, means no scaling action can occur. There should be \n",
      "an adequate difference between the maximum and minimum instance counts, which \n",
      "are inclusive. Autoscaling always scales between these limits.\n",
      "Concurrency\n",
      "Applications are designed for scalability to focus on concurrency. Applications should \n",
      "use asynchronous patterns to ensure that client requests do not wait indefinitely \n",
      "to acquire resources if resources are busy serving other requests. Implementing \n",
      "asynchronous patterns in code ensures that threads do not wait for resources and \n",
      "that systems are exhausted of all available threads. Applications should implement the \n",
      "concept of timeouts if intermittent failures are expected.\n",
      "---------------\n",
      "Designing stateless applications\n",
      "Applications and services should be designed to be stateless. Scalability can \n",
      "become a challenge to achieve with stateful services, and it is quite easy to scale \n",
      "stateless services. With states comes the requirement for additional components \n",
      "and implementations, such as replication, centralized or decentralized repository, \n",
      "maintenance, and sticky sessions. All these are impediments on the path to scalability. \n",
      "Imagine a service maintaining an active state on a local server. Irrespective of the \n",
      "number of requests on the overall application or the individual server, the subsequent \n",
      "requests must be served by the same server. Subsequent requests cannot be processed \n",
      "by other servers. This makes scalability implementation a challenge.\n",
      "---------------\n",
      "58 | Azure solution availability, scalability, and monitoring\n",
      "Caching and the Content Distribution Network (CDN)\n",
      "Applications and services should take advantage of caching. Caching helps \n",
      "eliminate multiple subsequent calls to either databases or filesystems. This helps in \n",
      "making resources available and free for more requests. The CDN is another mechanism \n",
      "that is used to cache static files, such as images and JavaScript libraries. They are \n",
      "available on servers across the globe. They also make resources available and free for \n",
      "additional client requests—this makes applications highly scalable.\n",
      "N+1 design\n",
      "N+1 design refers to building redundancy within the overall deployment for each \n",
      "component. It means to plan for some redundancy even when it is not required. This \n",
      "could mean additional VMs, storage, and network interfaces.\n",
      "Considering the preceding best practices while designing workloads using VMSSes\n",
      "---------------\n",
      "will improve the scalability of your applications. In the next section, we will explore \n",
      "monitoring.\n",
      "Monitoring\n",
      "Monitoring is an important architectural concern that should be part of any solution, \n",
      "big or small, mission‑critical or not, cloud‑based or not—it should not be neglected.\n",
      "Monitoring refers to the act of keeping track of solutions and capturing various \n",
      "telemetry information, processing it, identifying the information that qualifies for \n",
      "alerts based on rules, and raising them. Generally, an agent is deployed within the \n",
      "environment and monitors it, sending telemetry information to a centralized server, \n",
      "where the rest of the processing of generating alerts and notifying stakeholders takes \n",
      "place.\n",
      "Monitoring takes both proactive and reactive actions and measures against a solution. \n",
      "It is also the first step toward auditing a solution. Without the ability to monitor log \n",
      "records, it is difficult to audit a system from various perspectives, such as security,\n",
      "---------------\n",
      "performance, and availability.\n",
      "Monitoring helps us identify availability, performance, and scalability issues before \n",
      "they arise. Hardware failure, software misconfiguration, and patch update challenges \n",
      "can be discovered well before they impact users through monitoring, and performance \n",
      "degradation can be fixed before it happens.\n",
      "Monitoring reactively logs pinpoint areas and locations that are causing issues, \n",
      "identifies the issues, and enables faster and better repairs.\n",
      "---------------\n",
      "Monitoring | 59\n",
      "Teams can identify patterns of issues using monitoring telemetry information and \n",
      "eliminate them by innovating new solutions and features.\n",
      "Azure is a rich cloud environment that provides multiple rich monitoring features \n",
      "and resources to monitor not only cloud‑based deployment but also on‑premises \n",
      "deployment.\n",
      "Azure monitoring\n",
      "The first question that should be answered is, \"What must we monitor?\" This question \n",
      "becomes more important for solutions that are deployed on the cloud because of the \n",
      "constrained control over them.\n",
      "There are some important components that should be monitored. They include the \n",
      "following:\n",
      "• Custom applications\n",
      "• Azure resources\n",
      "• Guest OSes (VMs)\n",
      "• Host OSes (Azure physical servers)\n",
      "• Azure infrastructure\n",
      "There are different Azure logging and monitoring services for these components, and \n",
      "they are discussed in the following sections.\n",
      "Azure activity logs\n",
      "Previously known as audit logs and operational logs, activity logs are control‑plane\n",
      "---------------\n",
      "events on the Azure platform. They provide information and telemetry information at \n",
      "the subscription level, instead of the individual resource level. They track information \n",
      "about all changes that happen at the subscription level, such as creating, deleting, \n",
      "and updating resources using Azure Resource Manager (ARM). Activity logs help \n",
      "us discover the identity of (such as service principal, users, or groups), and perform \n",
      "actions on (such as write or update), resources (for example, storage, virtual machines, \n",
      "or SQL databases) at any given point in time. They provide information about resources \n",
      "that are modified in their configuration, but not their inner workings and execution. For \n",
      "example, you can get the logs for starting a VM, resizing a VM, or stopping a VM.\n",
      "The next topic that we are going to discuss is diagnostic logs.\n",
      "---------------\n",
      "60 | Azure solution availability, scalability, and monitoring\n",
      "Azure diagnostic logs\n",
      "The information originating within the inner workings of Azure resources is captured \n",
      "in what are known as diagnostic logs. They provide telemetry information about the \n",
      "operations of resources that are inherent to the resources. Not every resource provides \n",
      "diagnostic logs, and resources that provide logs on their own content are completely \n",
      "different from other resources. Diagnostic logs are configured individually for each \n",
      "resource. Examples of diagnostic logs include storing a file in a container in a blob in a \n",
      "storage account.\n",
      "The next type of log that we are going to discuss is application logs.\n",
      "Azure application logs\n",
      "Application logs can be captured by Application Insights resources and can be managed \n",
      "centrally. They get information about the inner workings of custom applications, such \n",
      "as their performance metrics and availability, and users can get insights from them in\n",
      "---------------\n",
      "order to manage them better.\n",
      "Lastly, we have guest and host OS logs. Let's understand what these are.\n",
      "Guest and host OS logs\n",
      "Both guest and host OS logs are offered to users using Azure Monitor. They provide \n",
      "information about the statuses of host and guest OSes:\n",
      "Figure 2.16: Logging in Azure\n",
      "---------------\n",
      "Monitoring | 61\n",
      "The important Azure resources related to monitoring are Azure Monitor, Azure \n",
      "Application Insights, and Log Analytics, previously known as Operational Insights.\n",
      "There are other tools, such as System Center Operations Manager (SCOM), that are \n",
      "not part of the cloud feature but can be deployed on IaaS‑based VMs to monitor any \n",
      "workload on Azure or an on‑premises datacenter. Let's discuss the three monitoring \n",
      "resources in the following section.\n",
      "Azure Monitor\n",
      "Azure Monitor is a central tool and resource that provides complete management \n",
      "features that allow you to monitor an Azure subscription. It provides management \n",
      "features for activity logs, diagnostic logs, metrics, Application Insights, and Log \n",
      "Analytics. It should be treated as a dashboard and management resource for all other \n",
      "monitoring capabilities.\n",
      "Our next topic is Azure Application Insights.\n",
      "Azure Application Insights\n",
      "Azure Application Insights provides centralized, Azure‑scale monitoring, logs, and\n",
      "---------------\n",
      "metrics capabilities to custom applications. Custom applications can send metrics, logs, \n",
      "and other telemetry information to Azure Application Insights. It also provides rich \n",
      "reporting, dashboarding, and analytics capabilities to get insights from incoming data \n",
      "and act on them.\n",
      "Now that we have covered Application Insights, let's look at another similar service \n",
      "called Azure Log Analytics.\n",
      "Azure Log Analytics\n",
      "Azure Log Analytics enables the centralized processing of logs and generates insights \n",
      "and alerts from them. Activity logs, diagnostic logs, application logs, event logs, and \n",
      "even custom logs can send information to Log Analytics, which can further provide rich \n",
      "reporting, dashboarding, and analytics capabilities to get insights from incoming data \n",
      "and act on them.\n",
      "Now that we know the purpose of Log Analytics, let's discuss how logs are stored in a \n",
      "Log Analytics workspace and how they can be queried.\n",
      "Logs\n",
      "---------------\n",
      "A Log Analytics workspace provides search capabilities to search for specific log entries, \n",
      "export all telemetry data to Excel and/or Power BI, and search a query language called \n",
      "Kusto Query Language (KQL), which is similar to SQL.\n",
      "---------------\n",
      "62 | Azure solution availability, scalability, and monitoring\n",
      "The Log Search screen is shown here:\n",
      "Figure 2.17: Log search in a Log Analytics workspace\n",
      "In the next section, we will be covering Log Analytics solutions, which are like additional \n",
      "capabilities in a Log Analytics workspace.\n",
      "Solutions\n",
      "Solutions in Log Analytics are further capabilities that can be added to a workspace, \n",
      "capturing additional telemetry data that is not captured by default. When these \n",
      "solutions are added to a workspace, appropriate management packs are sent to all the \n",
      "agents connected to the workspace so that they can configure themselves to capture \n",
      "solution-specific data from VMs and containers and then send it to the Log Analytics \n",
      "workspace. Monitoring solutions from Microsoft and partners are available from Azure \n",
      "Marketplace.\n",
      "---------------\n",
      "Monitoring | 63\n",
      "Azure provides lots of Log Analytics solutions for tracking and monitoring different \n",
      "aspects of environments and applications. At a minimum, a set of solutions that are \n",
      "generic and applicable to almost any environment should be added to the workspace:\n",
      "• Capacity and performance\n",
      "• Agent health\n",
      "• Change tracking\n",
      "• Containers\n",
      "• Security and audit\n",
      "• Update management\n",
      "• Network performance monitoring\n",
      "Another key aspect of monitoring is alerts. Alerts help to notify the right people during \n",
      "any monitored event. In the next section, we will cover alerts.\n",
      "Alerts\n",
      "Log Analytics allows us to generate alerts in relation to ingested data. It does so by \n",
      "running a pre-defined query composed of conditions for incoming data. If it finds \n",
      "any records that fall within the ambit of the query results, it generates an alert. Log \n",
      "Analytics provides a highly configurable environment for determining the conditions\n",
      "---------------\n",
      "for generating alerts, time windows in which the query should return the records, time \n",
      "windows in which the query should be executed, and actions to be taken when the \n",
      "query returns an alert:\n",
      "Figure 2.18: Configuring alerts through Log Analytics\n",
      "---------------\n",
      "64 | Azure solution availability, scalability, and monitoring\n",
      "Let's go through the steps for configuring alerts through Log Analytics:\n",
      "1. The first step in configuring an alert is to add a new alert rule from the Azure \n",
      "portal or automation from the alert menu of the Log Analytics resource.\n",
      "2. From the resultant panel, select a scope for the alert rule. The scope determines \n",
      "which resource should be monitored for alerts—it could be a resource instance, \n",
      "such as an Azure storage account, a resource type, such as an Azure VM, a \n",
      "resource group, or a subscription:\n",
      " \n",
      "Figure 2.19: Selecting a resource for the alert\n",
      "---------------\n",
      "Monitoring | 65\n",
      "3. Following resource selection, conditions must be set for the alert. The condition \n",
      "determines the rule that is evaluated against the logs and metrics on the selected \n",
      "resource, and only after the condition turns true is an alert generated. There \n",
      "are a ton of metrics and logs available for generating conditions. In the following \n",
      "example, an alert is created with a static threshold value of 80% for Percentage \n",
      "CPU (Avg) and the data is to be collected every five minutes and evaluated every \n",
      "minute:\n",
      "Figure 2.20: Creating an alert for Percentage CPU (Avg)\n",
      "Alerts also support dynamic thresholds, which use machine learning to learn the \n",
      "historical behavior of metrics and detect irregularities that could indicate service \n",
      "issues.\n",
      "---------------\n",
      "66 | Azure solution availability, scalability, and monitoring\n",
      "4. Finally, create an action group or reuse an existing group that determines \n",
      "notifications regarding alerts to stakeholders. The Action Groups section allows \n",
      "you to configure things that should follow an alert. Generally, there should be a \n",
      "remedial and/or notification action. Log Analytics provides eight different ways \n",
      "to create a new action. They can be combined in any way you like. An alert will \n",
      "execute any or all of the following configured actions:\n",
      "• Email/SMS/push/voice notification: This sends an email/SMS/push/voice \n",
      "notification to the configured recipients.\n",
      "• Webhooks: A webhook runs an arbitrary external process using an HTTP POST \n",
      "mechanism. For example, a REST API can be executed, or the Service Manager/\n",
      "ServiceNow APIs can be invoked to create a ticket.\n",
      "• Azure Functions: This runs an Azure function, passing the necessary payload \n",
      "and running the logic that the payload contains.\n",
      "---------------\n",
      "• Logic Apps: This executes a custom Logic Apps workflow.\n",
      "• Email Azure Resource Manager Role: This emails a holder of an Azure Resource \n",
      "Manager role, such as an owner, contributor, or reader. \n",
      "• Secure webhook: A webhook runs an arbitrary external process using an HTTP \n",
      "POST mechanism. Webhooks are protected using an identity provider, such as \n",
      "Azure Active Directory.\n",
      "• Automation runbooks: This action executes Azure Automation runbooks. \n",
      "• ITSM: ITSM solutions should be provisioned before using this option. It helps \n",
      "with connecting and sending information to ITSM systems.\n",
      "5. After all of this configuration, you need to provide the Name, Description, \n",
      "and Severity values for the alert rule to generate it.\n",
      "As mentioned at the beginning of this section, alerts play a vital role in monitoring that \n",
      "helps authorized personnel to take necessary actions based on the alert that's triggered.\n",
      "---------------\n",
      "Summary | 67\n",
      "Summary\n",
      "High availability and scalability are crucially important architectural concerns. \n",
      "Almost every application and every architect try to implement high availability. Azure \n",
      "is a mature platform that understands the need for these architectural concerns \n",
      "in applications and provides resources to implement them at multiple levels. \n",
      "These architectural concerns are not an afterthought, and they should be part of \n",
      "the application development life cycle, starting from the planning phase itself. \n",
      "Monitoring is an important architectural aspect of any solution. It is also the first step \n",
      "toward being able to audit an application properly. It enables operations to manage \n",
      "a solution, both reactively and proactively. It provides the necessary records for \n",
      "troubleshooting and fixing the issues that might arise from platforms and applications. \n",
      "There are many resources in Azure that are specific to implementing monitoring\n",
      "---------------\n",
      "for Azure, other clouds, and on‑premises datacenters. Application Insights and Log \n",
      "Analytics are two of the most important resources in this regard. Needless to say, \n",
      "monitoring is a must for making your solutions and products better by innovating based \n",
      "on insights derived from monitoring data. \n",
      "This chapter was purely about the availability, scalability, and monitoring of solutions; \n",
      "the next chapter is about design patterns related to virtual networks, storage accounts, \n",
      "regions, availability zones, and availability sets. While designing solutions in the cloud, \n",
      "these principles are very important in building cost‑effective solutions with increased \n",
      "productivity and availability.\n",
      "---------------\n",
      "In the previous chapter, you got an overview of the Azure cloud and learned about some \n",
      "of the important concepts related to it. This chapter is about Azure cloud patterns \n",
      "that are related to virtual networks, storage accounts, regions, Availability Zones, and \n",
      "Availability Sets. These are important constructs that affect the final architecture \n",
      "delivered to customers in terms of cost, efficiencies, and overall productivity. The \n",
      "chapter also briefly discusses the cloud patterns that help us to implement scalability \n",
      "and performance for an architecture.\n",
      "In this chapter, we'll cover the following topics:\n",
      "• Azure Virtual Network design\n",
      "• Azure Storage design\n",
      "• Azure Availability Zones, regions, and Availability Sets\n",
      "• Azure design patterns related to messaging, performance, and scalability\n",
      "Design pattern – \n",
      "Networks, storage, \n",
      "messaging, and events\n",
      "3\n",
      "---------------\n",
      "70 | Design pattern – Networks, storage, messaging, and events\n",
      "Azure Availability Zones and Regions\n",
      "Azure is backed up by large datacenters interconnected into a single large network. The \n",
      "datacenters are grouped together, based on their physical proximity, into Azure regions. \n",
      "For example, datacenters in Western Europe are available to Azure users in the West \n",
      "Europe region. Users cannot choose their preferred datacenter. They can select their \n",
      "Azure region and Azure will allocate an appropriate datacenter.\n",
      "Choosing an appropriate region is an important architectural decision as it affects:\n",
      "• The availability of resources\n",
      "• Data and privacy compliance\n",
      "• The performance of the application\n",
      "• The cost of running applications\n",
      "Let's discuss each of these points in detail.\n",
      "Availability of resources\n",
      "Not all resources are available in every Azure region. If your application architecture \n",
      "demands a resource that is not available in a region, choosing that region will not help.\n",
      "---------------\n",
      "Instead, a region should be chosen based on the availability of the resources required \n",
      "by the application. It might be that the resource is not available while developing \n",
      "the application architecture, and it could be on Azure's roadmap to make it available \n",
      "subsequently.\n",
      "For example, Log Analytics is not available in all regions. If your data sources are in \n",
      "Region A and the Log Analytics workspace is in Region B, you need to pay for the \n",
      "bandwidth, which is the data egress charges from Region A to B. Similarly, some \n",
      "services can work with resources that are located in the same region. For instance, if \n",
      "you would like to encrypt the disks of your virtual machine that is deployed in Region \n",
      "A, you need to have Azure Key Vault deployed in Region A to store the encryption \n",
      "keys. Before deploying any services, you need to check whether your dependency \n",
      "services are available in that region. A good source to check the availability of Azure\n",
      "---------------\n",
      "products across regions is this product page: https:/ /azure.microsoft.com/global-\n",
      "infrastructure/services. \n",
      "Data and privacy compliance\n",
      "Each country has its own rules for data and privacy compliance. Some countries are \n",
      "very specific about storing their citizens' data in their own territories. Hence, such legal \n",
      "requirements should be taken into consideration for every application's architecture.\n",
      "---------------\n",
      "Virtual networks | 71\n",
      "Application performance\n",
      "The performance of an application is dependent on the network route taken by \n",
      "requests and responses to get to their destinations and back again. The location that \n",
      "is geographically closer to you may not always be the region with the lowest latency. \n",
      "We calculate distance in kilometers or miles, but latency is based on the route the \n",
      "packet takes. For example, an application deployed in Western Europe for Southeast \n",
      "Asian users will not perform as well as an application deployed to the East Asia region \n",
      "for users in that region. So, it's very important that you architect your solutions in the \n",
      "closest region to provide the lowest latency and thus the best performance.\n",
      "Cost of running applications\n",
      "The cost of Azure services differs from region to region. A region with an overall lower \n",
      "cost should be chosen. There is a complete chapter on cost management in this book\n",
      "---------------\n",
      "(Chapter 6, Cost management for Azure solutions), and it should be referred to for more \n",
      "details on cost.\n",
      "So far, we have discussed how to choose the right region to architect our solution. Now \n",
      "that we have a suitable region in mind for our solution, let's discuss how to design our \n",
      "virtual networks in Azure.\n",
      "Virtual networks\n",
      "Virtual networks should be thought of like a physical office or home LAN network \n",
      "setup. Conceptually, they are the same, although Azure Virtual Network (VNet) is \n",
      "implemented as a software-defined network backed up by a giant physical network \n",
      "infrastructure. \n",
      "A VNet is required to host a virtual machine. It provides a secure communication \n",
      "mechanism between Azure resources so that they can connect to each other. The \n",
      "VNets provide internal IP addresses to the resources, facilitate access and connectivity \n",
      "to other resources (including virtual machines on the same virtual network), route \n",
      "requests, and provide connectivity to other networks.\n",
      "---------------\n",
      "A virtual network is contained within a resource group and is hosted within a region, \n",
      "for example, West Europe. It cannot span multiple regions but can span all datacenters \n",
      "within a region, which means we can span virtual networks across multiple Availability \n",
      "Zones in a region. For connectivity across regions, virtual networks can be connected \n",
      "using VNet-to-VNet connectivity.\n",
      "---------------\n",
      "72 | Design pattern – Networks, storage, messaging, and events\n",
      "Virtual networks also provide connectivity to on-premises datacenters, enabling hybrid \n",
      "clouds. There are multiple types of VPN technologies that you can use to extend your \n",
      "on-premises datacenters to the cloud, such as site-to-site VPN and point-to-site VPN. \n",
      "There is also dedicated connectivity between Azure VNet and on-premises networks \n",
      "through the use of ExpressRoute.\n",
      "Virtual networks are free of charge. Every subscription can create up to 50 virtual \n",
      "networks across all regions. However, this number can be increased by reaching out to \n",
      "Azure Support. You will not be charged if data does not leave the region of deployment. \n",
      "At the time of writing, inbound and outbound data transfers within Availability Zones \n",
      "from the same region don't incur charges; however, billing will commence from  \n",
      "July 1, 2020.\n",
      "Information about networking limits is available in the Microsoft documentation at\n",
      "---------------\n",
      "https:/ /docs.microsoft.com/azure/azure-resource-manager/management/azure-\n",
      "subscription-service-limits. \n",
      "Architectural considerations for virtual networks\n",
      "Virtual networks, like any other resource, can be provisioned using ARM templates, \n",
      "REST APIs, PowerShell, and the CLI. It is quite important to plan the network topology \n",
      "as early as possible to avoid troubles later in the development life cycle. This is because \n",
      "once a network is provisioned and resources start using it, it is difficult to change it \n",
      "without having downtime. For example, moving a virtual machine from one network to \n",
      "another will require the virtual machine to be shut down.\n",
      "Let's look at some of the key architectural considerations while designing a \n",
      "virtual network.\n",
      "Regions\n",
      "VNet is an Azure resource and is provisioned within a region, such as West Europe. \n",
      "Applications spanning multiple regions will need separate virtual networks, one per\n",
      "---------------\n",
      "region, and they also need to be connected using VNet-to-VNet connectivity. There is a \n",
      "cost associated with VNet-to-VNet connectivity for both inbound and outbound traffic. \n",
      "There are no charges for inbound (ingress) data, but there are charges associated with \n",
      "outbound data.\n",
      "---------------\n",
      "Virtual networks | 73\n",
      "Dedicated DNS\n",
      "VNet by default uses Azure's DNS to resolve names within a virtual network, and \n",
      "it also allows name resolution on the internet. If an application wants a dedicated \n",
      "name resolution service or wants to connect to on-premises datacenters, it should \n",
      "provision its own DNS server, which should be configured within the virtual network \n",
      "for successful name resolution. Also, you can host your public domain in Azure and \n",
      "completely manage the records from the Azure portal, without the need to manage \n",
      "additional DNS servers.\n",
      "Number of virtual networks\n",
      "The number of virtual networks is affected by the number of regions, bandwidth usage \n",
      "by services, cross-region connectivity, and security. Having fewer but larger VNets \n",
      "instead of multiple smaller VNets will eliminate the management overhead.\n",
      "Number of subnets in each virtual network\n",
      "Subnets provide isolation within a virtual network. They can also provide a security\n",
      "---------------\n",
      "boundary. Network security groups (NSGs) can be associated with subnets, thereby \n",
      "restricting or allowing specific access to IP addresses and ports. Application \n",
      "components with separate security and accessibility requirements should be placed \n",
      "within separate subnets.\n",
      "IP ranges for networks and subnets\n",
      "Each subnet has an IP range. The IP range should not be so large that IPs are \n",
      "underutilized, but conversely shouldn't be so small that subnets become suffocated \n",
      "because of a lack of IP addresses. This should be considered after understanding the \n",
      "future IP address needs of the deployment.\n",
      "Planning should be done for IP addresses and ranges for Azure networks, subnets, \n",
      "and on-premises datacenters. There should not be an overlap to ensure seamless \n",
      "connectivity and accessibility.\n",
      "---------------\n",
      "74 | Design pattern – Networks, storage, messaging, and events\n",
      "Monitoring\n",
      "Monitoring is an important architectural facet and must be included within the overall \n",
      "deployment. Azure Network Watcher provides logging and diagnostic capabilities with \n",
      "insights on network performance and health. Some of the capabilities of the Azure \n",
      "Network Watcher are:\n",
      "• Diagnosing network traffic filtering problems to or from a virtual machine\n",
      "• Understanding the next hop of user-defined routes\n",
      "• Viewing the resources in a virtual network and their relationships\n",
      "• Communication monitoring between a virtual machine and an endpoint\n",
      "• Traffic capture from a virtual machine\n",
      "• NSG flow logs, which log information related to traffic flowing through an NSG. \n",
      "This data will be stored in Azure Storage for further analysis\n",
      "It also provides diagnostic logs for all the network resources in a resource group.\n",
      "Network performance can be monitored through Log Analytics. The Network\n",
      "---------------\n",
      "Performance Monitor management solution provides network monitoring capability. It \n",
      "monitors the health, availability, and reachability of networks. It is also used to monitor \n",
      "connectivity between public cloud and on-premises subnets hosting various tiers of a \n",
      "multi-tiered application. \n",
      "Security considerations\n",
      "Virtual networks are among the first components that are accessed by any resource \n",
      "on Azure. Security plays an important role in allowing or denying access to a resource. \n",
      "NSGs are the primary means of enabling security for virtual networks. They can \n",
      "be attached to virtual network subnets, and every inbound and outbound flow is \n",
      "constrained, filtered, and allowed by them.\n",
      "User-defined routing (UDR) and IP forwarding also helps in filtering and routing \n",
      "requests to resources on Azure. You can read more about UDR and forced tunneling at \n",
      "https:/ /docs.microsoft.com/azure/virtual-network/virtual-networks-udr-overview.\n",
      "---------------\n",
      "Azure Firewall is a fully managed Firewall as a Service offering from Azure. It can help \n",
      "you protect the resources in your virtual network. Azure Firewall can be used for packet \n",
      "filtering in both inbound and outbound traffic, among other things. Additionally, the \n",
      "threat intelligence feature of Azure Firewall can be used to alert and deny traffic from \n",
      "or to malicious domains or IP addresses. The data source for IP addresses and domains \n",
      "is Microsoft's threat intelligence feed.\n",
      "---------------\n",
      "Virtual networks | 75\n",
      "Resources can also be secured and protected by deploying network appliances (https:/ /\n",
      "azure.microsoft.com/solutions/network-appliances) such as Barracuda, F5, and other \n",
      "third-party components. \n",
      "Deployment\n",
      "Virtual networks should be deployed in their own dedicated resource groups. Network \n",
      "administrators should have the owner's permission to use this resource group, while \n",
      "developers or team members should have contributor permissions to allow them to \n",
      "create other Azure resources in other resource groups that consume services from the \n",
      "virtual network. \n",
      "It is also a good practice to deploy resources with static IP addresses in a dedicated \n",
      "subnet, while dynamic IP address–related resources can be on another subnet.\n",
      "Policies should not only be created so that only network administrators can delete the \n",
      "virtual network, but also should also be tagged for billing purposes.\n",
      "Connectivity\n",
      "---------------\n",
      "Resources in a region on a virtual network can talk seamlessly. Even resources on \n",
      "other subnets within a virtual network can talk to each other without any explicit \n",
      "configuration. Resources in multiple regions cannot use the same virtual network. The \n",
      "boundary of a virtual network is within a region. To make a resource communicate \n",
      "across regions, we need dedicated gateways at both ends to facilitate conversation. \n",
      "Having said that, if you would like to initiate a private connection between two \n",
      "networks in different regions, you can use Global VNet peering. With Global VNet \n",
      "peering, the communication is done via Microsoft's backbone network, which means no \n",
      "public internet, gateway, or encryption is required during the communication. If your \n",
      "virtual networks are in the same region with different address spaces, resources in one \n",
      "network will not be able to communicate with the other. Since they are in the same\n",
      "---------------\n",
      "region, we can use virtual network peering, which is similar to Global VNet peering; the \n",
      "only difference is that the source and destination virtual networks are deployed in the \n",
      "same region.\n",
      "As many organizations have a hybrid cloud, Azure resources sometimes need \n",
      "to communicate or connect with on-premises datacenters or vice versa. Azure \n",
      "virtual networks can connect to on-premises datacenters using VPN technology \n",
      "and ExpressRoute. In fact, one virtual network is capable of connecting to multiple \n",
      "on-premises datacenters and other Azure regions in parallel. As a best practice, each of \n",
      "these connections should be in their dedicated subnets within a virtual network.\n",
      "Now that we have explored several aspects of virtual networking, let's go ahead and \n",
      "discuss the benefits of virtual networks.\n",
      "---------------\n",
      "76 | Design pattern – Networks, storage, messaging, and events\n",
      "Benefits of virtual networks\n",
      "Virtual networks are a must for deploying any meaningful IaaS solution. Virtual \n",
      "machines cannot be provisioned without virtual networks. Apart from being almost a \n",
      "mandatory component in IaaS solutions, they provide great architectural benefits, some \n",
      "of which are outlined here:\n",
      "• Isolation: Most application components have separate security and bandwidth \n",
      "requirements and have different life cycle management. Virtual networks help to \n",
      "create isolated pockets for these components that can be managed independently \n",
      "of other components with the help of virtual networks and subnets.\n",
      "• Security: Filtering and tracking the users that are accessing resources is an \n",
      "important feature provided by virtual networks. They can stop access to malicious \n",
      "IP addresses and ports.\n",
      "• Extensibility: Virtual networks act like a private LAN on the cloud. They can\n",
      "---------------\n",
      "also be extended into a Wide Area Network (WAN) by connecting other virtual \n",
      "networks across the globe and can be extensions to on-premises datacenters.\n",
      "We have explored the benefits of virtual networks. Now the question is how we can \n",
      "leverage these benefits and design a virtual network to host our solution. In the next \n",
      "section, we will look at the design of virtual networks.\n",
      "Virtual network design\n",
      "In this section, we will consider some of the popular designs and use case scenarios of \n",
      "virtual networks.\n",
      "There can be multiple usages of virtual networks. A gateway can be deployed at each \n",
      "virtual network endpoint to enable security and transmit packets with integrity and \n",
      "confidentiality. A gateway is a must when connecting to on-premises networks; \n",
      "however, it is optional when using Azure VNet peering. Additionally, you can make use \n",
      "of the Gateway Transit feature to simplify the process of extending your on-premises\n",
      "---------------\n",
      "datacenter without deploying multiple gateways. Gateway Transit allows you to share \n",
      "an ExpressRoute or VPN gateway with all peered virtual networks. This will make it easy \n",
      "to manage and reduce the cost of deploying multiple gateways. \n",
      "In the previous section, we touched on peering and mentioned that we don't use \n",
      "gateways or the public internet to establish communication between peered networks. \n",
      "Let's move on and explore some of the design aspects of peering, and which peering \n",
      "needs to be used in particular scenarios.\n",
      "---------------\n",
      "Virtual network design | 77\n",
      "Connecting to resources within the same region and subscription\n",
      "Multiple virtual networks within the same region and subscription can be connected \n",
      "to each other. With the help of VNet peering, both networks can be connected and \n",
      "use the Azure private network backbone to transmit packets to each other. Virtual \n",
      "machines and services on these networks can talk to each other, subject to network \n",
      "traffic constraints. In the following diagram, VNet1 and VNet2 both are deployed in the \n",
      "West US region. However, the address space for VNet1 is 172.16.0.0/16, and for VNet2 \n",
      "it is 10.0.0.0/16. By default, resources in VNet1 will not be able to communicate with \n",
      "resources in VNet2. Since we have established VNet peering between the two, the \n",
      "resources will be able to communicate with each other via the Microsoft backbone \n",
      "network:\n",
      "Figure 3.1: VNet peering for resources with the same subscription\n",
      "Connecting to resources within the same region in another subscription\n",
      "---------------\n",
      "This scenario is very similar to the previous one except that the virtual networks are \n",
      "hosted in two different subscriptions. The subscriptions can be part of the same tenant \n",
      "or from multiple tenants. If both the resources are part of the same subscription and \n",
      "from the same region, the previous scenario applies. This scenario can be implemented \n",
      "in two ways: by using gateways or by using virtual network peering.\n",
      "West US\n",
      "VNet1\n",
      "172.16.0.0/16\n",
      "West US\n",
      "VNet2\n",
      "10.0.0.0/16\n",
      "VNet peering\n",
      "---------------\n",
      "78 | Design pattern – Networks, storage, messaging, and events\n",
      "If we are using gateways in this scenario, we need to deploy a gateway at both ends to \n",
      "facilitate communication. Here is the architectural representation of using gateways to \n",
      "connect two resources with different subscriptions:\n",
      "Figure 3.2: VNet peering for resources with different subscriptions using gateways\n",
      "However, the deployment of gateways incurs some charges. We will discuss VNet \n",
      "peering, and after that we will compare these two implementations to see which is best \n",
      "for our solution.\n",
      "While using peering, we are not deploying any gateways. Figure 3.3 represents how \n",
      "peering is done:\n",
      "Figure 3.3: VNet peering across subscriptions\n",
      "VNet peering provides a low-latency, high-bandwidth connection, and, as shown in \n",
      "the diagram, we are not deploying any gateways to make the communication happen. \n",
      "This is useful for scenarios such as data replication or failover. As mentioned earlier,\n",
      "---------------\n",
      "peering uses the Microsoft backbone network, which eliminates the need for the \n",
      "public internet.\n",
      "West US\n",
      "Virtual Network B\n",
      "Azure\n",
      "Subscription B\n",
      "West US\n",
      "Virtual Network A\n",
      "Azure\n",
      "Subscription A\n",
      "West US\n",
      "Virtual Network B\n",
      "Azure\n",
      "Subscription B\n",
      "West US\n",
      "Virtual Network A\n",
      "Azure\n",
      "Subscription A\n",
      "VNet peering\n",
      "---------------\n",
      "Virtual network design | 79\n",
      "Gateways are used in scenarios where encryption is needed and bandwidth is not a \n",
      "concern, as this will be a limited-bandwidth connection. However, this doesn't mean \n",
      "that there is a constraint on bandwidth. Also, this approach is used where customers \n",
      "are not so latency-sensitive.\n",
      "So far, we have looked at resources in the same region across subscriptions. In the next \n",
      "section, we will explore how to establish a connection between virtual networks in two \n",
      "different regions.\n",
      "Connecting to resources in different regions in another subscription\n",
      "In this scenario, we have two implementations again. One uses a gateway and the other \n",
      "uses Global VNet peering.\n",
      "Traffic will pass through the public network, and we will have gateways deployed at \n",
      "both ends to facilitate an encrypted connection. Figure 3.4 explains how  \n",
      "it's done:\n",
      "Figure 3.4: Connecting resources in different regions with different subscriptions\n",
      "---------------\n",
      "We will take a similar approach using Global VNet peering. Figure 3.5 shows how Global \n",
      "VNet peering is done:\n",
      "Figure 3.5: Connecting resources in different regions using Global VNet peering\n",
      "Virtual Network A\n",
      "West US\n",
      "10.0.0.0/16\n",
      "Virtual Network B\n",
      "East US\n",
      "172.0.0.0/16\n",
      "VNet peering\n",
      "---------------\n",
      "80 | Design pattern – Networks, storage, messaging, and events\n",
      "The considerations in choosing gateways or peering have already been discussed. These \n",
      "considerations are applicable in this scenario as well. So far, we have been connecting \n",
      "virtual networks across regions and subscriptions; we haven't talked about connecting \n",
      "an on-premises datacenter to the cloud yet. In the next section, we will discuss ways to \n",
      "do this. \n",
      "Connecting to on-premises datacenters\n",
      "Virtual networks can be connected to on-premises datacenters so that both Azure and \n",
      "on-premises datacenters become a single WAN. An on-premises network needs to be \n",
      "deployed on gateways and VPNs on both sides of the network. There are three different \n",
      "technologies available for this purpose.\n",
      "Site-to-site VPN\n",
      "This should be used when both the Azure network and the on-premises datacenter \n",
      "are connected to form a WAN, where any resource on both networks can access any\n",
      "---------------\n",
      "other resource on the networks irrespective of whether they are deployed on Azure or \n",
      "an on-premises datacenter. VPN gateways are required to be available on both sides of \n",
      "networks for security reasons. Also, Azure gateways should be deployed on their own \n",
      "subnets on virtual networks connected to on-premises datacenters. Public IP addresses \n",
      "must be assigned to on-premises gateways for Azure to connect to them over the \n",
      "public network:\n",
      "Figure 3.6: Site-to-site VPN architecture\n",
      "---------------\n",
      "Virtual network design | 81\n",
      "Point-to-site VPN\n",
      "This is similar to site-to-site VPN connectivity, but there is a single server or computer \n",
      "attached to the on-premises datacenter. It should be used when there are very few \n",
      "users or clients that would connect to Azure securely from remote locations. Also, there \n",
      "is no need for public IPs and gateways on the on-premises side in this case:\n",
      "Figure 3.7: Point-to-site VPN architecture\n",
      "---------------\n",
      "82 | Design pattern – Networks, storage, messaging, and events\n",
      "ExpressRoute\n",
      "Both site-to-site and point-to-site VPNs work using the public internet. They encrypt \n",
      "the traffic on the networks using VPN and certificates technology. However, there are \n",
      "applications that want to be deployed using hybrid technologies—some components on \n",
      "Azure, with others on an on-premises datacenter—and at the same time do not want \n",
      "to use the public internet to connect to Azure and on-premises datacenters. Azure \n",
      "ExpressRoute is the best solution for them, although it's a costly option compared to \n",
      "the two other types of connection. It is also the most secure and reliable provider, with \n",
      "higher speed and reduced latency because the traffic never hits the public internet. \n",
      "Azure ExpressRoute can help to extend on-premises networks into Azure over a \n",
      "dedicated private connection facilitated by a connectivity provider. If your solution is\n",
      "---------------\n",
      "network intensive, for example, a transactional enterprise application such as SAP, use \n",
      "of ExpressRoute is highly recommended.\n",
      "Figure 3.8: ExpressRoute network architecture\n",
      "Customer’s\n",
      "Network\n",
      " Partner\n",
      "Edge\n",
      "Microsoft\n",
      "Edge\n",
      "Primary Connection\n",
      "Secondary Connection\n",
      "ExpressRoute Circuit\n",
      "Microsoft Peering for Office 365, Dynamics 365, \n",
      "Azure Public services (Public IPs)\n",
      "Azure Private Peering for Virtual Networks\n",
      "---------------\n",
      "Virtual network design | 83\n",
      "Figure 3.9 shows all three types of hybrid networks:\n",
      "Figure 3.9: Different types of hybrid networks\n",
      "It is a good practice for virtual networks to have separate subnets for each logical \n",
      "component with separate deployments, from a security and isolation perspective.\n",
      "All the resources we deploy in Azure require networking in one way or another, so a \n",
      "deep understanding of networking is required when architecting solutions in Azure. \n",
      "Another key element is storage. In the next section, you will be learning more about \n",
      "storage.\n",
      "---------------\n",
      "84 | Design pattern – Networks, storage, messaging, and events\n",
      "Storage\n",
      "Azure provides a durable, highly available, and scalable storage solution through storage \n",
      "services.\n",
      "Storage is used to persist data for long-term needs. Azure Storage is available on the \n",
      "internet for almost every programming language.\n",
      "Storage categories\n",
      "Storage has two categories of storage accounts:\n",
      "• A standard storage performance tier that allows you to store tables, queues, files, \n",
      "blobs, and Azure virtual machine disks.\n",
      "• A premium storage performance tier supporting Azure virtual machine disks, at \n",
      "the time of writing. Premium storage provides higher performance and IOPS than \n",
      "standard general storage. Premium storage is currently available as data disks for \n",
      "virtual machines backed up by SSDs.\n",
      "Depending on the kind of data that is being stored, the storage is classified into \n",
      "different types. Let's look at the storage types and learn more about them.\n",
      "Storage types\n",
      "---------------\n",
      "Azure provides four types of general storage services:\n",
      "• Azure Blob storage: This type of storage is most suitable for unstructured data, \n",
      "such as documents, images, and other kinds of files. Blob storage can be in the \n",
      "Hot, Cool, or Archive tier. The Hot tier is meant for storing data that needs to be \n",
      "accessed very frequently. The Cool tier is for data that is less frequently accessed \n",
      "than data in the Hot tier and is stored for 30 days. Finally, the Archive tier is for \n",
      "archival purposes where the access frequency is very low.\n",
      "• Azure Table storage: This is a NoSQL key-attribute data store. It should be used \n",
      "for structured data. The data is stored as entities.\n",
      "• Azure Queue storage: This provides reliable message storage for storing large \n",
      "numbers of messages. These messages can be accessed from anywhere via HTTP \n",
      "or HTTPS calls. A queue message can be up to 64 KB in size. \n",
      "• Azure Files: This is shared storage based on the SMB protocol. It is typically\n",
      "---------------\n",
      "used for storing and sharing files. It also stores unstructured data, but its main \n",
      "distinction is that it is sharable via the SMB protocol.\n",
      "• Azure disks: This is block-level storage for Azure Virtual Machines.\n",
      "---------------\n",
      "Storage | 85\n",
      "These five storage types cater to different architectural requirements and cover almost \n",
      "all types of data storage facilities.\n",
      "Storage features\n",
      "Azure Storage is elastic. This means that you can store as little as a few megabytes or as \n",
      "much as petabytes of data. You do not need to pre-block the capacity, and it will grow \n",
      "and shrink automatically. Consumers just need to pay for the actual usage of storage. \n",
      "Here are some of the key benefits of using Azure Storage:\n",
      "• Azure Storage is secure. It can only be accessed using the SSL protocol. Moreover, \n",
      "access should be authenticated.\n",
      "• Azure Storage provides the facility to generate an account-level Secure Access \n",
      "Signature (SAS) token that can be used by storage clients to authenticate \n",
      "themselves. It is also possible to generate individual service-level SAS tokens for \n",
      "blobs, queues, tables, and files. \n",
      "• Data stored in Azure storage can be encrypted. This is known as secure data at \n",
      "rest.\n",
      "---------------\n",
      "• Azure Disk Encryption is used to encrypt the OS and data disks in IaaS virtual \n",
      "machines. Client-Side Encryption (CSE) and Storage Service Encryption (SSE) \n",
      "are both used to encrypt data in Azure Storage. SSE is an Azure Storage setting \n",
      "that ensures that data is encrypted while data is being written to storage and \n",
      "decrypted while it is read by the storage engine. This ensures that no application \n",
      "changes are required to enable SSE. In CSE, client applications can use the Storage \n",
      "SDK to encrypt data before it is sent and written to Azure Storage. The client \n",
      "application can later decrypt this data while it is read. This provides security for \n",
      "both data in transit and data at rest. CSE is dependent on secrets from Azure Key \n",
      "Vault. \n",
      "• Azure Storage is highly available and durable. What this means is that Azure \n",
      "always maintains multiple copies of Azure accounts. The location and number of \n",
      "copies depend on the replication configuration.\n",
      "---------------\n",
      "86 | Design pattern – Networks, storage, messaging, and events\n",
      "Azure provides the following replication settings and data redundancy options:\n",
      "• Locally redundant storage (LRS): Within a single physical location in the \n",
      "primary region, there will be three replicas of your data synchronously. From a \n",
      "billing standpoint, this is the cheapest option; however, it's not recommended \n",
      "for solutions that require high availability. LRS provides a durability level of \n",
      "99.999999999% for objects over a given year.\n",
      "• Zone-redundant storage (ZRS): In the case of LRS, the replicas were stored \n",
      "in the same physical location. In the case of ZRS, the data will be replicated \n",
      "synchronously across the Availability Zones in the primary region. As each of \n",
      "these Availability Zones is a separate physical location in the primary region, ZRS \n",
      "provides better durability and higher availability than LRS.\n",
      "• Geo-redundant storage (GRS): GRS increases the high availability by\n",
      "---------------\n",
      "synchronously replicating three copies of data within a single primary region \n",
      "using LRS. It also copies the data to a single physical location in the secondary \n",
      "region.\n",
      "• Geo-zone-redundant storage (GZRS): This is very similar to GRS, but instead \n",
      "of replicating data within a single physical location in the primary region, GZRS \n",
      "replicates it synchronously across three Availability Zones. As we discussed in the \n",
      "case of ZRS, since the Availability Zones are isolated physical locations within the \n",
      "primary region, GZRS has better durability and can be included in highly available \n",
      "designs.\n",
      "• Read-access geo-redundant storage (RA-GRS) and read-access geo-zone-\n",
      "redundant storage: The data replicated to the secondary region by GZRS or GRS \n",
      "is not available for read or write. This data will be used by the secondary region in \n",
      "the case of the failover of the primary datacenter. RA-GRS and RA-GZRS follow the \n",
      "same replication pattern as GRS and GZRS respectively; the only difference is that\n",
      "---------------\n",
      "the data replicated to the secondary region via RA-GRS or RA-GZRS can be read.\n",
      "Now that we have understood the various storage and connection options available on \n",
      "Azure, let's learn about the underlying architecture of the technology.\n",
      "Architectural considerations for storage accounts\n",
      "Storage accounts should be provisioned within the same region as other application \n",
      "components. This would mean using the same datacenter network backbone without \n",
      "incurring any network charges.\n",
      "---------------\n",
      "Storage | 87\n",
      "Azure Storage services have scalability targets for capacity, transaction rate, and \n",
      "bandwidth associated with each of them. A general storage account allows 500 TB \n",
      "of data to be stored. If there is a need to store more than 500 TB of data, then either \n",
      "multiple storage accounts should be created, or premium storage should be used. \n",
      "General storage performs at a maximum of 20,000 IOPS or 60 MB of data per second. \n",
      "Any requirements for higher IOPS or data managed per second will be throttled. If this \n",
      "is not enough for your applications from a performance perspective, either premium \n",
      "storage or multiple storage accounts should be used. For an account, the scalability \n",
      "limit for accessing tables is up to 20,000 (1 KB each) entries. The count of entities being \n",
      "inserted, updated, deleted, or scanned will contribute toward the target. A single queue \n",
      "can process approximately 2,000 messages (1 KB each) per second, and each of the\n",
      "---------------\n",
      "AddMessage, GetMessage, and DeleteMessage counts will be treated as a message. If these \n",
      "values aren't sufficient for your application, you should spread the messages across \n",
      "multiple queues.\n",
      "The size of virtual machines determines the size and capacity of the available data disks. \n",
      "While larger virtual machines have data disks with higher IOPS capacity, the maximum \n",
      "capacity will still be limited to 20,000 IOPS and 60 MB per second. It is to be noted \n",
      "that these are maximum numbers and so generally lower levels should be taken into \n",
      "consideration when finalizing storage architecture.\n",
      "At the time of writing, GRS accounts offer a 10 Gbps bandwidth target in the US for \n",
      "ingress and 20 Gbps if RA-GRS/GRS is enabled. When it comes to LRS accounts, the \n",
      "limits are on the higher side compared to GRS. For LRS accounts, ingress is 20 Gbps and \n",
      "egress is 30 Gbps. Outside the US, the values are lower: the bandwidth target is 10 Gbps\n",
      "---------------\n",
      "and 5 Gbps for egress. If there is a requirement for a higher bandwidth, you can reach \n",
      "out to Azure Support and they will be able to help you with further options.\n",
      "Storage accounts should be enabled for authentication using SAS tokens. They should \n",
      "not allow anonymous access. Moreover, for blob storage, different containers should be \n",
      "created with separate SAS tokens generated based on the different types and categories \n",
      "of clients accessing those containers. These SAS tokens should be periodically \n",
      "regenerated to ensure that the keys are not at risk of being cracked or guessed. You \n",
      "will learn more about SAS tokens and other security options in Chapter 8, Architecting \n",
      "secure applications on Azure.\n",
      "Generally, blobs fetched for blob storage accounts should be cached. We can determine \n",
      "whether the cache is stale by comparing its last modified property to re-fetch the \n",
      "latest blob.\n",
      "---------------\n",
      "88 | Design pattern – Networks, storage, messaging, and events\n",
      "Storage accounts provide concurrency features to ensure that the same file and data is \n",
      "not modified simultaneously by multiple users. They offer the following:\n",
      "• Optimistic concurrency: This allows multiple users to modify data simultaneously, \n",
      "but while writing, it checks whether the file or data has changed. If it has, it tells \n",
      "the users to re-fetch the data and perform the update again. This is the default \n",
      "concurrency for tables.\n",
      "• Pessimistic concurrency: When an application tries to update a file, it places a \n",
      "lock, which explicitly denies any updates to it by other users. This is the default \n",
      "concurrency for files when accessed using the SMB protocol.\n",
      "• Last writer wins: The updates are not constrained, and the last user updates \n",
      "the file irrespective of what was read initially. This is the default concurrency for \n",
      "queues, blobs, and files (when accessed using REST).\n",
      "---------------\n",
      "By this point, you should know what the different storage services are and how they can \n",
      "be leveraged in your solutions. In the next section, we will look at design patterns and \n",
      "see how they relate to architectural designs.\n",
      "Cloud design patterns\n",
      "Design patterns are proven solutions to known design problems. They are reusable \n",
      "solutions that can be applied to problems. They are not reusable code or designs \n",
      "that can be incorporated as is within a solution. They are documented descriptions \n",
      "and guidance for solving a problem. A problem might manifest itself in different \n",
      "contexts, and design patterns can help to solve it. Azure provides numerous services, \n",
      "with each service providing specific features and capabilities. Using these services \n",
      "is straightforward, but creating solutions by weaving multiple services together can \n",
      "be a challenge. Moreover, achieving high availability, super scalability, reliability, \n",
      "performance, and security for a solution is not a trivial task.\n",
      "---------------\n",
      "Azure design patterns provide ready solutions that can be tailored to individual \n",
      "problems. They help us to make highly available, scalable, reliable, secure, and \n",
      "performance-centric solutions on Azure. Although there are many patterns and some \n",
      "of the patterns are covered in detail in subsequent chapters, some of the messaging, \n",
      "performance, and scalability patterns are mentioned in this chapter. Also, links are \n",
      "provided for detailed descriptions of these patterns. These design patterns deserve a \n",
      "complete book by themselves. They have been mentioned here to make you aware of \n",
      "their existence and to provide references for further information.\n",
      "---------------\n",
      "Cloud design patterns | 89\n",
      "Messaging patterns\n",
      "Messaging patterns help connect services in a loosely coupled manner. What this \n",
      "means is that services never talk to each other directly. Instead, a service generates and \n",
      "sends a message to a broker (generally a queue) and any other service that is interested \n",
      "in that message can pick it and process it. There is no direct communication between \n",
      "the sender and receiver service. This decoupling not only makes services and the \n",
      "overall application more reliable but also more robust and fault tolerant. Receivers can \n",
      "receive and read messages at their own speed. \n",
      "Messaging helps the creation of asynchronous patterns. Messaging involves sending \n",
      "messages from one entity to another. These messages are created and forwarded by a \n",
      "sender, stored in durable storage, and finally consumed by recipients.\n",
      "The top architectural concerns addressed by messaging patterns are as follows:\n",
      "---------------\n",
      "• Durability: Messages are stored in durable storage, and applications can read \n",
      "them after they are received in case of a failover.\n",
      "• Reliability: Messages help implement reliability as they are persisted on disk and \n",
      "never lost.\n",
      "• Availability of messages: The messages are available for consumption by \n",
      "applications after the restoration of connectivity and before downtime.\n",
      "Azure provides Service Bus queues and topics to implement messaging patterns within \n",
      "applications. Azure Queue storage can also be used for the same purpose. \n",
      "Choosing between Azure Service Bus queues and Queue storage is about deciding on \n",
      "how long the message should be stored, the size of the message, latency, and cost. \n",
      "Azure Service Bus provides support for 256 KB messages, while Queue storage provides \n",
      "support for 64 KB messages. Azure Service Bus can store messages for an unlimited \n",
      "period, while Queue storage can store messages for 7 days. The cost and latency are \n",
      "higher with Service Bus queues.\n",
      "---------------\n",
      "Depending on your application's requirements and needs, the preceding factors \n",
      "should be considered before deciding on the best queue. In the next section, we will be \n",
      "discussing different types of messaging patterns.\n",
      "---------------\n",
      "90 | Design pattern – Networks, storage, messaging, and events\n",
      "The Competing Consumers pattern\n",
      "A single consumer of messages works in a synchronous manner unless the application \n",
      "implements the logic of reading messages asynchronously. The Competing Consumers \n",
      "pattern implements a solution in which multiple consumers are ready to process \n",
      "incoming messages, and they compete to process each message. This can lead to \n",
      "solutions that are highly available and scalable. This pattern is scalable because with \n",
      "multiple consumers, it is possible to process a higher number of messages in a smaller \n",
      "period. It is highly available because there should be at least one consumer to process \n",
      "messages even if some of the consumers crash.\n",
      "This pattern should be used when each message is independent of other messages. \n",
      "The messages by themselves contain all the information required for a consumer to \n",
      "complete a task. This pattern should not be used if there is any dependency among\n",
      "---------------\n",
      "messages. The consumers should be able to complete the tasks in isolation. Also, this \n",
      "pattern is applicable if there is variable demand for services. Additional consumers can \n",
      "be added or removed based on demand.\n",
      "A message queue is required to implement the Competing Consumers pattern. Here, \n",
      "patterns from multiple sources pass through a single queue, which is connected to \n",
      "multiple consumers at the other end. These consumers should delete each message \n",
      "after reading so that they are not re-processed:\n",
      "Figure 3.10: The Competing Consumers pattern\n",
      "Refer to the Microsoft documentation at https:/ /docs.microsoft.com/azure/\n",
      "architecture/patterns/competing-consumers to learn more about this pattern.\n",
      "---------------\n",
      "Cloud design patterns | 91\n",
      "The Priority Queue pattern\n",
      "There is often a need to prioritize some messages over others. This pattern is important \n",
      "for applications that provide different service-level agreements (SLAs) to consumers, \n",
      "which provide services based on differential plans and subscriptions.\n",
      "Queues follow the first-in, first-out pattern. Messages are processed in a sequence. \n",
      "However, with the help of the Priority Queue pattern, it is possible to fast-track the \n",
      "processing of certain messages due to their higher priority. There are multiple ways to \n",
      "implement this. If the queue allows you to assign priority and re-order messages based \n",
      "on priority, then even a single queue is enough to implement this pattern:\n",
      "Figure 3.11: The single Priority Queue pattern\n",
      "---------------\n",
      "92 | Design pattern – Networks, storage, messaging, and events\n",
      "However, if the queue cannot re-order messages, then separate queues can be created \n",
      "for different priorities, and each queue can have separate consumers associated with it:\n",
      "Figure 3.12: Using separate message queues for different priorities\n",
      "In fact, this pattern can use the Competing Consumer pattern to fast-track the \n",
      "processing of messages from each queue using multiple consumers. Refer to the \n",
      "Microsoft documentation at https:/ /docs.microsoft.com/azure/architecture/patterns/\n",
      "priority-queue to read more about the Priority Queue pattern.\n",
      "---------------\n",
      "Cloud design patterns | 93\n",
      "The Queue-Based Load Leveling pattern\n",
      "The Queue-Based Load Leveling pattern reduces the impact of peaks in demand on \n",
      "the availability and alertness of both tasks and services. Between a task and a service, a \n",
      "queue will act as a buffer. It can be invoked to handle the unexpected heavy loads that \n",
      "can cause service interruption or timeouts. This pattern helps to address performance \n",
      "and reliability issues. To prevent the service from getting overloaded, we will introduce \n",
      "a queue that will store a message until it's retrieved by the service. Messages will be \n",
      "taken from the queue by the service in a consistent manner and processed.\n",
      "Figure 3.13 shows how the Queue-Based Load Leveling pattern works:\n",
      "Figure 3.13: The Queue-Based Load Leveling pattern\n",
      "Even though this pattern helps to handle spikes of unexpected demand, it is not the \n",
      "best choice when you are architecting a service with minimal latency. Talking of latency,\n",
      "---------------\n",
      "which is a performance measurement, in the next section we will be focusing on \n",
      "performance and scalability patterns.\n",
      "Performance and scalability patterns\n",
      "Performance and scalability go together. Performance is the measure of how quickly \n",
      "a system can execute an action within a given time interval in a positive manner. On \n",
      "the other hand, scalability is the ability of a system to handle unexpected load without \n",
      "affecting the performance of the system, or how quickly the system can be expanded \n",
      "with the available resources. In this section, a couple of design patterns related to \n",
      "performance and scalability will be described.\n",
      "---------------\n",
      "94 | Design pattern – Networks, storage, messaging, and events\n",
      "The Command and Query Responsibility Segregation (CQRS) pattern\n",
      "CQRS is not an Azure-specific pattern but a general pattern that can be applied in any \n",
      "application. It increases the overall performance and responsiveness of an application. \n",
      "CQRS is a pattern that segregates the operations that read data (queries) from the \n",
      "operations that update data (commands) by using separate interfaces. This means that \n",
      "the data models used for querying and updates are different. The models can then be \n",
      "isolated, as shown in Figure 3.14, although that's not an absolute requirement.\n",
      "This pattern should be used when there are large and complex business rules executed \n",
      "while updating and retrieving data. Also, this pattern has an excellent use case in which \n",
      "one team of developers can focus on the complex domain model that is part of the write \n",
      "model, and another team can focus on the read model and the user interfaces. It is also\n",
      "---------------\n",
      "wise to use this pattern when the ratio of read to write is skewed. The performance of \n",
      "data reads should be fine-tuned separately from the performance of data writes.\n",
      "CQRS not only improves the performance of an application, but it also helps the design \n",
      "and implementation of multiple teams. Due to its nature of using separate models, \n",
      "CQRS is not suitable if you are using model and scaffolding generation tools:\n",
      "Figure 3.14: The CQRS pattern\n",
      "Refer to the Microsoft documentation at https:/ /docs.microsoft.com/azure/\n",
      "architecture/patterns/cqrs to read more about this pattern.\n",
      "---------------\n",
      "Cloud design patterns | 95\n",
      "The Event Sourcing pattern\n",
      "As most applications work with data and as the users are working with it, the classic \n",
      "approach for the application would be to maintain and update the current state of the \n",
      "data. Reading data from the source, modifying it, and updating the current state with \n",
      "the modified value is the typical data processing approach. However, there are some \n",
      "limitations:\n",
      "• As the update operations are directly made against the data store, this will slow \n",
      "down the overall performance and responsiveness.\n",
      "• If there are multiple users working on and updating the data, there may be \n",
      "conflicts and some of the relevant updates may fail.\n",
      "The solution for this is to implement the Event Sourcing pattern, where the changes \n",
      "will be recorded in an append-only store. A series of events will be pushed by the \n",
      "application code to the event store, where they will be persisted. The events persisted\n",
      "---------------\n",
      "in an event store act as a system of record about the current state of data. Consumers \n",
      "will be notified, and they can handle the events if needed once they are published.\n",
      "The Event Sourcing pattern is shown in Figure 3.15:\n",
      "Figure 3.15: The Event Sourcing pattern\n",
      "---------------\n",
      "96 | Design pattern – Networks, storage, messaging, and events\n",
      "More information about this pattern is available at https:/ /docs.microsoft.com/azure/\n",
      "architecture/patterns/event-sourcing. \n",
      "The Throttling pattern\n",
      "At times, there are applications that have very stringent SLA requirements from a \n",
      "performance and scalability perspective, irrespective of the number of users consuming \n",
      "the service. In these circumstances, it is important to implement the Throttling pattern \n",
      "because it can limit the number of requests that are allowed to be executed. The load \n",
      "on applications cannot be predicted accurately in all circumstances. When the load \n",
      "on an application spikes, throttling reduces pressure on the servers and services by \n",
      "controlling the resource consumption. The Azure infrastructure is a very good example \n",
      "of this pattern.\n",
      "This pattern should be used when meeting the SLA is a priority for applications to\n",
      "---------------\n",
      "prevent some users from consuming more resources than allocated, to optimize spikes \n",
      "and bursts in demand, and to optimize resource consumption in terms of cost. These \n",
      "are valid scenarios for applications that have been built to be deployed on the cloud.\n",
      "There can be multiple strategies for handling throttling in an application. The Throttling \n",
      "strategy can reject new requests once the threshold is crossed, or it can let the user \n",
      "know that the request is in the queue and it will get the opportunity to be executed \n",
      "once the number of requests is reduced.\n",
      "Figure 3.16 illustrates the implementation of the Throttling pattern in a multi-tenant \n",
      "system, where each tenant is allocated a fixed resource usage limit. Once they cross \n",
      "this limit, any additional demand for resources is constrained, thereby maintaining \n",
      "enough resources for other tenants:\n",
      "---------------\n",
      "Cloud design patterns | 97\n",
      "Figure 3.16: The Throttling pattern\n",
      "Read more about this pattern at https:/ /docs.microsoft.com/azure/architecture/\n",
      "patterns/throttling.\n",
      "---------------\n",
      "98 | Design pattern – Networks, storage, messaging, and events\n",
      "Retry pattern\n",
      "The Retry pattern is an extremely important pattern that makes applications and \n",
      "services more resilient to transient failures. Imagine you are trying to connect to and \n",
      "use a service, and the service is not available for some reason. If the service is going to \n",
      "become available soon, it makes sense to keep trying to get a successful connection. \n",
      "This will make the application more robust, fault tolerant, and stable. In Azure, most of \n",
      "the components are running on the internet, and that internet connection can produce \n",
      "transient faults intermittently. Since these faults can be rectified within seconds, an \n",
      "application should not be allowed to crash. The application should be designed in a \n",
      "manner that means it can try to use the service again repeatedly in the case of failure \n",
      "and stop retrying when either it is successful or it eventually determines that there is a \n",
      "fault that will take time to rectify.\n",
      "---------------\n",
      "This pattern should be implemented when an application could experience transient \n",
      "faults as it interacts with a remote service or accesses a remote resource. These faults \n",
      "are expected to be short-lived, and repeating a request that has previously failed could \n",
      "succeed on a subsequent attempt.\n",
      "The Retry pattern can adopt different retry strategies depending on the nature of the \n",
      "errors and the application:\n",
      "• Retry a fixed number of times: This denotes that the application will try to \n",
      "communicate with the service a fixed number of times before determining that \n",
      "there's been a failure and raising an exception. For example, it will retry three \n",
      "times to connect to another service. If it is successful in connecting within these \n",
      "three tries, the entire operation will be successful; otherwise, it will raise an \n",
      "exception.\n",
      "• Retry based on schedule: This denotes that the application will try to \n",
      "communicate with the service repeatedly for a fixed number of seconds or\n",
      "---------------\n",
      "minutes and wait for a fixed number of seconds or minutes before retrying. For \n",
      "example, the application will try to connect to the service every three seconds for \n",
      "60 seconds. If it is successful in connecting within this time, the entire operation \n",
      "will be successful. Otherwise, it will raise an exception.\n",
      "• Sliding and delaying the retry: This denotes that the application will try to \n",
      "communicate with the service repeatedly based on the schedule and keep adding \n",
      "an incremental delay in subsequent tries. For example, for a total of 60 seconds, \n",
      "the first retry happens after a second, the second retry happens two seconds after \n",
      "the previous retry, the third retry happens four seconds after the previous retry, \n",
      "and so on. This reduces the overall number of retries.\n",
      "---------------\n",
      "Cloud design patterns | 99\n",
      "Figure 3.17 illustrates the Retry pattern. The first request gets an HTTP 500 response, \n",
      "the second retry again gets an HTTP 500 response, and finally the request is successful \n",
      "and gets HTTP 200 as the response:\n",
      "Figure 3.17: The Retry pattern\n",
      "Refer to this Microsoft documentation at https:/ /docs.microsoft.com/azure/\n",
      "architecture/patterns/retry to find out more about this pattern.\n",
      "The Circuit Breaker pattern\n",
      "This is an extremely useful pattern. Imagine again that you are trying to connect to and \n",
      "use a service, and the service is not available for some reason. If the service is not going \n",
      "to become available soon, there is no use continuing to retry the connection. Moreover, \n",
      "keeping other resources occupied while retrying wastes a lot of resources that could \n",
      "potentially be used elsewhere.\n",
      "The Circuit Breaker pattern helps eliminate this waste of resources. It can prevent\n",
      "---------------\n",
      "applications from repeatedly trying to connect to and use a service that is not available. \n",
      "It also helps applications to detect whether a service is up and running again, and allow \n",
      "applications to connect to it.\n",
      "To implement the Circuit Breaker pattern, all requests to the service should pass \n",
      "through a service that acts as a proxy to the original service. The purpose of this proxy \n",
      "service is to maintain a state machine and act as a gateway to the original service. There \n",
      "are three states that it maintains. There could be more states included, depending on \n",
      "the application's requirements.\n",
      "---------------\n",
      "100 | Design pattern – Networks, storage, messaging, and events\n",
      "The minimal states needed to implement this pattern are as follows:\n",
      "• Open: This denotes that the service is down and the application is shown as an \n",
      "exception immediately, instead of allowing it to retry or wait for a timeout. When \n",
      "the service is up again, the state is transitioned to Half-Open.\n",
      "• Closed: This state denotes that the service is healthy and the application can go \n",
      "ahead and connect to it. Generally, a counter shows the number of failures before \n",
      "it can transition to the Open state.\n",
      "• Half-Open: At some point, when the service is up and running, this state allows \n",
      "a limited number of requests to pass through it. This state is a litmus test that \n",
      "checks whether the requests that pass through are successful. If the requests are \n",
      "successful, the state is transitioned from Half-Open to Closed. This state can also \n",
      "implement a counter to allow a certain number of requests to be successful before\n",
      "---------------\n",
      "it can transition to Closed.\n",
      "The three states and their transitions are illustrated in Figure 3.18:\n",
      "Figure 3.18: The Circuit Breaker pattern\n",
      "---------------\n",
      "Summary | 101\n",
      "Read more this pattern in the Microsoft documentation at https:/ /docs.microsoft.com/\n",
      "azure/architecture/patterns/circuit-breaker. \n",
      "In this section, we discussed design patterns that can be used to architect reliable, \n",
      "scalable, and secure applications in the cloud. There are other patterns, though, which \n",
      "you can explore at https:/ /docs.microsoft.com/azure/architecture/patterns. \n",
      "Summary\n",
      "There are numerous services available on Azure, and most of them can be combined \n",
      "to create real solutions. This chapter explained the three most important services \n",
      "provided by Azure—regions, storage, and networks. They form the backbone of the \n",
      "majority of solutions deployed on any cloud. This chapter provided details about these \n",
      "services and how their configuration and provisioning can affect design decisions. \n",
      "Important considerations for both storage and networks were detailed in this chapter. \n",
      "Both networks and storage provide lots of choices, and it is important to choose an\n",
      "---------------\n",
      "appropriate configuration based on your requirements. \n",
      "Finally, some of the important design patterns related to messaging, such as Competing \n",
      "Consumers, Priority Queue, and Load Leveling, were described. Patterns such as CQRS \n",
      "and Throttling were illustrated, and other patterns, such as Retry and Circuit Breaker, \n",
      "were also discussed. We will keep these patterns as the baseline when we deploy \n",
      "our solutions. \n",
      "In the next chapter, we will be discussing how to automate the solutions we are going \n",
      "to architect. As we move ahead in the world of automation, every organization wants \n",
      "to eliminate the overhead of creating resources one by one, which is very demanding. \n",
      "Since automation is the solution for this, in the next chapter you will learn more \n",
      "about it.\n",
      "---------------\n",
      "Every organization wants to reduce manual effort and error in their pursuits, and \n",
      "automation plays an important role in bringing about predictability, standardization, \n",
      "and consistency in both building a product and in operations. Automation has been the \n",
      "focus of almost every Chief information officer (CIO) and digital officer to ensure that \n",
      "their systems are highly available, scalable, reliable, and able to cater to the needs of \n",
      "their customers.\n",
      "Automation became more prominent with the advent of the cloud because new \n",
      "resources can be provisioned on the fly without the procurement of hardware \n",
      "resources. Hence, cloud companies want automation in almost all of their activities to \n",
      "reduce misuse, errors, governance, maintenance, and administration.\n",
      "Automating \n",
      "architecture on Azure\n",
      "4\n",
      "---------------\n",
      "104 | Automating architecture on Azure\n",
      "In this chapter, we will evaluate Azure Automation as a major service that provides \n",
      "automation capabilities, along with its differentiating capabilities compared to other \n",
      "apparently similar-looking services. This chapter will cover the following:\n",
      "• The Azure Automation landscape\n",
      "• The Azure Automation service\n",
      "• Resources for Azure Automation services\n",
      "• Writing Azure Automation runbooks\n",
      "• Webhooks\n",
      "• Hybrid Workers\n",
      "Let's get started with Azure Automation, a cloud service for process automation.\n",
      "Automation\n",
      "Automation is needed for the provisioning, operations, management, and \n",
      "deprovisioning of IT resources within an organization. Figure 4.1 gives you a closer look \n",
      "at what each of these use cases represents:\n",
      "Figure 4.1: Use cases of automation\n",
      "Before the advent of the cloud, IT resources were primarily on-premises, and manual \n",
      "processes were often used for these activities. However, since cloud adoption has\n",
      "---------------\n",
      "increased, automation has found increased focus and attention. The primary reason \n",
      "is that cloud technology's agility and flexibility provide an opportunity to provision, \n",
      "deprovision, and manage these resources on the fly in a tiny fraction of the time it \n",
      "used to take. Along with this flexibility and agility come the requirements to be more \n",
      "predictable and consistent with the cloud because it has become easy for organizations \n",
      "to create resources.\n",
      "Provisioning\n",
      "• Data centers\n",
      "• PaaS services\n",
      "• Application \n",
      "deployments\n",
      "• Environment \n",
      "and application \n",
      "Operations\n",
      "• Backups and \n",
      "restores\n",
      "• Monitoring\n",
      "• Performance\n",
      "• Disaster \n",
      "recovery\n",
      "• Availability\n",
      "• Scalability\n",
      "• Stop-start-\n",
      "pause-resume\n",
      "Management\n",
      "• Access \n",
      "management\n",
      "• Resource \n",
      "locking\n",
      "• Policies\n",
      "• Tagging\n",
      "• Cost\n",
      "• releases\n",
      "Deprovisioning\n",
      "• Delete and \n",
      "tear down \n",
      "resources\n",
      "• Soft delete \n",
      "of application \n",
      "• Transfer\n",
      "---------------\n",
      "Azure Automation | 105\n",
      "Microsoft has a great tool for IT automation known as System Center Orchestrator. It is \n",
      "a great tool for automation for on-premises and cloud environments, but it is a product \n",
      "and not a service. It should be licensed and deployed on servers, and then runbooks can \n",
      "be executed to effect changes on cloud and on-premises environments.\n",
      "Microsoft realized that an automation solution was required that could be provided \n",
      "to customers as a service rather than bought and deployed as a product. Enter Azure \n",
      "Automation.\n",
      "Azure Automation\n",
      "Azure provides a service called Azure Automation, which is an essential service for the \n",
      "automation of processes, activities, and tasks not only on Azure but also on-premises \n",
      "as well. Using Azure Automation, organizations can automate their processes and tasks \n",
      "related to processing, tear-down, operations, and the management of their resources \n",
      "across the cloud, IT environments, platforms, and languages. In Figure 4.2, we can see\n",
      "---------------\n",
      "some features of Azure Automation:\n",
      "Figure 4.2: Features of Azure Automation\n",
      "Azure Automation architecture\n",
      "Azure Automation comprises multiple components, and each of these components is \n",
      "completely decoupled from the others. Most of the integration happens at the data \n",
      "store level, and no components talk to each other directly.\n",
      "When an Automation account is created on Azure, it is managed by a management \n",
      "service. The management service is a single point of contact for all activities within \n",
      "Azure Automation. All requests from the portal, including saving, publishing, and \n",
      "creating runbooks, to execution, stopping, suspending, starting, and testing are sent \n",
      "to the automation management service and the service writes the request data to its \n",
      "data store. It also creates a job record in the data store and, based on the status of the \n",
      "runbook workers, assigns it to a worker.\n",
      "Cross-cloud Cross-environment Cross-platform Cross-language\n",
      "• Azure\n",
      "• Other clouds\n",
      "• Any combination\n",
      "• Cloud\n",
      "---------------\n",
      "• On-premises\n",
      "• Hybrid\n",
      "• Linux\n",
      "• Windows\n",
      "• PowerShell\n",
      "• Python\n",
      "• Bash\n",
      "---------------\n",
      "106 | Automating architecture on Azure\n",
      "Figure 4.3: Azure Automation architecture\n",
      "The worker keeps polling the database for any new jobs assigned to it. Once it finds \n",
      "a job assignment, it fetches the job information and starts executing the job using \n",
      "its execution engine. The results are written back to the database, read by the \n",
      "management service, and displayed back on the Azure portal.\n",
      "The Hybrid Workers that we will read about later in this chapter are also runbook \n",
      "workers, although they're not shown in Figure 4.3.\n",
      "The first step in getting started with Azure Automation is to create a new account. \n",
      "Once the account is created, all other artifacts are created within the account.\n",
      "The account acts as the main top-level resource that can be managed using Azure \n",
      "resource groups and its own control plane.\n",
      "The account should be created within a region, and all automation within this account \n",
      "gets executed on servers in that region.\n",
      "---------------\n",
      "It is important to choose the region wisely, preferably close to other Azure resources \n",
      "that the Automation account integrates or manages, to reduce the network traffic and \n",
      "latency between the regions.\n",
      "The Automation account also supports a couple of Run As accounts, which can be \n",
      "created from the Automation account. As these Run As accounts are analogous to a \n",
      "service account, we mostly create them to execute actions. Even though we generally \n",
      "say Run As account, there are two types of Run As account: one is called the Azure \n",
      "Classic Run As account, and the other one is simply the Run As account, and both of \n",
      "them are used to connect to Azure subscriptions. The Azure Classic Run As account \n",
      "is for connecting to Azure using the Azure Service Management API, and the Run As \n",
      "account is for connecting to Azure using the Azure Resource Management (ARM) API.\n",
      "Both of these accounts use certificates to authenticate with Azure. These accounts can\n",
      "---------------\n",
      "be created while creating the Automation account, or you can opt to create them at a \n",
      "later stage from the Azure portal.\n",
      "---------------\n",
      "Azure Automation architecture | 107\n",
      "It is recommended to create these Run As accounts later instead of creating them \n",
      "while creating the Automation account because if they are created while setting \n",
      "up the Automation account, Automation will generate the certificates and service \n",
      "principals behind the scenes with the default configuration. If more control and custom \n",
      "configuration is needed for these Run As accounts, such as using an existing certificate \n",
      "or service principal, then the Run As accounts should be created after the Automation \n",
      "account.\n",
      "Once the Automation account is created, it provides a dashboard through which \n",
      "multiple automation scenarios can be enabled.\n",
      "Some of the important scenarios that can be enabled using an Automation account are \n",
      "related to:\n",
      "• Process automation\n",
      "• Configuration management\n",
      "• Update management\n",
      "Automation is about writing scripts that are reusable and generic so that they can be\n",
      "---------------\n",
      "reused in multiple scenarios. For example, an automation script should be generic \n",
      "enough to start and stop any VM in any resource group in any subscription and \n",
      "management group. Hardcoding VM server information, along with resource group, \n",
      "subscription, and management group names, will result in the creation of multiple \n",
      "similar scripts, and any change in one will undoubtedly result in changing all the scripts. \n",
      "It is better to create a single script for this purpose by using scripting parameters and \n",
      "variables, and you should ensure that the values are supplied by the executor for these \n",
      "artifacts.\n",
      "Let's take a closer look at each of the aforementioned scenarios.\n",
      "Process automation\n",
      "Process automation refers to the development of scripts that reflect real-world \n",
      "processes. Process automation comprises multiple activities, where each activity \n",
      "performs a discrete task. Together, these activities form a complete process. The\n",
      "---------------\n",
      "activities might be executed on the basis of whether the previous activity executed \n",
      "successfully or not.\n",
      "---------------\n",
      "108 | Automating architecture on Azure\n",
      "There are some requirements that any process automation requires from the \n",
      "infrastructure it is executed on. Some of them are as follows:\n",
      "• The ability to create workflows\n",
      "• The ability to execute for a long duration\n",
      "• The ability to save the execution state when the workflow is not complete, which \n",
      "is also known as checkpointing and hydration\n",
      "• The ability to resume from the last saved state instead of starting from the \n",
      "beginning\n",
      "The next scenario we are going to explore is configuration management.\n",
      "Configuration management\n",
      "Configuration management refers to the process of managing the system configuration \n",
      "throughout its life cycle. Azure Automation State Configuration is the Azure \n",
      "configuration management service that allows users to write, manage, and compile \n",
      "PowerShell DSC configuration for cloud nodes and on-premises datacenters.\n",
      "Azure Automation State Configuration lets us manage Azure VMs, Azure Classic VMs,\n",
      "---------------\n",
      "and physical machines or VMs (Windows/Linux) on-premises, and it also provides \n",
      "support for VMs in other cloud providers.\n",
      "One of the biggest advantages of Azure Automation State Configuration is it provides \n",
      "scalability. We can manage thousands of machines from a single central management \n",
      "interface. We can assign configurations to machines with ease and verify whether they \n",
      "are compliant with the desired configuration.\n",
      "Another advantage is that Azure Automation can be used as a repository to store your \n",
      "Desired State Configuration (DSC) configurations, and at the time of need they can be \n",
      "used.\n",
      "In the next section, we will be talking about update management.\n",
      "---------------\n",
      "Concepts related to Azure Automation | 109\n",
      "Update management\n",
      "As you already know, update management is the responsibility of the customer to \n",
      "manage updates and patches when it comes to IaaS. The Update Management feature \n",
      "of Azure Automation can be used to automate or manage updates and patches for your \n",
      "Azure VMs. There are multiple methods by which you can enable Update Management \n",
      "on your Azure VM:\n",
      "• From your Automation account\n",
      "• By browsing the Azure portal\n",
      "• From a runbook\n",
      "• From an Azure VM\n",
      "Enabling it from an Azure VM is the easiest method. However, if you have a large \n",
      "number of VMs and need to enable Update Management, then you have to consider a \n",
      "scalable solution such as a runbook or from an Automation account.\n",
      "Now that you are clear about the scenarios, let's explore the concepts related to Azure \n",
      "Automation.\n",
      "Concepts related to Azure Automation\n",
      "You now know that Azure Automation requires an account, which is called an Azure\n",
      "---------------\n",
      "Automation account. Before we dive deeper, let's examine the concepts related to Azure \n",
      "Automation. Understanding the meaning of each of these terms is very important, as we \n",
      "are going to use these terms throughout this chapter. Let's start with runbook.\n",
      "Runbook \n",
      "An Azure Automation runbook is a collection of scripting statements representing a \n",
      "single step in process automation or a complete process automation. It is possible to \n",
      "invoke other runbooks from a parent runbook, and these runbooks can be authored in \n",
      "multiple scripting languages. The languages that support authoring runbooks are as \n",
      "follows:\n",
      "• PowerShell\n",
      "• Python 2 (at the time of writing)\n",
      "• PowerShell workflows\n",
      "• Graphical PowerShell\n",
      "• Graphical PowerShell workflows\n",
      "---------------\n",
      "110 | Automating architecture on Azure\n",
      "Creating an Automation account is very easy and can be done from the Azure portal. In \n",
      "the All Services blade, you can find Automation Account, or you can search for it in the \n",
      "Azure portal. As mentioned before, during creation you will get an option to create a \n",
      "Run As account. Figure 4.4 shows the inputs required to create an Automation account:\n",
      "Figure 4.4: Creating an Automation account\n",
      "Run As accounts\n",
      "Azure Automation accounts, by default, do not have access to any resources included in \n",
      "any Azure subscription, including the subscription in which they are hosted. An account \n",
      "needs access to an Azure subscription and its resources in order to manage them. A \n",
      "Run As account is one way to provide access to subscriptions and the resources within \n",
      "them.\n",
      "This is an optional exercise. There can be at most one Run As account for each classic \n",
      "and resource manager-based subscription; however, an Automation account might\n",
      "---------------\n",
      "need to connect to numerous subscriptions. In such cases, it is advisable to create \n",
      "shared resources for each of the subscriptions and use them in runbooks.\n",
      "---------------\n",
      "Concepts related to Azure Automation | 111\n",
      "After creating the Automation account, navigate to the Run as accounts view on the \n",
      "portal and you will see that two types of accounts can be created. In Figure 4.5, you can \n",
      "see that the option to create an Azure Run As Account and an Azure Classic Run As \n",
      "Account is available in the Run as accounts blade:\n",
      "  \n",
      "Figure 4.5: Azure Run As Account options\n",
      "These Run As accounts can be created using the Azure portal, PowerShell, and the CLI. \n",
      "For information about creating these accounts using PowerShell,  visit https:/ /docs.\n",
      "microsoft.com/azure/automation/manage-runas-account.\n",
      "In the case of the ARM Run As account, this script creates a new Azure AD service \n",
      "principal and a new certificate and provides contributor RBAC permissions to the newly \n",
      "created service principal on the subscription.\n",
      "Jobs \n",
      "The submission of a job request is not linked directly to the execution of the job request\n",
      "---------------\n",
      "because of Azure Automation's decoupled architecture. The linkage between them \n",
      "is indirect using a data store. When a request to execute a runbook is received by \n",
      "Automation, it creates a new record in its database with all the relevant information. \n",
      "There is another service running on multiple servers, known as Hybrid Runbook \n",
      "Worker, within Azure, which looks for any new entries added to the database for the \n",
      "execution of a runbook. Once it sees a new record, it locks the record so that no other \n",
      "service can read it and then executes the runbook.\n",
      "---------------\n",
      "112 | Automating architecture on Azure\n",
      "Assets\n",
      "Azure Automation assets refer to shared artifacts that can be used across runbooks. \n",
      "They are shown in Figure 4.6:\n",
      "Figure 4.6: Shared artifacts in Azure Automation\n",
      "Credentials\n",
      "Credentials refers to the secrets, such as the username/password combination, \n",
      "that can be used to connect to other integration services that need authentication. \n",
      "These credentials can be used within runbooks using the Get-AutomationPSCredential \n",
      "PowerShell cmdlet along with its associated name:\n",
      "$myCredential = Get-AutomationPSCredential -Name 'MyCredential'\n",
      "The Python syntax requires that we import the automationassets module and use the \n",
      "get_automation_credential function along with the associated credential name:\n",
      "import automationassets\n",
      "cred = automationassets.get_automation_credential(\"credtest\")\n",
      "---------------\n",
      "Concepts related to Azure Automation | 113\n",
      "Certificates\n",
      "Certificates refers to the X.509 certificate that can be purchased from certificate \n",
      "authorities or can be self-signed. Certificates are used for identification purposes \n",
      "in Azure Automation. Every certificate has a pair of keys known as private/public \n",
      "keys. The private key is used for creating a certificate asset in Azure Automation, \n",
      "and the public key should be available in the target service. Using the private key, the \n",
      "Automation account can create a digital signature and append it to the request before \n",
      "sending it to the target service. The target service can fetch the details (the hash) from \n",
      "the digital signature using the already available public key and ascertain the identity of \n",
      "the sender of the request.\n",
      "Certificate assets store certificate information and keys in Azure Automation. These \n",
      "certificates can be used directly within runbooks, and they are also used by the\n",
      "---------------\n",
      "connection's assets. The next section shows the way to consume certificates in a \n",
      "connection asset. The Azure service principal connection asset uses a certificate \n",
      "thumbprint to identify the certificate it wants to use, while other types of connection \n",
      "use the name of the certificate asset to access the certificate.\n",
      "A certificate asset can be created by providing a name and uploading a certificate. It \n",
      "is possible to upload public certificates (.cer files) as well as private certificates (.pfx \n",
      "files). The private part of the certificate also has a password that should be used before \n",
      "accessing the certificate.\n",
      " \n",
      "Figure 4.7: Adding a certificate to Azure Automation\n",
      "---------------\n",
      "114 | Automating architecture on Azure\n",
      "Creating a certificate involves providing a name and a description, uploading the \n",
      "certificate, providing a password (in the case of .pfx files), and informing the user \n",
      "whether the certificate is exportable or not.\n",
      "There should be a certificate available before this certificate asset can be created. \n",
      "Certificates can be purchased from certificate authorities or can be generated. \n",
      "Generated certificates are known as self-signed certificates. It is always a good practice \n",
      "to use certificates from certificate authorities for important environments such as \n",
      "production environments. It is fine to use self-signing certificates for development \n",
      "purposes.\n",
      "To generate a self-signed certificate using PowerShell, use this command:\n",
      "$cert = New-SelfSignedCertificate -CertStoreLocation \"Cert:\\CurrentUser\\my\" \n",
      "-KeySpec KeyExchange -Subject \"cn=azureforarchitects\"\n",
      "This will create a new certificate in the current user certificate store in your personal\n",
      "---------------\n",
      "folder. Since this certificate also needs to be uploaded to the Azure Automation \n",
      "certificate asset, it should be exported to the local file system, as shown in Figure 4.8:\n",
      "Figure 4.8: Exporting the certificate\n",
      "When exporting the certificate, the private key should also be exported, so Yes, export \n",
      "the private key should be selected.\n",
      "Select the Personal Information Exchange option, and the rest of the values should \n",
      "remain as the defaults.\n",
      "---------------\n",
      "Concepts related to Azure Automation | 115\n",
      "Provide a password and the filename C:\\azureforarchitects.pfx, and the export should \n",
      "be successful.\n",
      "Connecting to Azure can be done in multiple ways. However, the most secure is by \n",
      "way of a certificate. A service principal is created on Azure using the certificate. The \n",
      "service principal can be authenticated against using the certificate. The private key of \n",
      "the certificate is with the user and the public part is with Azure. In the next section, a \n",
      "service principal will be created using the certificate created in this section.\n",
      "Creating a service principal using certificate credentials\n",
      "A service principal can be created using the Azure portal, Azure CLI, or Azure \n",
      "PowerShell. The script for creating a service principal using Azure PowerShell is \n",
      "available in this section.\n",
      "After logging into Azure, the certificate created in the previous section is converted\n",
      "---------------\n",
      "into base64 encoding. A new service principal, azureforarchitects, is created, and the \n",
      "certificate credential is associated with the newly created service principal. Finally, the \n",
      "new service principal is provided contributor role-based access control permissions on \n",
      "the subscription:\n",
      "Login-AzAccount\n",
      "$certKey = [system.Convert]::ToBase64String($cert.GetRawCertData())\n",
      "$sp =  New-AzADServicePrincipal -DisplayName \"azureforarchitects\"\n",
      "New-AzADSpCredential -ObjectId $sp.Id -CertValue $certKey -StartDate \n",
      "$cert.NotBefore -EndDate $cert.NotAfter\n",
      "New-AzRoleAssignment -RoleDefinitionName contributor -ServicePrincipalName \n",
      "$sp.ApplicationId \n",
      "Get-AzADServicePrincipal -ObjectId $sp.Id\n",
      "$cert.Thumbprint\n",
      "Get-AzSubscription\n",
      "---------------\n",
      "116 | Automating architecture on Azure\n",
      "To create a connection asset, the application ID can be obtained using the \n",
      "Get-AzADServicePrincipal cmdlet, and the result is shown in Figure 4.9:\n",
      "Figure 4.9: Checking the service principal\n",
      "The certificate thumbprint can be obtained using the certificate reference along with \n",
      "SubscriptionId, which can be obtained using the Get-AzSubscription cmdlet.\n",
      "Connections\n",
      "Connection assets are used for creating connection information to external services. In \n",
      "this regard, even Azure is considered as an external service. Connection assets hold all \n",
      "the necessary information needed for successfully connecting to a service. There are \n",
      "three connection types provided out of the box by Azure Automation:\n",
      "• Azure  \n",
      "• Azure classic certificate\n",
      "• Azure service principal\n",
      "It is a good practice to use Azure service principal to connect to Azure Resource \n",
      "Manager resources and to use the Azure classic certificate for Azure classic resources.\n",
      "---------------\n",
      "It is important to note that Azure Automation does not provide any connection type to \n",
      "connect to Azure using credentials such as a username and password.\n",
      "Azure and Azure classic certificates are similar in nature. They both help us connect to \n",
      "Azure Service management API-based resources. In fact, Azure Automation creates an \n",
      "Azure classic certificate connection while creating a Classic Run As account.\n",
      "Azure service principal is used internally by Run As accounts to connect to Azure \n",
      "Resource Manager-based resources.\n",
      "---------------\n",
      "Concepts related to Azure Automation | 117\n",
      "A new connection asset of type AzureServicePrincipal is shown in Figure 4.10. It needs:\n",
      "• The name of the connection. It is mandatory to provide a name.\n",
      "• A description of the connection. This value is optional.\n",
      "• Select an appropriate Type. It is mandatory to select an option; \n",
      "AzureServicePrincipal is selected for creating a connection asset for all purposes \n",
      "in this chapter.\n",
      "• ApplicationId, also known as clientid, is the application ID generated during \n",
      "the creation of a service principal. The next section shows the process of \n",
      "creating a service principal using Azure PowerShell. It is mandatory to provide an \n",
      "application ID.\n",
      "• TenantId is the unique identifier of the tenant. This information is available from \n",
      "the Azure portal or by using the Get-AzSubscription cmdlet. It is mandatory to \n",
      "provide a tenant identifier.\n",
      "• CertificateThumbprint is the certificate identifier. This certificate should already\n",
      "---------------\n",
      "be uploaded to Azure Automation using the certificate asset. It is mandatory to \n",
      "provide a certificate thumbprint.\n",
      "• SubscriptionId is the identifier of the subscription. It is mandatory to provide a \n",
      "subscription ID.\n",
      "You can add a new connection using the Connections blade in the Automation account, \n",
      "as shown in Figure 4.10:\n",
      "Figure 4.10: Adding a new connection to the Automation Account\n",
      "---------------\n",
      "118 | Automating architecture on Azure\n",
      "Runbook authoring and execution\n",
      "Azure Automation allows the creation of automation scripts known as runbooks. \n",
      "Multiple runbooks can be created using the Azure portal or PowerShell ISE. They \n",
      "can also be imported from Runbook Gallery. The gallery can be searched for specific \n",
      "functionality, and the entire code is displayed within the runbook.\n",
      "A runbook can accept parameter values just like a normal PowerShell script. The next \n",
      "example takes a single parameter named connectionName of type string. It is mandatory \n",
      "to supply a value for this parameter when executing this runbook:\n",
      "param(\n",
      "    [parameter(mandatory=$true)]\n",
      "    [string] $connectionName\n",
      ")\n",
      "$connection = Get-AutomationConnection  -name $connectionName  \n",
      "$subscriptionid = $connection.subscriptionid\n",
      "$tenantid = $connection.tenantid\n",
      "$applicationid = $connection.applicationid\n",
      "$cretThumbprint = $connection.CertificateThumbprint\n",
      "Login-AzureRMAccount -CertificateThumbprint $cretThumbprint\n",
      "---------------\n",
      "-ApplicationId $applicationid -ServicePrincipal -Tenant $tenantid  \n",
      "Get-AzureRMVM\n",
      "The runbook uses the Get-AutomationConnection cmdlet to reference the shared \n",
      "connection asset. The name of the asset is contained within the parameter value. Once \n",
      "the reference to the connection asset has been made, the values from the connection \n",
      "reference are populated into the $connection variable, and subsequently, they are \n",
      "assigned to multiple other variables.\n",
      "The Login-AzureRMAccount cmdlet authenticates with Azure, and it supplies the values \n",
      "obtained from the connection object. It uses the service principal created earlier in this \n",
      "chapter for authentication.\n",
      "Finally, the runbook invokes the Get-AzureRMVm cmdlet to list all the VMs in the \n",
      "subscription.\n",
      "---------------\n",
      "Runbook authoring and execution | 119\n",
      "By default, Azure Automation still provides AzureRM modules for working with Azure. It \n",
      "does not install Az modules by default. Later we will install an Az module manually in the \n",
      "Azure Automation account and use cmdlets in runbooks.\n",
      "Parent and child runbooks\n",
      "Runbooks have a life cycle, from being authored to being executed. These life cycles can \n",
      "be divided into authoring status and execution status.\n",
      "The authoring life cycle is shown in Figure 4.11.\n",
      "When a new runbook is created, it has the New status and as it is edited and saved \n",
      "multiple times, it takes the In edit status, and finally, when it is published, the status \n",
      "changes to Published. It is also possible to edit a published runbook, and in that case, it \n",
      "goes back to the In edit status.\n",
      "Figure 4.11: Authoring life cycle\n",
      "The execution life cycle is described next.\n",
      "The life cycle starts with the beginning of a runbook execution request. A runbook can \n",
      "be executed in multiple ways:\n",
      "---------------\n",
      "• Manually from the Azure portal\n",
      "• By using a parent runbook as a child runbook\n",
      "• By means of a webhook\n",
      "It does not matter how a runbook is initiated; the life cycle remains the same. A request \n",
      "to execute the runbook is received by the Automation engine. The Automation engine \n",
      "creates a job and assigns it to a runbook worker. Currently, the runbook has a status of \n",
      "Queued.\n",
      "There are multiple runbook workers, and the chosen one picks up the job request and \n",
      "changes the status to Starting. At this stage, if there are any scripting and parsing \n",
      "issues in the script, the status changes to Failed and the execution is halted.\n",
      "Once the runbook execution is started by the worker, the status is changed to Running. \n",
      "The runbook can have multiple different statuses once it is running.\n",
      "The runbook will change its status to Completed if the execution happens without any \n",
      "unhandled and terminating exceptions.\n",
      "---------------\n",
      "120 | Automating architecture on Azure\n",
      "The running runbook can be manually stopped by the user, and it will have the Stopped \n",
      "status.\n",
      "Figure 4.12: The execution life cycle for runbooks\n",
      "The user can also suspend and resume the execution of the runbook.\n",
      "Creating a runbook\n",
      "A runbook can be created from the Azure portal by going to the Runbook menu item \n",
      "in the left navigation pane. A runbook has a name and type. The type determines \n",
      "the scripting language used for creating the runbook. We have already discussed \n",
      "the possible languages, and in this chapter, PowerShell will be used primarily for all \n",
      "examples.\n",
      "Creating a PowerShell runbook is exactly the same as creating a PowerShell script. It \n",
      "can declare and accept multiple parameters—the parameters can have attributes such \n",
      "as data types, which are mandatory (just like any PowerShell parameter attributes). It \n",
      "can invoke PowerShell cmdlets whose modules are available and already loaded and\n",
      "---------------\n",
      "declared, and it can invoke functions and return output.\n",
      "A runbook can also invoke another runbook. It can invoke a child runbook inline within \n",
      "the original process and context or in a separate process and context.\n",
      "Invoking a runbook inline is similar to invoking a PowerShell script. The next example \n",
      "invokes a child runbook using the inline approach:\n",
      ".\\ConnectAzure.ps1 -connectionName \"azureforarchitectsconnection\"\n",
      "Get-AzSqlServer\n",
      "---------------\n",
      "Runbook authoring and execution | 121\n",
      "In the preceding code, we saw how the ConnectAzure runbook accepts a parameter \n",
      "named connectionName and an appropriate value is supplied to it. This runbook creates a \n",
      "connection to Azure after authenticating with it using a service principal. Check out the \n",
      "syntax for invoking the child runbook. It is very similar to invoking a general PowerShell \n",
      "script along with parameters.\n",
      "The next line of code, Get-AzVm, fetches the relevant information from Azure and lists \n",
      "the VM details. You will notice that although the authentication happens within a child \n",
      "runbook, the Get-AzVm cmdlet succeeds and lists all the VMs in the subscription because \n",
      "the child runbook executes in the same job as that of the parent runbook, and they \n",
      "share the context.\n",
      "Alternatively, a child runbook can be invoked using the Start-AzurermAutomationRunbook \n",
      "cmdlet provided by Azure Automation. This cmdlet accepts the name of the Automation\n",
      "---------------\n",
      "account, the resource group name, and the name of the runbook along with parameters, \n",
      "as mentioned here:\n",
      "$params = @{\"connectionName\"=\"azureforarchitectsconnection\"}\n",
      "$job = Start-AzurermAutomationRunbook '\n",
      "    –AutomationAccountName 'bookaccount' '\n",
      "    –Name 'ConnectAzure' '\n",
      "    -ResourceGroupName 'automationrg' -parameters $params\n",
      "if($job -ne $null) {\n",
      "    Start-Sleep -s 100\n",
      "    $job = Get-AzureAutomationJob -Id $job.Id -AutomationAccountName \n",
      "'bookaccount'\n",
      "    if ($job.Status -match \"Completed\") {\n",
      "        $jobout = Get-AzureAutomationJobOutput '\n",
      "                                    -Id $job.Id '\n",
      "                                    -AutomationAccountName 'bookaccount' '\n",
      "                                    -Stream Output\n",
      "                    if ($jobout) {Write-Output $jobout.Text}\n",
      "    }\n",
      "}\n",
      "Using this approach creates a new job that's different from the parent job, and they run \n",
      "in different contexts.\n",
      "---------------\n",
      "122 | Automating architecture on Azure\n",
      "Using Az modules\n",
      "So far, all examples have used AzureRM modules. The previously shown runbooks will be \n",
      "re-written to use cmdlets from the Az module.\n",
      "As mentioned before, Az modules are not installed by default. They can be installed \n",
      "using the Modules gallery menu item in Azure Automation.\n",
      "Search for Az in the gallery and the results will show multiple modules related to it. If \n",
      "the Az module is selected to be imported and installed, it will throw an error saying \n",
      "that its dependent modules are not installed and that they should be installed before \n",
      "installing the current module. The module can be found on the Modules gallery blade \n",
      "by searching for Az, as shown in Figure 4.13:\n",
      "  \n",
      "Figure 4.13: Finding the Az module on the Modules gallery blade\n",
      "Instead of selecting the Az module, select Az.Accounts and import the module by \n",
      "following the wizard, as shown in Figure 4.14:\n",
      "Figure 4.14: Importing the Az.Accounts module\n",
      "---------------\n",
      "Using Az modules | 123\n",
      "After installing Az.Accounts, the Az.Resources module can be imported. Azure virtual \n",
      "machine-related cmdlets are available in the Az.Compute module, and it can also be \n",
      "imported using the same method as we used to import Az.Accounts.\n",
      "Once these modules are imported, the runbooks can use the cmdlets provided by these \n",
      "modules. The previously shown ConnectAzure runbook has been modified to use the Az \n",
      "module:\n",
      "param(\n",
      "    [parameter(mandatory=$true)]\n",
      "    [string] $connectionName\n",
      ")\n",
      "$connection = Get-AutomationConnection  -name $connectionName  \n",
      "$subscriptionid = $connection.subscriptionid\n",
      "$tenantid = $connection.tenantid\n",
      "$applicationid = $connection.applicationid\n",
      "$cretThumbprint = $connection.CertificateThumbprint\n",
      "Login-AzAccount -CertificateThumbprint $cretThumbprint \n",
      "-ApplicationId $applicationid -ServicePrincipal \n",
      "-Tenant $tenantid  -SubscriptionId  $subscriptionid \n",
      "Get-AzVm\n",
      "The last two lines of the code are important. They are using Az cmdlets instead of\n",
      "---------------\n",
      "AzureRM cmdlets.\n",
      "---------------\n",
      "124 | Automating architecture on Azure\n",
      "Executing this runbook will give results similar to this:\n",
      "Figure 4.15: The Az.Accounts module successfully imported\n",
      "In the next section, we will work with webhooks.\n",
      "---------------\n",
      "Webhooks | 125\n",
      "Webhooks\n",
      "Webhooks became famous after the advent of REST endpoints and JSON data payloads. \n",
      "Webhooks are an important concept and architectural decision in the extensibility \n",
      "of any application. Webhooks are placeholders that are left within special areas of \n",
      "an application so that the user of the application can fill those placeholders with \n",
      "endpoint URLs containing custom logic. The application will invoke the endpoint URL, \n",
      "automatically passing in the necessary parameters, and then execute the login available \n",
      "therein.\n",
      "Azure Automation runbooks can be invoked manually from the Azure portal. They can \n",
      "also be invoked using PowerShell cmdlets and the Azure CLI. There are SDKs available \n",
      "in multiple languages that are capable of invoking runbooks.\n",
      "Webhooks are one of the most powerful ways to invoke a runbook. It is important to \n",
      "note that runbooks containing the main logic should never be exposed directly as a\n",
      "---------------\n",
      "webhook. They should be called using a parent runbook, and the parent runbook should \n",
      "be exposed as a webhook. The parent runbook should ensure that appropriate checks \n",
      "are made before invoking the main child runbook.\n",
      "The first step in creating a webhook is to author a runbook normally, as done previously. \n",
      "After a runbook has been authored, it will be exposed as a webhook.\n",
      "A new PowerShell-based runbook named exposedrunbook is created. This runbook takes \n",
      "a single parameter, $WebhookData, of the object type. It should be named verbatim. This \n",
      "object is created by the Azure Automation runtime and is supplied to the runbook. \n",
      "The Azure Automation runtime constructs this object after obtaining the HTTP \n",
      "request header values and body content and fills in the RequestHeader and RequestBody \n",
      "properties of this object:\n",
      "param(\n",
      "    [parameter(mandatory=$true)]\n",
      "    [object] $WebhookData\n",
      ")\n",
      "$webhookname = $WebhookData.WebhookName\n",
      "$headers = $WebhookData.RequestHeader\n",
      "---------------\n",
      "$body = $WebhookData.RequestBody\n",
      "Write-output \"webhook header data\"\n",
      "---------------\n",
      "126 | Automating architecture on Azure\n",
      "Write-Output $webhookname\n",
      "Write-output $headers.message\n",
      "Write-output $headers.subject\n",
      " $connectionname = (ConvertFrom-Json -InputObject $body)\n",
      "./connectAzure.ps1 -connectionName  $connectionname[0].name\n",
      "The three important properties of this object are WebhookName, RequestHeader, and \n",
      "RequestBody. The values are retrieved from these properties and sent to the output \n",
      "stream by the runbook.\n",
      "The header and body content can be anything that the user supplies when invoking the \n",
      "webhook. These values get filled up into the respective properties and become available \n",
      "within the runbook. In the previous example, there are two headers set by the caller, \n",
      "namely message and status header. The caller will also supply the name of the shared \n",
      "connection to be used as part of the body content.\n",
      "After the runbook is created, it should be published before a webhook can be created. \n",
      "After publishing the runbook, clicking on the Webhook menu at the top starts the\n",
      "---------------\n",
      "process of creating a new webhook for the runbook, as shown in Figure 4.16:\n",
      "Figure 4.16: Creating a webhook\n",
      "A name for the webhook should be provided. This value is available within the runbook \n",
      "using the WebhookData parameter with the WebhookName property name.\n",
      "---------------\n",
      "Webhooks | 127\n",
      "The webhook can be in the enabled or disabled state, and it can expire at a given date \n",
      "and time. It also generates a URL that is unique for this webhook and runbook. This URL \n",
      "should be provided to anyone who wishes to invoke the webhook.\n",
      "Invoking a webhook\n",
      "Webhooks are invoked as HTTP requests using the POST method. When a webhook \n",
      "is invoked, the HTTP request lands up with Azure Automation to start a runbook. It \n",
      "creates the WebHookData object, filling it with the incoming HTTP header and body data, \n",
      "and creates a job to be picked up by a runbook worker. This call uses the webhook URL \n",
      "generated in the previous step.\n",
      "The webhook can be invoked using Postman, by any code having the capability of calling \n",
      "a REST endpoint using the POST method. In the next example, PowerShell will be used to \n",
      "invoke the webhook:\n",
      "$uri = \"https://s16events.azure-automation.net/\n",
      "webhooks?token=rp0w93L60fAPYZQ4vryxl%2baN%2bS1Hz4F3qVdUaKUDzgM%3d\"\n",
      "$connection  = @(\n",
      "---------------\n",
      "@{  name=\"azureforarchitectsconnection\"}\n",
      "           \n",
      "        )\n",
      "$body = ConvertTo-Json -InputObject $ connection  \n",
      "$header = @{ subject=\"VMS specific to Ritesh\";message=\"Get all virtual \n",
      "machine details\"}\n",
      "        \n",
      "$response = Invoke-WebRequest -Method Post -Uri $uri -Body $body -Headers \n",
      "$header\n",
      "$jobid = (ConvertFrom-Json ($response.Content)).jobids[0] \n",
      "The PowerShell code declares the URL for the webhook and constructs the body in \n",
      "JSON format, with name set to azureforarchitectsconnection and a header with two \n",
      "header name-value pairs – subject and message. Both the header and body data can be \n",
      "retrieved in the runbook using the WebhookData parameter.\n",
      "The invoke-webrequest cmdlet raises the request on the previously mentioned endpoint \n",
      "using the POST method, supplying both the header and the body.\n",
      "---------------\n",
      "128 | Automating architecture on Azure\n",
      "The request is asynchronous in nature, and instead of the actual runbook output, the \n",
      "job identifier is returned as an HTTP response. It is also available within the response \n",
      "content. The job is shown in Figure 4.17:\n",
      "Figure 4.17: Checking the job\n",
      "Clicking on WEBHOOKDATA shows the values that arrived in the runbook automation \n",
      "service in the HTTP request:\n",
      "Figure 4.18: Verifying the output\n",
      "Clicking on the output menu shows the list of VMs and SQL Server in the subscription.\n",
      "The next important concepts in Azure Automation are Azure Monitor and Hybrid \n",
      "Workers, and the next sections will explain them in detail.\n",
      "---------------\n",
      "Webhooks | 129\n",
      "Invoking a runbook from Azure Monitor\n",
      "Azure Automation runbooks can be invoked as responses to alerts generated within \n",
      "Azure. Azure Monitor is the central service that manages logs and metrics across \n",
      "resources and resource groups in a subscription. You can use Azure Monitor to create \n",
      "new alert rules and definitions that, when triggered, can execute Azure Automation \n",
      "runbooks. They can invoke an Azure Automation runbook in its default form or a \n",
      "webhook that in turn can execute its associated runbook. This integration between \n",
      "Azure Monitor and the ability to invoke runbooks opens numerous automation \n",
      "opportunities to autocorrect the environment, scale up and down compute resources, \n",
      "or take corrective actions without any manual intervention.\n",
      "Azure alerts can be created and configured in individual resources and resource \n",
      "levels, but it is always a good practice to centralize alert definitions for easy and better \n",
      "maintenance and administration.\n",
      "---------------\n",
      "Let's go through the process of associating a runbook with an alert and invoking the \n",
      "runbook as part of the alert being raised.\n",
      "The first step is to create a new alert, as shown in Figure 4.19:\n",
      " \n",
      "Figure 4.19: Creating an alert rule\n",
      "Select a resource that should be monitored and evaluated for alert generation. A \n",
      "resource group has been selected from the list, and it automatically enables all \n",
      "resources within the resource group. It is possible to remove the resource selections \n",
      "from the resource group:\n",
      "---------------\n",
      "130 | Automating architecture on Azure\n",
      "Figure 4.20: Selecting the scope of the alert\n",
      "Configure the condition and rules that should get evaluated. Select the Power Off \n",
      "Virtual Machine signal name after selecting Activity Log as the Signal type:\n",
      "Figure 4.21: Selecting the signal type\n",
      "---------------\n",
      "Webhooks | 131\n",
      "The resultant window will allow you to configure the Alert logic/condition. Select \n",
      "critical for Event Level, and set Status to Succeeded:\n",
      "Figure 4.22: Setting up the alert logic\n",
      "After determining the alert condition comes the most important configuration, which \n",
      "configures the response to the alert by invoking a runbook. We can use Action groups \n",
      "to configure the response to an alert. It provides numerous options to invoke an Azure \n",
      "function, webhook, or Azure Automation runbook, as well as to send emails and SMS.\n",
      "Create an action group by providing a name, a short name, its hosting subscription, \n",
      "a resource group, and an Action name. Corresponding to Action name select the \n",
      "Automation Runbook option as Action Type:\n",
      "Figure 4.23 Configuring the action group\n",
      "---------------\n",
      "132 | Automating architecture on Azure\n",
      "Selecting an automation runbook will open another blade for selecting an appropriate \n",
      "Azure Automation account and runbook. Several runbooks are available out of the box, \n",
      "and one of them has been used here:\n",
      "Figure 4.24 Creating the runbook\n",
      "---------------\n",
      "Webhooks | 133\n",
      "Finally, provide a name and hosting resource group to create a new alert.\n",
      "If the VM is deallocated manually, the alert condition gets satisfied and it will raise an \n",
      "alert:\n",
      "Figure 4.25 Testing alerts\n",
      "If you check the details of the VM after a few seconds, you should see that the VM is \n",
      "being deleted:\n",
      "Figure 4.26 Verifying the results\n",
      "---------------\n",
      "134 | Automating architecture on Azure\n",
      "Hybrid Workers\n",
      "So far, all the execution of runbooks has primarily been on infrastructure provided \n",
      "by Azure. The runbook workers are Azure compute resources that are provisioned \n",
      "by Azure with appropriate modules and assets deployed in them. Any execution of \n",
      "runbooks happens on this compute. However, it is possible for users to bring their \n",
      "own compute and execute the runbook on this user-provided compute rather than on \n",
      "default Azure compute.\n",
      "This has multiple advantages. The first and foremost is that the entire execution and \n",
      "its logs are owned by the user with Azure having no visibility of it. Second, the user-\n",
      "provided compute could be on any cloud, as well as on-premises.\n",
      "Adding a Hybrid Worker involves multiple steps\n",
      "• First and foremost, an agent needs to be installed on the user-provided \n",
      "compute. Microsoft provides a script that can download and configure the agent\n",
      "---------------\n",
      "automatically. This script is available from https:/ /www.powershellgallery.com/\n",
      "packages/New-OnPremiseHybridWorker/1.6.\n",
      "The script can also be executed from PowerShell ISE as an administrator from \n",
      "within the server that should be part of the Hybrid Worker using the following \n",
      "command:\n",
      "Install-Script -Name New-OnPremiseHybridWorker -verbose\n",
      "• After the script is installed, it can be executed along with parameters related to \n",
      "the Azure Automation account details. A name is also provided for the Hybrid \n",
      "Worker. If the name does not exist already, it will be created; if it exists, the server \n",
      "will be added to the existing Hybrid Worker. It is possible to have multiple servers \n",
      "within a single Hybrid Worker, and it is possible to have multiple Hybrid Workers \n",
      "as well:\n",
      "New-OnPremiseHybridWorker.ps1 -AutomationAccountName bookaccount \n",
      "-AAResourceGroupName automationrg '\n",
      "-HybridGroupName \"localrunbookexecutionengine\" '\n",
      "-SubscriptionID xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n",
      "---------------\n",
      "Webhooks | 135\n",
      "• Once the execution finishes, navigating back to the portal will show an entry for a \n",
      "Hybrid Worker, as shown in Figure 4.27:\n",
      "Figure 4.27: Checking user Hybrid Worker groups\n",
      "• If, at this time, an Azure runbook is executed that has a dependency on the Az \n",
      "module and a custom certificate uploaded to the certificate asset, it will fail with \n",
      "errors related to the Az module and the certificate not being found:\n",
      "Figure 4.28: Checking errors\n",
      "• Install the Az module using the following command on the server:\n",
      "Install-module -name Az -AllowClobber -verbose\n",
      "It is also important to have the .pfx certificate available on this server. The \n",
      "previously exported certificate should be copied to the server and installed \n",
      "manually.\n",
      "---------------\n",
      "136 | Automating architecture on Azure\n",
      "• After installation of the Az module and certificate, re-executing the runbook on \n",
      "the Hybrid Worker is shown in Figure 4.29, and it should show the list of VMs in \n",
      "the subscription:\n",
      "Figure 4.29: Setting up a runbook to run on a Hybrid Worker \n",
      "When we discussed different scenarios, we talked about configuration management. \n",
      "In the next section, we will be discussing configuration management with Azure \n",
      "Automation in more detail.\n",
      "Azure Automation State Configuration\n",
      "Azure Automation provides a Desired State Configuration (DSC) pull server along \n",
      "with every Azure Automation account. The pull server can hold configuration scripts \n",
      "that can be pulled by servers across clouds and on-premises. This means that Azure \n",
      "Automation can be used to configure any server hosted anywhere in the world.\n",
      "The DSC needs a local agent on these servers, also known as a local configuration\n",
      "---------------\n",
      "manager (LCM). It should be configured with the Azure Automation DSC pull server so \n",
      "it can download the required configuration and autoconfigure the server.\n",
      "The autoconfiguration can be scheduled to be periodic (by default it is half an hour), \n",
      "and if the agent finds any deviation in the server configuration compared to the one \n",
      "available in the DSC script, it will autocorrect and bring back the server to the desired \n",
      "and expected state.\n",
      "---------------\n",
      "Azure Automation State Configuration | 137\n",
      "In this section, we will configure one server hosted on Azure, and the process will \n",
      "remain the same irrespective of whether the server is on a cloud or on-premises.\n",
      "The first step is to create a DSC configuration. A sample configuration is shown here, \n",
      "and complex configurations can be authored similarly:\n",
      "configuration ensureiis {\n",
      "import-dscresource -modulename psdesiredstateconfiguration\n",
      "    \n",
      "node localhost {        \n",
      "        WindowsFeature iis {\n",
      "            Name = \"web-server\"\n",
      "            Ensure = \"Present\"\n",
      "             \n",
      "        }\n",
      "    }\n",
      "}\n",
      "The configuration is quite simple. It imports the PSDesiredStateConfiguration base DSC \n",
      "module and declares a single-node configuration. This configuration is not associated \n",
      "with any specific node and can be used to configure any server. The configuration is \n",
      "supposed to configure an IIS web server and ensure that it is present on any server to \n",
      "which it is applied.\n",
      "---------------\n",
      "This configuration is not yet available on the Azure Automation DSC pull server, and so \n",
      "the first step is to import the configuration into the pull server. This can be done using \n",
      "the Automation account Import-AzAutomationDscConfiguration cmdlet, as shown next:\n",
      "Import-AzAutomationDscConfiguration -SourcePath \"C:\\Ritesh\\ensureiis.ps1\" \n",
      "-AutomationAccountName bookaccount -ResourceGroupName automationrg -Force \n",
      "-Published\n",
      "There are a few important things to note here. The name of the configuration should \n",
      "match the filename, and it must only contain alphanumeric characters and underscores. \n",
      "A good naming convention is to use verb/noun combinations. The cmdlets need the \n",
      "path of the configuration file and the Azure Automation account details to import the \n",
      "configuration script.\n",
      "---------------\n",
      "138 | Automating architecture on Azure\n",
      "At this stage, the configuration is visible on the portal:\n",
      "Figure 4.30: Adding configuration\n",
      "Once the configuration script is imported, it is compiled and stored within the DSC pull \n",
      "server using the Start-AzAutomationDscCompilationJob cmdlet, as shown next:\n",
      "Start-AzAutomationDscCompilationJob -ConfigurationName 'ensureiis' \n",
      "-ResourceGroupName 'automationrg' -AutomationAccountName 'bookaccount'\n",
      "The name of the configuration should match the one that was recently uploaded, and \n",
      "the compiled configuration should be available now on the Compiled configurations \n",
      "tab, as shown in Figure 4.31:\n",
      "Figure 4.31: Listing compiled configurations\n",
      "---------------\n",
      "Azure Automation State Configuration | 139\n",
      "It is important to note that the Node Count in Figure 4.31 is 0. It means that a node \n",
      "configuration called ensureiss.localhost exists but it is not assigned to any node. The \n",
      "next step is to assign the configuration to the node.\n",
      "By now, we have a compiled DSC configuration available on the DSC pull server, but \n",
      "there are no nodes to manage. The next step is to onboard the VMs and associate them \n",
      "with the DSC pull server. This is done using the Register-AzAutomationDscNode cmdlet:\n",
      "Register-AzAutomationDscNode -ResourceGroupName 'automationrg' \n",
      "-AutomationAccountName 'bookaccount' -AzureVMLocation \"west \n",
      "Europe\" -AzureVMResourceGroup 'spark' -AzureVMName 'spark' \n",
      "-ConfigurationModeFrequencyMins 30 -ConfigurationMode 'ApplyAndAutoCorrect' \n",
      "This cmdlet takes the name of the resource group for both the VM and the \n",
      "Azure Automation account. It also configures the configuration mode and the\n",
      "---------------\n",
      "configurationModeFrequencyMins property of the local configuration manager of the \n",
      "VM. This configuration will check and autocorrect any deviation from the configuration \n",
      "applied to it every 30 minutes.\n",
      "If VMresourcegroup is not specified, the cmdlet tries to find the VM in the same resource \n",
      "group as the Azure Automation account, and if the VM location value is not provided, \n",
      "it tries to find the VM in the Azure Automation region. It is always better to provide \n",
      "values for them. Notice that this command can only be used for Azure VMs as it asks \n",
      "for AzureVMname explicitly. For servers on other clouds and on-premises, use the \n",
      "Get-AzAutomationDscOnboardingMetaconfig cmdlet.\n",
      "Now, a new node configuration entry can also be found in the portal, as shown in \n",
      "Figure 4.32:\n",
      "Figure 4.32: Verifying node status\n",
      "---------------\n",
      "140 | Automating architecture on Azure\n",
      "The node information can be obtained as follows:\n",
      "$node = Get-AzAutomationDscNode -ResourceGroupName 'automationrg' \n",
      "-AutomationAccountName 'bookaccount' -Name 'spark' \n",
      "And a configuration can be assigned to the node:\n",
      "Set-AzAutomationDscNode -ResourceGroupName 'automationrg' \n",
      "-AutomationAccountName 'bookaccount' -NodeConfigurationName 'ensureiis.\n",
      "localhost' -NodeId $node.Id \n",
      "Once the compilation is complete, it can be assigned to the nodes. The initial status is \n",
      "Pending, as shown in Figure 4.33:\n",
      "Figure 4.33: Verifying node status\n",
      "After a few minutes, the configuration is applied to the node, the node becomes \n",
      "Compliant, and the status becomes Completed:\n",
      "Figure 4.34: Verifying if the node is compliant\n",
      "---------------\n",
      "Azure Automation pricing | 141\n",
      "Later, logging into the server and checking if the web server (IIS) is installed confirms \n",
      "that it is installed, as you can see in Figure 4.35:\n",
      "Figure 4.35: Checking whether the desired state has been achieved\n",
      "In the next section, Azure Automation pricing will be discussed.\n",
      "Azure Automation pricing\n",
      "There is no cost for Azure Automation if no runbooks are executed on it. The cost of \n",
      "Azure Automation is charged per minute for execution of runbook jobs. This means \n",
      "that if the total number of runbook execution minutes is 10,000, the cost of Azure \n",
      "Automation would be $0.002 per minute multiplied by 9,500, as the first 500 minutes \n",
      "are free.\n",
      "There are other costs involved in Azure Automation depending on features consumed. \n",
      "For example, a DSC pull server does not cost anything within Azure Automation; neither \n",
      "does onboarding Azure VMs on to the pull server. However, if non-Azure servers are\n",
      "---------------\n",
      "onboarded, typically from other clouds or on-premises, then the first five servers are \n",
      "free and anything on top of that costs $6 per server per month in the West US region.\n",
      "Pricing may vary from region to region, and it's always a good practice to verify the \n",
      "pricing on the official pricing page: https:/ /azure.microsoft.com/pricing/details/\n",
      "automation.\n",
      "You might ask, why do we need an Automation account when we can deploy serverless \n",
      "applications via Azure Functions? In the next section, we will explore the key \n",
      "differences between Azure Automation and serverless automation.\n",
      "Comparison with serverless automation\n",
      "Azure Automation and Azure serverless technologies, especially Azure Functions, are \n",
      "quite similar and overlap in terms of functionality. However, these are separate services \n",
      "with different capabilities and pricing.\n",
      "It is important to understand that Azure Automation is a complete suite for process\n",
      "---------------\n",
      "automation and configuration management, while Azure Functions is meant for \n",
      "implementing business functionality.\n",
      "---------------\n",
      "142 | Automating architecture on Azure\n",
      "Azure Automation is used for automating the processes of provisioning, deprovisioning, \n",
      "management, and operations of infrastructure and configuration management \n",
      "thereafter. On the other hand, Azure Functions is meant for the creation of services, \n",
      "implementing functionality that can be part of microservices and other APIs.\n",
      "Azure Automation is not meant for unlimited scale, and the load is expected to be \n",
      "moderate, while Azure Functions can handle unlimited traffic and scale automatically.\n",
      "There are a host of shared assets, such as connections, variables, and modules, that can \n",
      "be reused across runbooks in Azure Automation; however, there is no out-of-the-box \n",
      "shared concept in Azure Functions.\n",
      "Azure Automation can manage intermediate state by way of checkpointing and \n",
      "continue from the last saved state, while Azure functions are generally stateless and do \n",
      "not maintain any state.\n",
      "Summary\n",
      "---------------\n",
      "Azure Automation is an important service within Azure and the only service for process \n",
      "automation and configuration management. This chapter covered a lot of important \n",
      "concepts related to Azure Automation and process automation, including shared assets \n",
      "such as connection, certificates, and modules.\n",
      "It covered the creation of runbooks, including invoking runbooks in different ways, \n",
      "such as parent-child relationships, webhooks, and using the portal. The chapter also \n",
      "discussed the architecture and life cycle of runbooks.\n",
      "We also looked at the usage of Hybrid Workers and, toward the end of the chapter, \n",
      "explored configuration management using a DSC pull server and a local configuration \n",
      "manager. Finally, we made comparisons with other technologies, such as Azure \n",
      "Functions. \n",
      "In the next chapter, we will explore designing policies, locks, and tags for Azure \n",
      "deployments.\n",
      "---------------\n",
      "Azure is a versatile cloud platform. Customers can not only create and deploy their \n",
      "applications; they can also actively manage and govern their environments. Clouds \n",
      "generally follow a pay-as-you-go paradigm, where a customer subscribes and can then \n",
      "deploy virtually anything to the cloud. It could be as small as a basic virtual machine, \n",
      "or it could be thousands of virtual machines with higher stock-keeping units (SKUs). \n",
      "Azure will not stop any customer from provisioning the resources they want to \n",
      "provision. Within an organization, there could be a large number of people with access \n",
      "to the organization's Azure subscription. There needs to be a governance model in \n",
      "place so that only necessary resources are provisioned by people who have the right to \n",
      "create them. Azure provides resource management features, such as Azure Role-Based \n",
      "Access Control (RBAC), Azure Policy, management groups, blueprints, and resource \n",
      "locks, for managing and providing governance for resources.\n",
      "---------------\n",
      "Designing policies, \n",
      "locks, and tags for \n",
      "Azure deployments\n",
      "5\n",
      "---------------\n",
      "146 | Designing policies, locks, and tags for Azure deployments\n",
      "Other major aspects of governance include cost, usage, and information management. \n",
      "An organization's management team always wants to be kept up to date about cloud \n",
      "consumption and costs. They would like to identify what team, department, or unit is \n",
      "using what percentage of their total cost. In short, they want to have reports based on \n",
      "various dimensions of consumption and cost. Azure provides a tagging feature that can \n",
      "help provide this kind of information on the fly.\n",
      "In this chapter, we will cover the following topics:\n",
      "• Azure management groups\n",
      "• Azure tags\n",
      "• Azure Policy\n",
      "• Azure locks\n",
      "• Azure RBAC\n",
      "• Azure Blueprints\n",
      "• Implementing Azure governance features\n",
      "Azure management groups\n",
      "We are starting with Azure management groups because, in most of the upcoming \n",
      "sections, we will be referencing or mentioning management groups. Management\n",
      "---------------\n",
      "groups act as a level of scope for you to effectively assign or manage roles and policies. \n",
      "Management groups are very useful if you have multiple subscriptions.\n",
      "Management groups act as a placeholder for organizing subscriptions. You can also \n",
      "have nested management groups. If you apply a policy or access at the management \n",
      "group level, it will be inherited by the underlying management groups and \n",
      "subscriptions. From the subscription level, that policy or access will be inherited by \n",
      "resource groups and then finally by the resources.\n",
      "---------------\n",
      "Azure tags | 147\n",
      "The hierarchy of management groups is shown here:\n",
      "Figure 5.1: Hierarchy of Azure management groups\n",
      "In Figure 5.1, we are using management groups to separate the operations of different \n",
      "departments, such as marketing, IT, and HR. Inside each of these departments, \n",
      "there are nested management groups and subscriptions, which helps to organize \n",
      "resources into a hierarchy for policy and access management. Later, you will see how \n",
      "management groups are used as a scope for governance, policy management, and \n",
      "access management.\n",
      "In the next section, we will be discussing Azure tags, which play another vital role in the \n",
      "logical grouping of resources. \n",
      "Azure tags\n",
      "Azure allows the tagging of resource groups and resources with name-value pairs. \n",
      "Tagging helps in the logical organization and categorization of resources. Azure \n",
      "also allows the tagging of 50 name-value pairs for a resource group and its resources.\n",
      "---------------\n",
      "Although a resource group acts as a container or a placeholder for resources, tagging \n",
      "a resource group does not mean the tagging of its constituent resources. Resource \n",
      "groups and resources should be tagged based on their usage, which will be explained \n",
      "later in this section. Tags are bound to a subscription, resource group, or resource. \n",
      "Azure accepts any name-value pair, and so it is important for an organization to define \n",
      "both the names and their possible values.\n",
      "---------------\n",
      "148 | Designing policies, locks, and tags for Azure deployments\n",
      "But why is tagging important? In other words, what problems can be solved using \n",
      "tagging? Tagging has the following benefits:\n",
      "• Categorization of resources: An Azure subscription can be used by multiple \n",
      "departments within an organization. It is important for the management team \n",
      "to identify the owners of any resources. Tagging helps in assigning identifiers to \n",
      "resources that can be used to represent departments or roles.\n",
      "• Information management for Azure resources: Again, Azure resources can be \n",
      "provisioned by anyone with access to the subscription. Organizations like to \n",
      "have a proper categorization of resources in place to comply with information \n",
      "management policies. Such policies can be based on application life cycle \n",
      "management, such as the management of development, testing, and production \n",
      "environments. They could also be based on usage or any other priorities. Each\n",
      "---------------\n",
      "organization has its own way of defining information categories, and Azure caters \n",
      "for this with tags.\n",
      "• Cost management: Tagging in Azure can help in identifying resources based \n",
      "on their categorization. Queries can be executed against Azure to identify cost \n",
      "per category, for instance. For example, the cost of resources in Azure for the \n",
      "development of an environment for the finance department and the marketing \n",
      "department can be easily ascertained. Moreover, Azure also provides billing \n",
      "information based on tags. This helps in identifying the consumption rates of \n",
      "teams, departments, or groups.\n",
      "Tags in Azure do have certain limitations, however:\n",
      "• Azure allows a maximum of 50 tag name-value pairs to be associated with \n",
      "resource groups. \n",
      "• Tags are non-inheritable. Tags applied to a resource group do not apply to the \n",
      "individual resources within it. However, it is quite easy to forget to tag resources \n",
      "when provisioning them. Azure Policy provides the mechanism to use to ensure\n",
      "---------------\n",
      "that tags are tagged with the appropriate value during provision time. We will \n",
      "consider the details of such policies later in this chapter.\n",
      "Tags can be assigned to resources and resource groups using PowerShell, the Azure \n",
      "CLI 2.0, Azure Resource Manager templates, the Azure portal, and the Azure Resource \n",
      "Manager REST APIs.\n",
      "---------------\n",
      "Azure tags | 149\n",
      "An example of information management categorization using Azure tags is shown here:\n",
      " \n",
      "Figure 5.2: Information management categorization using Azure tags\n",
      "In this example, the Department, Project, Environment, Owner, Approver, Maintainer, \n",
      "Start Date, Retire Date, and Patched Date name-value pairs are used to tag resources. \n",
      "It is extremely easy to find all the resources for a particular tag or a combination of tags \n",
      "using PowerShell, the Azure CLI, or REST APIs. The next section will discuss ways to use \n",
      "PowerShell to assign tags to resources.\n",
      "Project\n",
      "Environment\n",
      "Owner\n",
      "Department\n",
      "Start DatePatched\n",
      "Date\n",
      "Retire Date\n",
      "Maintainer\n",
      "Approver\n",
      "Azure\n",
      "Resource\n",
      "---------------\n",
      "150 | Designing policies, locks, and tags for Azure deployments\n",
      "Tags with PowerShell\n",
      "Tags can be managed using PowerShell, Azure Resource Manager templates, the Azure \n",
      "portal, and REST APIs. In this section, PowerShell will be used to create and apply tags. \n",
      "PowerShell provides a cmdlet for retrieving and attaching tags to resource groups and \n",
      "resources:\n",
      "• To retrieve tags associated with a resource using PowerShell, \n",
      "the Get-AzResource cmdlet can be used:\n",
      "(Get-AzResource -Tag @{ \"Environment\"=\"Production\"}).Name \n",
      "• To retrieve tags associated with a resource group using PowerShell, the following \n",
      "command can be used:\n",
      "Get-AzResourceGroup -Tag @{\"Environment\"=\"Production\"} \n",
      "• To set tags to a resource group, the Update-AzTag cmdlet can be used:\n",
      "$tags = @{\"Dept\"=\"IT\"; \"Environment\"=\"Production\"}\n",
      "$resourceGroup = Get-AzResourceGroup -Name demoGroup\n",
      "New-AzTag -ResourceId $resourceGroup.ResourceId -Tag $tags\n",
      "• To set tags to a resource, the same Update-AzTag cmdlet can be used:\n",
      "---------------\n",
      "$tags = @{\"Dept\"=\"Finance\"; \"Status\"=\"Normal\"}\n",
      "$resource = Get-AzResource -Name demoStorage -ResourceGroup demoGroup \n",
      "New-AzTag -ResourceId $resource.id -Tag $tags\n",
      "• You can update existing tags using the Update-AzTag command; however, you need \n",
      "to specify the operation as Merge or Replace. Merge will append the new tags you \n",
      "are passing into the existing tags; however, the Replace operation will replace \n",
      "all the old tags with the new ones. Here is one example of updating the tags in a \n",
      "resource group without replacing the existing ones:\n",
      "$tags = @{\"Dept\"=\"IT\"; \"Environment\"=\"Production\"}\n",
      "$resourceGroup = Get-AzResourceGroup -Name demoGroup\n",
      "Update-AzTag -ResourceId $resourceGroup.ResourceId -Tag $tags  -Operation \n",
      "Merge\n",
      "Let's now look at tags with Azure Resource Manager templates.\n",
      "Tags with Azure Resource Manager templates\n",
      "Azure Resource Manager templates also help in defining tags for each resource. They \n",
      "can be used to assign multiple tags to each resource, as follows:\n",
      "---------------\n",
      "Azure tags | 151\n",
      "{ \n",
      "    \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/\n",
      "deploymentTemplate.json#\", \n",
      "    \"contentVersion\": \"1.0.0.0\", \n",
      "    \"resources\": [ \n",
      "    { \n",
      "      \"apiVersion\": \"2019-06-01\", \n",
      "      \"type\": \"Microsoft.Storage/storageAccounts\", \n",
      "      \"name\": \"[concat('storage', uniqueString(resourceGroup().id))]\", \n",
      "      \"location\": \"[resourceGroup().location]\", \n",
      "      \"tags\": { \n",
      "        \"Dept\": \"Finance\", \n",
      "        \"Environment\": \"Production\" \n",
      "      }, \n",
      "      \"sku\": { \n",
      "        \"name\": \"Standard_LRS\" \n",
      "      }, \n",
      "      \"kind\": \"Storage\", \n",
      "      \"properties\": { } \n",
      "    } \n",
      "    ] \n",
      "}\n",
      "In the previous example, a couple of tags, Dept and Environment, were added to a storage \n",
      "account resource using Azure Resource Manager templates.\n",
      "Tagging resource groups versus resources\n",
      "It is a must for architects to decide the taxonomy and information architecture for \n",
      "Azure resources and resource groups. They should identify the categories by which\n",
      "---------------\n",
      "resources will be classified based on the query requirements. However, they must also \n",
      "identify whether tags should be attached to individual resources or to resource groups.\n",
      "---------------\n",
      "152 | Designing policies, locks, and tags for Azure deployments\n",
      "If all resources within a resource group need the same tag, then it is better to tag the \n",
      "resource group, even though tags don't inherit the resources in the resource group. \n",
      "If your organization requires tags to be passed to all the underlying resources, then \n",
      "you can consider writing a PowerShell script to get the tags from the resource group \n",
      "and update the tags for the resources in the resource group. It is important to take \n",
      "queries on tags into consideration before deciding whether tags should be applied \n",
      "at the resource level or the resource group level. If the queries relate to individual \n",
      "resource types across a subscription and across resource groups, then assigning tags \n",
      "to individual resources makes more sense. However, if identifying resource groups is \n",
      "enough for your queries to be effective, then tags should be applied only to resource\n",
      "---------------\n",
      "groups. If you are moving resources across resource groups, the tags applied at the \n",
      "resource group level will be lost. If you are moving resources, consider adding the \n",
      "tags again.\n",
      "Azure Policy\n",
      "In the previous section, we talked about applying tags for Azure deployments. Tags \n",
      "are great for organizing resources; however, there is one more thing that was not \n",
      "discussed: how do organizations ensure that tags are applied for every deployment? \n",
      "There should be automated enforcement of Azure tags to resources and resource \n",
      "groups. There is no check from Azure to ensure that appropriate tags are applied to \n",
      "resources and resource groups. This is not just specific to tags—this applies to the \n",
      "configuration of any resource on Azure. For example, you may wish to restrict where \n",
      "your resources can be provisioned geographically (to only the US-East region, for \n",
      "instance).\n",
      "You might have guessed by now that this section is all about formulating a governance\n",
      "---------------\n",
      "model on Azure. Governance is an important element in Azure because it ensures that \n",
      "everyone accessing the Azure environment is aware of organizational priorities and \n",
      "processes. It also helps to bring costs under control. It helps in defining organizational \n",
      "conventions for managing resources.\n",
      "Each policy can be built using multiple rules, and multiple policies can be applied to \n",
      "a subscription or resource group. Based on whether the rules are satisfied, policies \n",
      "can execute various actions. An action could be to deny an ongoing transaction, to \n",
      "audit a transaction (which means writing to logs and allowing it to finish), or to append \n",
      "metadata to a transaction if it's found to be missing.\n",
      "Policies could be related to the naming convention of resources, the tagging of \n",
      "resources, the types of resources that can be provisioned, the location of resources, or \n",
      "any combination of those.\n",
      "Azure provides numerous built-in policies and it is possible to create custom policies.\n",
      "---------------\n",
      "There is a policy language based on JSON that can be used to define custom policies.\n",
      "---------------\n",
      "Azure Policy | 153\n",
      "Now that you know the purpose and use case of Azure Policy, let's go ahead and discuss \n",
      "built-in policies, policy language, and custom policies.\n",
      "Built-in policies\n",
      "Azure provides a service for the creation of custom policies; however, it also provides \n",
      "some out-of-the-box policies that can be used for governance. These policies relate to \n",
      "allowed locations, allowed resource types, and tags. More information for these built-in \n",
      "policies can be found at https:/ /docs.microsoft.com/azure/azure-resource-manager/\n",
      "resource-manager-policy.\n",
      "Policy language\n",
      "Policies in Azure use JSON to define and describe policies. There are two steps in policy \n",
      "adoption. The policy should be defined and then it should be applied and assigned. \n",
      "Policies have scope and can be applied at the management group, subscription, or \n",
      "resource group level.\n",
      "Policies are defined using if...then blocks, similar to any popular programming\n",
      "---------------\n",
      "language. The if block is executed to evaluate the conditions, and based on the result of \n",
      "those conditions, the then block is executed:\n",
      "{ \n",
      "  \"if\": { \n",
      "    <condition> | <logical operator> \n",
      "  }, \n",
      "  \"then\": { \n",
      "    \"effect\": \"deny | audit | append\" \n",
      "  } \n",
      "} \n",
      "The policies not only allow simple if conditions but also allow multiple if conditions \n",
      "to be joined together logically to create complex rules. These conditions can be joined \n",
      "using AND, OR, and NOT operators:\n",
      "• The AND syntax requires all conditions to be true.\n",
      "• The OR syntax requires one of the conditions to be true.\n",
      "• The NOT syntax inverts the result of the condition.\n",
      "---------------\n",
      "154 | Designing policies, locks, and tags for Azure deployments\n",
      "The AND syntax is shown next. It is represented by the allOf keyword:\n",
      "\"if\": { \n",
      "  \"allOf\": [ \n",
      "    { \n",
      "       \"field\": \"tags\", \n",
      "        \"containsKey\": \"application\" \n",
      "    }, \n",
      "    { \n",
      "      \"field\": \"type\", \n",
      "      \"equals\": \"Microsoft.Storage/storageAccounts\" \n",
      "    } \n",
      "  ] \n",
      "}, \n",
      "The OR syntax is shown next. It is represented by the anyOf keyword:\n",
      "\"if\": { \n",
      "  \"anyOf\": [ \n",
      "    { \n",
      "       \"field\": \"tags\", \n",
      "        \"containsKey\": \"application\" \n",
      "    }, \n",
      "    { \n",
      "      \"field\": \"type\", \n",
      "      \"equals\": \"Microsoft.Storage/storageAccounts\" \n",
      "    } \n",
      "  ] \n",
      "}, \n",
      "The NOT syntax is shown next. It is represented by the not keyword:\n",
      "\"if\": { \n",
      "  \"not\": [ \n",
      "    { \n",
      "       \"field\": \"tags\",\n",
      "---------------\n",
      "Azure Policy | 155\n",
      "        \"containsKey\": \"application\" \n",
      "    }, \n",
      "    { \n",
      "      \"field\": \"type\", \n",
      "      \"equals\": \"Microsoft.Storage/storageAccounts\" \n",
      "    } \n",
      "  ] \n",
      "}, \n",
      "In fact, these logical operators can be combined together, as follows:\n",
      "\"if\": { \n",
      "  \"allOf\": [ \n",
      "    { \n",
      "      \"not\": { \n",
      "        \"field\": \"tags\", \n",
      "        \"containsKey\": \"application\" \n",
      "      } \n",
      "    }, \n",
      "    { \n",
      "      \"field\": \"type\", \n",
      "      \"equals\": \"Microsoft.Storage/storageAccounts\" \n",
      "    } \n",
      "  ] \n",
      "},\n",
      "This is very similar to the use of if conditions in popular programming languages such \n",
      "as C# and Node.js:\n",
      "If (\"type\" == \"Microsoft.Storage/storageAccounts\") { \n",
      "      Deny \n",
      "}\n",
      "It is important to note that there is no allow action, although there is a Deny action. This \n",
      "means that policy rules should be written with the possibility of denial in mind. Rules \n",
      "should evaluate conditions and Deny the action if the conditions are met.\n",
      "---------------\n",
      "156 | Designing policies, locks, and tags for Azure deployments\n",
      "Allowed fields\n",
      "The fields that are allowed in conditions while defining policies are as follows:\n",
      "• Name: The name of the resource for applying the policy to. This is very specific and \n",
      "applicable to a resource by its usage.\n",
      "• Type: The type of resource, such as Microsoft.Compute/VirtualMachines. That \n",
      "would apply the policy to all instances of virtual machines, for example.\n",
      "• Location: The location (that is, the Azure region) of a resource.\n",
      "• Tags: The tags associated with a resource.\n",
      "• Property aliases: Resource-specific properties. These properties are different for \n",
      "different resources.\n",
      "In the next section, you will learn more about safekeeping resources in production \n",
      "environments. \n",
      "Azure locks\n",
      "Locks are mechanisms for stopping certain activities on resources. RBAC provides \n",
      "rights to users, groups, and applications within a certain scope. There are out-of-the-\n",
      "---------------\n",
      "box RBAC roles, such as owner, contributor, and reader. With the contributor role, it is \n",
      "possible to delete or modify a resource. How can such activities be prevented despite \n",
      "the user having a contributor role? Enter Azure locks.\n",
      "Azure locks can help in two ways:\n",
      "• They can lock resources such that they cannot be deleted, even if you have owner \n",
      "access.\n",
      "• They can lock resources in such a way that they can neither be deleted nor have \n",
      "their configuration modified.\n",
      "Locks are typically very helpful for resources in production environments that should \n",
      "not be modified or deleted accidentally.\n",
      "Locks can be applied at the levels of subscription, resource group, management group, \n",
      "and individual resource. Locks can be inherited between subscriptions, resource \n",
      "groups, and resources. Applying a lock at the parent level will ensure that those \n",
      "resources at the child level will also inherit it. Resources that are added later in the\n",
      "---------------\n",
      "sub-scope also inherit the lock configuration by default. Applying a lock at the resource \n",
      "level will also prevent the deletion of the resource group containing the resource.\n",
      "---------------\n",
      "Azure locks | 157\n",
      "Locks are applied only to operations that help in managing a resource, rather than \n",
      "operations that are within a resource. Users need either  \n",
      "Microsoft.Authorization/* or Microsoft.Authorization/locks/* RBAC permissions  \n",
      "to create and modify locks.\n",
      "Locks can be created and applied through the Azure portal, Azure PowerShell, the \n",
      "Azure CLI, Azure Resource Manager templates, and REST APIs.\n",
      "Creating a lock using an Azure Resource Manager template is done as follows:\n",
      "{ \n",
      "  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/\n",
      "deploymentTemplate.json#\", \n",
      "  \"contentVersion\": \"1.0.0.0\", \n",
      "  \"parameters\": { \n",
      "    \"lockedResource\": { \n",
      "      \"type\": \"string\" \n",
      "    } \n",
      "  }, \n",
      "  \"resources\": [ \n",
      "    { \n",
      "      \"name\": \"[concat(parameters('lockedResource'), '/Microsoft.\n",
      "Authorization/myLock')]\", \n",
      "      \"type\": \"Microsoft.Storage/storageAccounts/providers/locks\", \n",
      "      \"apiVersion\": \"2019-06-01\", \n",
      "      \"properties\": { \n",
      "        \"level\": \"CannotDelete\" \n",
      "      } \n",
      "    } \n",
      "  ]\n",
      "---------------\n",
      "}\n",
      "The resources section of the Azure Resource Manager template code consists of a list of \n",
      "all the resources to be provisioned or updated within Azure. There is a storage account \n",
      "resource, and the storage account has a lock resource. A name for the lock is provided \n",
      "using dynamic string concatenation, and the lock that's applied is of the CannotDelete \n",
      "type, which means that the storage account is locked for deletion. The storage account \n",
      "can only be deleted after the removal of the lock.\n",
      "---------------\n",
      "158 | Designing policies, locks, and tags for Azure deployments\n",
      "Creating and applying a lock to a resource using PowerShell is done as follows:\n",
      "New-AzResourceLock -LockLevel CanNotDelete -LockName LockSite '\n",
      "  -ResourceName examplesite -ResourceType Microsoft.Web/sites '\n",
      "  -ResourceGroupName exampleresourcegroup\n",
      "Creating and applying a lock to a resource group using PowerShell is done as follows:\n",
      "New-AzResourceLock -LockName LockGroup -LockLevel CanNotDelete '\n",
      "  -ResourceGroupName exampleresourcegroup\n",
      "Creating and applying a lock to a resource using the Azure CLI is done as follows:\n",
      "az lock create --name LockSite --lock-type CanNotDelete \\\n",
      "  --resource-group exampleresourcegroup --resource-name examplesite \\\n",
      "  --resource-type Microsoft.Web/sites\n",
      "Creating and applying a lock to a resource group using the Azure CLI is done as follows:\n",
      "az lock create --name LockGroup --lock-type CanNotDelete \\ --resource-group \n",
      "exampleresourcegroup\n",
      "---------------\n",
      "To create or delete resource locks, the user should have access to the  \n",
      "Microsoft.Authorization/* or Microsoft.Authorization/locks/* actions. You can \n",
      "further give granular permissions as well. Owners and user access administrators will \n",
      "have access to creating or deleting locks by default.\n",
      "If you are wondering what the Microsoft.Authorization/* and  \n",
      "Microsoft.Authorization/locks/* keywords are, you will get to know more about them \n",
      "in the next section.\n",
      "Let's now look at Azure RBAC.\n",
      "Azure RBAC\n",
      "Azure provides authentication using Azure Active Directory for its resources. Once \n",
      "an identity has been authenticated, the resources the identity will be allowed to \n",
      "access should be decided. This is known as authorization. Authorization evaluates the \n",
      "permissions that have been afforded to an identity. Anybody with access to an Azure \n",
      "subscription should be given just enough permissions so that their specific job can be \n",
      "performed, and nothing more.\n",
      "---------------\n",
      "Authorization is popularly also known as RBAC. RBAC in Azure refers to the assigning \n",
      "of permissions to identities within a scope. The scope could be a management group, a \n",
      "subscription, a resource group, or individual resources.\n",
      "---------------\n",
      "Azure RBAC | 159\n",
      "RBAC helps in the creation and assignment of different permissions to different \n",
      "identities. This helps in segregating duties within teams, rather than everyone having \n",
      "all permissions. RBAC helps in making people responsible for their job only, because \n",
      "others might not even have the necessary access to perform it. It should be noted that \n",
      "providing permissions at a greater scope automatically ensures that child resources \n",
      "inherit those permissions. For example, providing an identity with read access to a \n",
      "resource group means that the identity will have read access to all the resources within \n",
      "that group, too.\n",
      "Azure provides three general-purpose, built-in roles. They are as follows:\n",
      "• The owner role, which has full access to all resources\n",
      "• The contributor role, which has access to read/write resources\n",
      "• The reader role, which has read-only permissions to resources\n",
      "There are more roles provided by Azure, but they are resource-specific, such as the\n",
      "---------------\n",
      "network contributor and security manager roles.\n",
      "To get all roles provided by Azure for all resources, execute \n",
      "the Get-AzRoleDefinition command in the PowerShell console.\n",
      "Each role definition has certain allowed and disallowed actions. For example, the owner \n",
      "role has all actions permitted; no action is prohibited:\n",
      "PS C:\\Users\\riskaria> Get-AzRoleDefinition -Name \"Owner\"\n",
      "Name             : Owner\n",
      "Id               : 8e3af657-a8ff-443c-a75c-2fe8c4bcb635\n",
      "IsCustom         : False\n",
      "Description      : Lets you manage everything, including access to resources.\n",
      "Actions          : {*}\n",
      "NotActions       : {}\n",
      "DataActions      : {}\n",
      "NotDataActions   : {}\n",
      "AssignableScopes : {/}\n",
      "Each role comprises multiple permissions. Each resource provides a list of \n",
      "operations. The operations supported by a resource can be obtained using \n",
      "the Get-AzProviderOperation cmdlet. This cmdlet takes the name of the provider and \n",
      "resource to retrieve the operations:\n",
      "---------------\n",
      "160 | Designing policies, locks, and tags for Azure deployments\n",
      "PS C:\\Users\\riskaria> Get-AzProviderOperation -OperationSearchString \n",
      "\"Microsoft.Insights/*\" | select Operation\n",
      "This will result in the following output:\n",
      "PS C:\\Users\\riskaria> Get-AzProviderOperation -OperationSearchString \n",
      "\"Microsoft.Insights/*\" | select Operation\n",
      "Operation                                                                                 \n",
      "---------                                                                                 \n",
      "Microsoft.Insights/Metrics/Action                                                         \n",
      "Microsoft.Insights/Register/Action                                                        \n",
      "Microsoft.Insights/Unregister/Action                                                      \n",
      "Microsoft.Insights/ListMigrationDate/Action                                               \n",
      "Microsoft.Insights/MigrateToNewpricingModel/Action\n",
      "---------------\n",
      "Microsoft.Insights/RollbackToLegacyPricingModel/Action  \n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "Microsoft.Insights/PrivateLinkScopes/PrivateEndpointConnectionProxies/Read                \n",
      "Microsoft.Insights/PrivateLinkScopes/PrivateEndpointConnectionProxies/Write               \n",
      "Microsoft.Insights/PrivateLinkScopes/PrivateEndpointConnectionProxies/Delete              \n",
      "Microsoft.Insights/PrivateLinkScopeOperationStatuses/Read                                 \n",
      "Microsoft.Insights/DiagnosticSettingsCategories/Read\n",
      "The output shown here provides all the actions available within the Microsoft.Insights \n",
      "resource provider across its associated resources. The resources include Metrics, \n",
      "Register, and others, while the actions include Read, Write, and others. \n",
      "Let's now look at custom roles.\n",
      "---------------\n",
      "Azure RBAC | 161\n",
      "Custom roles\n",
      "Azure provides numerous out-of-the-box generic roles, such as owner, contributor, \n",
      "and reader, as well as specialized resource-specific roles, such as virtual machine \n",
      "contributor. Having a reader role assigned to a user/group or service principal will \n",
      "mean reader permissions being assigned to the scope. The scope could be a resource, \n",
      "resource group, or a subscription. Similarly, a contributor would be able to read as well \n",
      "as modify the assigned scope. A virtual machine contributor would be able to modify \n",
      "virtual machine settings and not any other resource settings. There are, however, times \n",
      "when existing roles might not suit our requirements. In such cases, Azure allows the \n",
      "creation of custom roles. They can be assigned to users, groups, and service principals \n",
      "and are applicable to resources, resource groups, and subscriptions.\n",
      "Custom roles are created by combining multiple permissions. For example, a custom\n",
      "---------------\n",
      "role can consist of operations from multiple resources. In the next code block, a new \n",
      "role definition is being created, but instead of setting all properties manually, one \n",
      "of the existing \"Virtual Machine Contributor\" roles is retrieved because it almost \n",
      "matches with the configuration of the new custom role. Avoid using the same name \n",
      "as built-in roles, as that would create conflict. Then, the ID property is nullified and a \n",
      "new name and description is provided. The code also clears all the actions, adds some \n",
      "actions, adds a new scope after clearing the existing scope, and finally creates a new \n",
      "custom role:\n",
      "$role = Get-AzRoleDefinition \"Virtual Machine Contributor\" \n",
      "$role.Id = $null \n",
      "$role.Name = \"Virtual Machine Operator\" \n",
      "$role.Description = \"Can monitor and restart virtual machines.\" \n",
      "$role.Actions.Clear()\n",
      "$role.Actions.Add(\"Microsoft.Storage/*/read\") \n",
      "$role.Actions.Add(\"Microsoft.Network/*/read\") \n",
      "$role.Actions.Add(\"Microsoft.Compute/*/read\")\n",
      "---------------\n",
      "$role.Actions.Add(\"Microsoft.Compute/virtualMachines/start/action\") \n",
      "$role.Actions.Add(\"Microsoft.Compute/virtualMachines/restart/action\")\n",
      "$role.Actions.Add(\"Microsoft.Authorization/*/read\") \n",
      "$role.Actions.Add(\"Microsoft.Resources/subscriptions/resourceGroups/read\") \n",
      "$role.Actions.Add(\"Microsoft.Insights/alertRules/*\") \n",
      "$role.Actions.Add(\"Microsoft.Support/*\") \n",
      "$role.AssignableScopes.Clear() \n",
      "$role.AssignableScopes.Add(\"/subscriptions/548f7d26-b5b1-468e-ad45-\n",
      "---------------\n",
      "162 | Designing policies, locks, and tags for Azure deployments\n",
      "6ee12accf7e7\") \n",
      "New-AzRoleDefinition -Role $role\n",
      "There is a preview feature available in the Azure portal that you can use to create \n",
      "custom RBAC roles from the Azure portal itself. You have the option to create roles \n",
      "from scratch, clone an existing role, or start writing the JSON manifest. Figure 5.3 shows \n",
      "the Create a custom role blade, which is available at IAM > +Add section:\n",
      "Figure 5.3: Creating custom roles from the Azure portal\n",
      "This makes the process of custom role creation hassle-free.\n",
      "How are locks different from RBAC?\n",
      "Locks are not the same as RBAC. RBAC helps in allowing or denying permissions for \n",
      "resources. These permissions relate to performing operations, such as read, write, \n",
      "and update operations on resources. Locks, on the other hand, relate to disallowing \n",
      "permissions to configure or delete resources.\n",
      "In the next section, we will be discussing Azure Blueprints, which helps us with the\n",
      "---------------\n",
      "orchestration of artifacts, such as role assignments, policy assignments, and more, that \n",
      "we have discussed so far.\n",
      "Azure Blueprints\n",
      "You will be familiar with the word blueprint, which refers to the plan or drawing that \n",
      "is used by an architect to architect a solution. Similarly, in Azure, cloud architects can \n",
      "leverage Azure Blueprints to define a set of repeatable Azure resources that adheres to \n",
      "an organization's standards, processes, and patterns.\n",
      "---------------\n",
      "An example of implementing Azure governance features | 163\n",
      "Blueprints allows us to orchestrate the deployment of various resources and other \n",
      "artifacts, such as:\n",
      "• Role assignments\n",
      "• Policy assignments\n",
      "• Azure Resource Manager templates\n",
      "• Resource groups\n",
      "Azure blueprint objects are replicated to multiple regions and are backed by Azure \n",
      "Cosmos DB. The replication helps in providing consistent access to resources and \n",
      "maintaining the organization's standards irrespective of which region you are \n",
      "deploying to.\n",
      "Azure Blueprints comprises various artifacts, and you can find the list of supported \n",
      "artifacts here: https:/ /docs.microsoft.com/azure/governance/blueprints/\n",
      "overview#blueprint-definition.\n",
      "Blueprints can be created from the Azure portal, Azure PowerShell, the Azure CLI, REST \n",
      "APIs, or ARM templates. \n",
      "In the next section, we will look at an example of implementing Azure governance \n",
      "features. Services and features such as RBAC, Azure Policy, and Azure resource locks\n",
      "---------------\n",
      "will be used in the example.\n",
      "An example of implementing Azure governance features\n",
      "In this section, we will go through a sample architecture implementation for a fictitious \n",
      "organization that wants to implement Azure governance and cost management \n",
      "features.\n",
      "Background\n",
      "Company Inc is a worldwide company that is implementing a social media solution \n",
      "on an Azure IaaS platform. They use web servers and application servers deployed on \n",
      "Azure virtual machines and networks. Azure SQL Server acts as the backend database.\n",
      "RBAC for Company Inc\n",
      "The first task is to ensure that the appropriate teams and application owners can access \n",
      "their resources. It is recognized that each team has different requirements. For the \n",
      "sake of clarity, Azure SQL is deployed in a separate resource group to the Azure IaaS \n",
      "artifacts.\n",
      "---------------\n",
      "164 | Designing policies, locks, and tags for Azure deployments\n",
      "The administrator assigns the following roles for the subscription:\n",
      "Table 5.1: Different roles with access details \n",
      "Azure Policy\n",
      "The company should implement Azure Policy to ensure that its users always provision \n",
      "resources according to the company guidelines. \n",
      "The policies in Azure govern various aspects related to the deployment of resources. \n",
      "The policies will also govern updates after the initial deployment. Some of the policies \n",
      "that should be implemented are given in the following section.\n",
      "Deployments to certain locations\n",
      "Azure resources and deployments can only be executed for certain chosen locations. It \n",
      "would not be possible to deploy resources in regions outside of the policy. For example, \n",
      "the regions that are allowed are West Europe and East US. It should not be possible to \n",
      "deploy resources in any other region.\n",
      "Tags of resources and resource groups\n",
      "---------------\n",
      "Every resource in Azure, including the resource groups, will mandatorily \n",
      "have tags assigned to it. The tags will include, as a minimum, details about the \n",
      "department, environment, creation data, and project name.\n",
      "Diagnostic logs and Application Insights for all resources\n",
      "Every resource deployed on Azure should have diagnostic logs and application logs \n",
      "enabled wherever possible.\n",
      "Role Assigned to Description\n",
      "OwnerA dministrator Manages all resource groups and the subscription\n",
      "Security \n",
      "Manager\n",
      "Security \n",
      "administrators\n",
      "This role allows users to look at Azure Security Center and \n",
      "the status of the resources.\n",
      "ContributorI nfrastructure \n",
      "management\n",
      "Managing virtual machines and other resources.\n",
      "Reader Developers Can view resources but cannot modify them. Developers are \n",
      "expected to work in their development/testing environments.\n",
      "---------------\n",
      "Summary | 165\n",
      "Azure locks\n",
      "A company should implement Azure locks to ensure that crucial resources are not \n",
      "deleted accidentally. Every resource that is crucial for the functioning of a solution \n",
      "needs to be locked down. This means that even the administrators of the services \n",
      "running on Azure do not have the capability to delete these resources; the only way to \n",
      "delete a resource is to remove the lock first.\n",
      "You should also note that:\n",
      "All production and pre-production environments, apart from the development and \n",
      "testing environments, would be locked for deletion.\n",
      "All development and testing environments that have single instances would also be \n",
      "locked for deletion.\n",
      "All resources related to the web application would be locked for deletion for all \n",
      "production environments.\n",
      "All shared resources would be locked for deletion irrespective of the environment.\n",
      "Summary\n",
      "In this chapter, you learned that governance and cost management are among the\n",
      "---------------\n",
      "top priorities for companies moving to the cloud. Having an Azure subscription with a \n",
      "pay-as-you-go scheme can harm the company budget, because anyone with access to \n",
      "the subscription can provision as many resources as they like. Some resources are free, \n",
      "but others are expensive. \n",
      "You also learned that it is important for organizations to remain in control of their \n",
      "cloud costs. Tags help in generating billing reports. These reports could be based \n",
      "on departments, projects, owners, or any other criteria. While cost is important, \n",
      "governance is equally important. Azure provides locks, policies, and RBAC to implement \n",
      "proper governance. Policies ensure that resource operations can be denied or audited, \n",
      "locks ensure that resources cannot be modified or deleted, and RBAC ensures that \n",
      "employees have the right permissions to perform their jobs. With these features, \n",
      "companies can have sound governance and cost control for their Azure deployments.\n",
      "---------------\n",
      "In the next chapter, we will be discussing cost management in Azure. We will go \n",
      "through different optimization methods, cost management, and billing APIs.\n",
      "---------------\n",
      "In the previous chapter, we discussed tags, policies, and locks and how they can \n",
      "be leveraged from a compliance standpoint. Tags allow us to add metadata to our \n",
      "resources; they also help us in the logical management of resources. In the Azure \n",
      "portal, we can filter resources based on tags. If we assume there is a large number of \n",
      "resources, which is quite common in enterprises, filtering will help us to easily manage \n",
      "our resources. Another benefit of tags is that they can be used to filter our billing \n",
      "reports or usage reports in terms of tags. In this chapter, we are going to explore cost \n",
      "management for Azure solutions.\n",
      "The primary reason why corporations are moving to the cloud is to save on cost. There \n",
      "is no upfront cost for having an Azure subscription. Azure provides a pay-as-you-go \n",
      "subscription, where billing is based on consumption. Azure measures resource usage \n",
      "and provides monthly invoices. There is no upper limit for Azure consumption. As we\n",
      "---------------\n",
      "are on a public cloud, Azure (like any other service provider) has some hard and soft \n",
      "limits on the number of resources that can be deployed. Soft limits can be increased \n",
      "by working with Azure Support. There are some resources that have a hard limit. The \n",
      "service limits can be found at https:/ /docs.microsoft.com/azure/azure-resource-\n",
      "manager/management/azure-subscription-service-limits, and the default limit varies \n",
      "based on the type of subscription you have.\n",
      "Cost management for \n",
      "Azure solutions\n",
      "6\n",
      "---------------\n",
      "168 | Cost management for Azure solutions\n",
      "It is important for companies to keep a close watch on Azure consumption and usage. \n",
      "Although they can create policies to set organizational standards and conventions, \n",
      "there is also a need to keep track of billing and consumption data. Moreover, they \n",
      "should employ best practices for consuming Azure resources so that the return is \n",
      "maximized. For this, architects need to know about Azure resources and features, their \n",
      "corresponding costs, and the cost/benefit analysis of features and solutions.\n",
      "In this chapter, we will cover the following topics:\n",
      "• Azure offer details\n",
      "• Billing\n",
      "• Invoicing\n",
      "• Usage and quotas\n",
      "• Usage and Billing APIs\n",
      "• Azure pricing calculator\n",
      "• Best practices for cost optimization\n",
      "Let's go ahead and discuss each of these points.\n",
      "Azure offer details\n",
      "Azure has different offers that you can purchase. So far, we have discussed pay-as-\n",
      "you-go, but there are others, such as Enterprise Agreements (EAs), Azure Sponsorship,\n",
      "---------------\n",
      "and Azure in CSP. We will cover each of these as they are very important for billing:\n",
      "• Pay-as-you-go: This is a commonly known offering, where customers pay based \n",
      "on the consumption and the rates are available in the public-facing documentation \n",
      "of Azure. Customers receive an invoice every month for usage from Microsoft and \n",
      "they can pay via credit card or an invoice payment method.\n",
      "• EAs: EAs entail a monetary commitment with Microsoft, which means that \n",
      "organizations sign an agreement with Microsoft and promise that they will use \n",
      "x amount of Azure resources. If the usage goes above the agreed amount, the \n",
      "customer will receive an overage invoice. Customers can create multiple accounts \n",
      "under an EA and have multiple subscriptions inside these accounts. There are two \n",
      "types of EA: Direct EA and Indirect EA. Direct EA customers have a direct billing \n",
      "relationship with Microsoft; on the other hand, with Indirect EA, the billing is\n",
      "---------------\n",
      "managed by a partner. EA customers will get better offers and discounts because \n",
      "of the commitment they make with Microsoft. The EA is managed via a portal \n",
      "called the EA portal (https:/ /ea.azure.com), and you need to have enrolment \n",
      "privileges to access this portal.\n",
      "---------------\n",
      "Understanding billing | 169\n",
      "• Azure in CSP: Azure in CSP is where a customer reaches out to a Cloud Solution \n",
      "Provider (CSP) partner, and this partner provisions a subscription for the \n",
      "customer. The billing will be completely managed by the partner; the customer \n",
      "will not have a direct billing relationship with Microsoft. Microsoft invoices the \n",
      "partner and the partner invoices the customer, adding their margin.\n",
      "• Azure Sponsorship: Sponsorship is given by Microsoft to start-ups, NGOs, and \n",
      "other non-profit organizations to use Azure. The sponsorship is for a fixed term \n",
      "and a fixed amount of credit. If the term expires or credit gets exhausted, the \n",
      "subscription will be converted to a pay-as-you-go subscription. If organizations \n",
      "want to renew their sponsorship entitlement, they have to work with Microsoft.\n",
      "We have just outlined a few of Azure's offerings. The complete list is available at https:/ /\n",
      "---------------\n",
      "azure.microsoft.com/support/legal/offer-details, which includes other offerings, such \n",
      "as Azure for Students, Azure Pass, and Dev/Test subscriptions.\n",
      "Next, let's discuss billing in Azure.\n",
      "Understanding billing\n",
      "Azure is a service utility that has the following features:\n",
      "• No upfront costs\n",
      "• No termination fees\n",
      "• Billing per second, per minute, or per hour depending on the type of resource\n",
      "• Payment based on consumption-on-the-go\n",
      "In such circumstances, it is very difficult to estimate the upfront cost of consuming \n",
      "Azure resources. Every resource in Azure has its own cost model and charge based on \n",
      "storage, usage, and time span. It is very important for management, administration, and \n",
      "finance departments to keep track of usage and costs. Azure provides usage and billing \n",
      "reporting capabilities to help upper management and administrators generate cost and \n",
      "usage reports based on multiple criteria.\n",
      "The Azure portal provides detailed billing and usage information through the Cost\n",
      "---------------\n",
      "Management + Billing feature, which can be accessed from the master navigation \n",
      "blade, as shown in Figure 6.1:\n",
      "---------------\n",
      "170 | Cost management for Azure solutions\n",
      "Figure 6.1: Cost Management + Billing service in the Azure portal\n",
      "Please note that if your billing is managed by a CSP, you will not have access to this \n",
      "feature. CSP customers can view their costs in the pay-as-you-go scheme if they \n",
      "transition their CSP legacy subscriptions to the Azure plan. We will discuss the Azure \n",
      "plan and the Modern Commerce platform later in the chapter.\n",
      "Cost Management + Billing shows all the subscriptions and the billing scopes you have \n",
      "access to, as shown in Figure 6.2:\n",
      "Figure 6.2: Billing overview of user subscriptions\n",
      "---------------\n",
      "Understanding billing | 171\n",
      "The Cost Management section has several blades, such as:\n",
      "• Cost analysis for analyzing the usage of a scope.\n",
      "• Budgets for settings budgets.\n",
      "• Cost alerts for notifying administrators when the usage exceeds a certain \n",
      "threshold.\n",
      "• Advisor recommendations for getting advice on how to make potential savings. \n",
      "We will discuss Azure Advisor in the last section of this chapter.\n",
      "• Exports for automating usage exports to Azure Storage.\n",
      "• Cloudyn, which is a tool used by CSP partners to analyze costs, as they don't have \n",
      "access to Cost Management.\n",
      "• Connectors for AWS for connecting your AWS consumption data to Azure Cost \n",
      "Management.\n",
      "The different options available in Azure Cost Management are shown in Figure 6.3:\n",
      "Figure 6.3: Cost Management overview\n",
      "---------------\n",
      "172 | Cost management for Azure solutions\n",
      "Clicking on the Cost analysis menu on this blade provides a comprehensive interactive \n",
      "dashboard, using which cost can be analyzed with different dimensions and measures:\n",
      "Figure 6.4: Analyzing subscription costs through the Cost analysis option\n",
      "The dashboard not only shows the current cost but also forecasts the cost and breaks \n",
      "it down based on multiple dimensions. Service name, Location, and Resource group \n",
      "name are provided by default, but they can be changed to other dimensions as well. \n",
      "There will be always a scope associated with every view. Some of the available scopes \n",
      "are billing account, management group, subscription, and resource group. You can \n",
      "switch the scope depending on the level you want to analyze.\n",
      "The Budget menu on the left allows us to set up the budget for better cost management \n",
      "and provides alerting features in case the actual cost is going to breach the budget \n",
      "estimates:\n",
      "---------------\n",
      "Understanding billing | 173\n",
      "Figure 6.5: Creating a budget\n",
      "Cost Management also allows us to fetch cost data from other clouds, such as AWS, \n",
      "within the current dashboards, thereby managing costs for multiple clouds from a \n",
      "single blade and dashboard. However, this feature is in preview at the time of writing. \n",
      "This connector will become chargeable after August 25, 2020.\n",
      "---------------\n",
      "174 | Cost management for Azure solutions\n",
      "You need to fill in your AWS role details and other details to pull the cost information, as \n",
      "shown in Figure 6.5. If you are unsure how to create the policy and role in AWS, refer to \n",
      "https:/ /docs.microsoft.com/azure/cost-management-billing/costs/aws-integration-\n",
      "set-up-configure#create-a-role-and-policy-in-aws:\n",
      "Figure 6.6: Creating an AWS connector in Cost Management\n",
      "The cost reports can also be exported to a storage account on a scheduled basis.\n",
      "Some of the cost analysis is also available in the Subscriptions blade. In the Overview \n",
      "section, you can see resources and their cost. Also, there is another graph, where you \n",
      "can see your current spend, forecast, and balance credit (if you are using a credit-based \n",
      "subscription).\n",
      "---------------\n",
      "Understanding billing | 175\n",
      "Figure 6.7 shows cost information:\n",
      "Figure 6.7: Cost analysis for the subscription\n",
      "Clicking on any of the costs in Figure 6.7 will redirect you to the Cost Management – \n",
      "Cost Analysis section. There are a lot of dimensions in Cost Management with which \n",
      "you can group the data for analysis. The available dimensions will vary based on the \n",
      "scope you have selected. Some of the commonly used dimensions are as follows:\n",
      "• Resource types\n",
      "• Resource group\n",
      "• Tag\n",
      "• Resource location\n",
      "• Resource ID\n",
      "• Meter category\n",
      "• Meter subcategory\n",
      "• Service\n",
      "---------------\n",
      "176 | Cost management for Azure solutions\n",
      "At the beginning of the chapter, we said that tags can be used for cost management. For \n",
      "example, let's say you have a tag called Department with values of IT, HR, and Finance. \n",
      "Tagging the resources appropriately will help you understand the cost incurred by each \n",
      "department. You can also download the cost report as a CSV, Excel, or PNG file using \n",
      "the Download button.\n",
      "Additionally, Cost Management supports multiple views. You can create your own \n",
      "dashboard and save it. EA customers get the added benefit of the Cost Management \n",
      "connector or Power BI. With the connector, users can pull the usage statistics to Power \n",
      "BI and create visualizations.\n",
      "Up to this point, we have been discussing how we can keep track of our usage using \n",
      "Cost Management. In the next section, we will explore how invoicing works for the \n",
      "services we have used.\n",
      "Invoicing\n",
      "Azure's billing system also provides information about invoices that are generated \n",
      "monthly.\n",
      "---------------\n",
      "Depending on the offer type, the method of invoicing may vary. For pay-as-you-go \n",
      "users, the invoices will be sent monthly to the account administrator. However, for EA \n",
      "customers, the invoice will be sent to the contact on the enrollment.\n",
      "Clicking on the Invoices menu brings up a list of all the invoices generated, and clicking \n",
      "on any of the invoices provides details about that invoice. Figure 6.8 shows how the \n",
      "invoices are shown in the Azure portal:\n",
      "Figure 6.8: List of invoices and their details\n",
      "---------------\n",
      "Invoicing | 177\n",
      "There are two types of invoices: one is for Azure services such as SQL, Virtual \n",
      "Machines, and Networking. Another type is for Azure Marketplace and Reservations. \n",
      "Azure Marketplace provides partner services from different vendors for customers. We \n",
      "will be talking about Azure Reservations later on.\n",
      "By default, for a pay-as-you-go subscription, the account admin has access to \n",
      "the invoices. If they want, they can delegate access to other users, such as the \n",
      "organization's finance team, by choosing the Access invoice option in Figure 6.8. \n",
      "Additionally, the account admin can opt for email addresses where they want to send \n",
      "out the copies of the invoices.\n",
      "The Email Invoice option is not available for Support Plan now. Alternatively, you can \n",
      "visit the Accounts portal and download the invoice. Microsoft is slowly moving away \n",
      "from this portal, and most of the features are getting deprecated as they are integrated \n",
      "into the Azure portal.\n",
      "---------------\n",
      "So far, we have discussed subscriptions and how invoicing is done. Something new \n",
      "that has been introduced by Microsoft is Modern Commerce. With this new commerce \n",
      "experience, the purchase process and experience has been simplified. Let's take a \n",
      "closer look at Modern Commerce and learn how it is different from the legacy platform \n",
      "that we have discussed so far.\n",
      "The Modern Commerce experience\n",
      "If your organization is already working with Microsoft, you will know that there are \n",
      "multiple agreements involved for each offer, such as Web Direct, EAs, CSP, Microsoft \n",
      "Service and Product Agreement (MSPA), Server Cloud Enrollments (SCE), and so on. \n",
      "Along with this, each of them has its own portal; for example, EAs have the EA portal, \n",
      "CSP has the Partner Center portal, and Volume Licensing has its own portal too.\n",
      "Each offer comes with a different set of terms and conditions, and the customers need \n",
      "to go through them every time they make a purchase. The transition from one offer\n",
      "---------------\n",
      "to another is not very easy as each offer has a different set of terms and conditions. \n",
      "Let's imagine that you already have an EA subscription and would like to convert it \n",
      "to a CSP subscription; you may have to delete some of the partner services as they \n",
      "are not supported in CSP. For each product, each offer will have different rules. From \n",
      "a customer standpoint, it's very hard to understand what supports what and how \n",
      "rules differ.\n",
      "Addressing this issue, Microsoft has recently issued a new agreement called Microsoft \n",
      "Customer Agreement (MCA). This will act as the basic terms and conditions. You can \n",
      "make amendments to it whenever required when you sign up for a new program.\n",
      "---------------\n",
      "178 | Cost management for Azure solutions\n",
      "For Azure, there will be three Go-To-Market (GTM) programs:\n",
      "• Field Led: Customers will interact directly with the Microsoft Accounts Team and \n",
      "the billing will be directly managed by Microsoft. Eventually, this will replace EAs.\n",
      "• Partner Led: This is equivalent to the Azure-in-CSP program, where a partner \n",
      "manages your billing. There are different partners across the world. A quick web \n",
      "search will help you find the partners around you. This program will replace the \n",
      "Azure-in-CSP program. As the first step to Modern Commerce, a partner will sign \n",
      "a Microsoft Partner Agreement (MPA) with Microsoft and transition their existing \n",
      "customers by making them sign the MCA. At the time of writing this book, many \n",
      "partners have transitioned their customers to Modern Commerce, and the new \n",
      "commerce experience is available in 139 countries.\n",
      "• Self Service: This will be a replacement for Web Direct. It doesn't require any\n",
      "---------------\n",
      "involvement from the partner or the Microsoft Accounts Team. Customers can \n",
      "directly purchase from microsoft.com and they will sign the MCA during the \n",
      "purchase.\n",
      "In Azure, the billing will be done on the Azure Plan, and the billing will be always aligned \n",
      "with the calendar month. Buying an Azure Plan is very similar to buying any other \n",
      "subscription. The difference is that the MCA will be signed during the process.\n",
      "Azure Plan can host multiple subscriptions, and it will act as a root-level container. All \n",
      "the usage is tied back to a single Azure Plan. All the subscriptions inside the Azure Plan \n",
      "will act as containers to host services, such as Virtual Machines, SQL Database, and \n",
      "Networking.\n",
      "Some of the changes and advancements we could observe after the introduction of \n",
      "Modern Commerce are as follows:\n",
      "• Eventually, the portals will be deprecated. For example, earlier EA customers were \n",
      "only able to download the enrollment usage information from the EA portal. Now\n",
      "---------------\n",
      "Microsoft has integrated it into Azure Cost Management with a richer experience \n",
      "than the EA portal.\n",
      "• Pricing will be done in USD and billed in the local currency. If your currency is not \n",
      "USD, then the foreign exchange (FX) rate will be applied and is available in your \n",
      "invoice. Microsoft uses FX rates from Thomson Reuters, and these rates will be \n",
      "assigned on the first of every month. This value will be consistent throughout the \n",
      "month, irrespective of what the market rate is.\n",
      "• CSP customers who transition to the new Azure Plan will be able to use Cost \n",
      "Management. Access to Cost Management opens a new world of cost tracking, as \n",
      "it provides access to all native Cost Management features.\n",
      "---------------\n",
      "Usage and quotas | 179\n",
      "All the subscriptions that we have discussed so far will eventually be moved to an Azure \n",
      "Plan, which is the future of Azure. Now that you understand the basics of Modern \n",
      "Commerce, let's discuss another topic that has a very important role when we are \n",
      "architecting solutions. Most services have limits by default; some of these limits can be \n",
      "increased while some are hard limits. When we are architecting a solution, we need to \n",
      "make sure that there is ample quota. Capacity planning is a vital part of architectural \n",
      "design. In the next section, you will learn more about limits on subscriptions.\n",
      "Usage and quotas\n",
      "As mentioned in the previous section, capacity planning needs to be one of the first \n",
      "steps when we architect a solution. We need to verify whether the subscription has \n",
      "enough quota to accommodate the new resources we are architecting. If not, during \n",
      "the deployment, we may face issues.\n",
      "---------------\n",
      "Each subscription has a limited quota for each resource type. For example, there \n",
      "could be a maximum of 10 public IP addresses provisioned with an MSDN Microsoft \n",
      "account. Similarly, all resources have a maximum default limit for each resource type. \n",
      "These resource type numbers for a subscription can be increased by contacting Azure \n",
      "Support or clicking on the Request Increase button in the Usage + Quota blade on the \n",
      "Subscription page.\n",
      "Considering the number of resources in each region, it'll be a challenge to go through \n",
      "the list. The portal provides options to filter the dataset and look for what we want. In \n",
      "Figure 6.9, you can see that if we filter the location to Central US and set the resource \n",
      "provider to Microsoft.Storage, we can confirm which quotas are available for storage \n",
      "accounts:\n",
      " \n",
      "Figure 6.9: Usage and quota for a given location and resource provider\n",
      "---------------\n",
      "180 | Cost management for Azure solutions\n",
      "You can clearly see in Figure 6.9 that we haven't created any storage accounts in Central \n",
      "US, and that leaves us with a quota of 250 accounts. If the solution we are architecting \n",
      "requires more than 250 accounts, we need to click on Request Increase, which would \n",
      "contact Azure Support.\n",
      "This blade gives us the freedom to perform capacity planning prior to deployment.\n",
      "When filtering the report, we used the term resource provider and selected Microsoft.\n",
      "Storage. In the next section, will take a closer look at what this term means.\n",
      "Resource providers and resource types\n",
      "Whether you are interacting with the Azure portal, filtering services, or filtering the \n",
      "billing usage report, you might need to work with resource providers and resource \n",
      "types. For example, when you are creating a virtual machine, you are interacting with \n",
      "the Microsoft.Compute resource provider and the virtualMachines resource type. The\n",
      "---------------\n",
      "create button that you click on to create the virtual machines communicates with the \n",
      "resource provider via an API to get your deployment done. This is always denoted in \n",
      "the format {resource-provider}/{resource-type}. So, the resource type for the virtual \n",
      "machine is Microsoft.Compute/virtualMachines. In short, resource providers help to \n",
      "create resource types.\n",
      "Resource providers need to be registered with an Azure subscription. Resource types \n",
      "will not be available in a subscription if resource providers are not registered. By \n",
      "default, most providers are automatically registered; having said that, there will be \n",
      "scenarios where we must manually register.\n",
      "To get a list of providers that are available, the ones that are registered and the ones \n",
      "that are not registered, and to register non-registered providers or vice versa, the \n",
      "dashboard shown in Figure 6.10 can be used. For this operation, you need to have the\n",
      "---------------\n",
      "necessary roles assigned—Owner or Contributor roles will suffice. Figure 6.10 shows \n",
      "what the dashboard looks like:\n",
      "---------------\n",
      "Resource providers and resource types | 181\n",
      "Figure 6.10: List of registered and non-registered resource providers\n",
      "In the previous section, we discussed how to download invoices and usage information. \n",
      "If you need to download the data programmatically and save it, then you can use APIs. \n",
      "The next section is all about Azure Billing APIs.\n",
      "---------------\n",
      "182 | Cost management for Azure solutions\n",
      "Usage and Billing APIs\n",
      "Although the portal is a great way to find usage, billing, and invoice information \n",
      "manually, Azure also provides the following APIs to programmatically retrieve details \n",
      "and create customized dashboards and reports. The APIs vary depending on the kind \n",
      "of subscription you are using. As there are many APIs, we will be sharing the Microsoft \n",
      "documentation with each API so that you can explore them all.\n",
      "Azure Enterprise Billing APIs\n",
      "EA customers have a dedicated set of APIs available for them to work with billing data. \n",
      "The following APIs use the API key from the EA portal for authentication; tokens from \n",
      "Azure Active Directory will not work with them:\n",
      "• Balance and Summary API: As we discussed earlier, EA works with a monetary \n",
      "commitment, and it is very important to track the balance, overage, credit \n",
      "adjustments, and Azure Marketplace charges. Using this API, customers can pull\n",
      "---------------\n",
      "the balance and summary for a billing period.\n",
      "• Usage Details API: The Usage Details API will help you to get daily usage \n",
      "information about the enrollment with granularity down to the instance level. The \n",
      "response of this API will be like the usage report that can be downloaded from the \n",
      "EA portal.\n",
      "• Marketplace Store Charge API: This is a dedicated API for extracting the charges \n",
      "for Marketplace purchases.\n",
      "• Price Sheet API: Each enrollment will have a special price sheet, and discounts \n",
      "vary from customers to customers. The Price Sheet API can pull the price list.\n",
      "• Reserved Instance Details API: We haven't discussed Azure Reservations so far, \n",
      "but it will be discussed by the end of this chapter. Using this API, you can get \n",
      "usage information about the reservations and a list of the reservations in the \n",
      "enrollment.\n",
      "Here is the link to the documentation for the EA APIs: https:/ /docs.microsoft.com/\n",
      "azure/cost-management-billing/manage/enterprise-api.\n",
      "---------------\n",
      "Let's take a look at the Azure Consumption APIs now.\n",
      "---------------\n",
      "Usage and Billing APIs | 183\n",
      "Azure Consumption APIs\n",
      "Azure Consumption APIs can be used with EA as well as Web Direct (with some \n",
      "exceptions) subscriptions. This requires a token, which needs to be generated by \n",
      "authenticating against Azure Active Directory. Since these APIs also support EA, don't \n",
      "confuse this token with the EA API key that we mentioned in the previous section. Here \n",
      "are the key APIs that are self-explanatory:\n",
      "• Usage Details API\n",
      "• Marketplace Charges API\n",
      "• Reservation Recommendations API\n",
      "• Reservation Details and Summary API\n",
      "EA customers have additional support for the following APIs:\n",
      "• Price sheet\n",
      "• Budgets\n",
      "• Balances\n",
      "Documentation is available here: https:/ /docs.microsoft.com/azure/cost-management-\n",
      "billing/manage/consumption-api-overview.\n",
      "Additionally, there is another set of APIs that can be only used by Web Direct customers:\n",
      "• Azure Resource Usage API: This API can be used with an EA or pay-as-you-go \n",
      "subscription to download the usage data.\n",
      "---------------\n",
      "• Azure Resource RateCard API: This is only applicable to Web Direct; EAs are not \n",
      "supported. Web Direct customers can use this to download price sheets.\n",
      "• Azure Invoice Download API: This is only applicable to Web Direct customers. It's \n",
      "used to download invoices programmatically.\n",
      "The names may look familiar, and the difference is only the endpoint that we are calling. \n",
      "For Azure Enterprise Billing APIs, the URL will start with https://consumption.azure.com, \n",
      "and for Azure Consumption APIs, the URL starts with https://management.azure.com.  \n",
      "This is how you can differentiate them. In the next section, you will be seeing a new set \n",
      "of APIs that are specifically used by Cost Management.\n",
      "---------------\n",
      "184 | Cost management for Azure solutions\n",
      "Azure Cost Management APIs\n",
      "With the introduction of Azure Cost Management, a new set of APIs are available for the \n",
      "customer to use. These APIs are the backbone of Cost Management, which we used in \n",
      "the Azure portal earlier. The key APIs are as follows:\n",
      "• Query Usage API: This is the same API used by Cost Analysis in the Azure portal. \n",
      "We can customize the response with what we need using a payload. It's very useful \n",
      "when we want a customized report. The date range cannot exceed 365 days.\n",
      "• Budgets API: Budgets is another feature of Azure Cost Management, and this API \n",
      "lets us interact with the budgets programmatically.\n",
      "• Forecast API: This can be used to get a forecast of a scope. The Forecast API is \n",
      "only available for EA customers now.\n",
      "• Dimensions API: Earlier, when we were discussing Cost Management, we said that \n",
      "Cost Management supports multiple dimensions based on the scope. If you would\n",
      "---------------\n",
      "like to get the list of dimensions supported based on a specific scope, you can use \n",
      "this API.\n",
      "• Exports API: Another feature of Cost Management is that we can automate the \n",
      "report export to a storage account. The Exports API can be used to interact with \n",
      "the export configuration, such as the name of the storage account, customization, \n",
      "frequency, and so on.\n",
      "Check out the official documentation at https:/ /docs.microsoft.com/rest/api/cost-\n",
      "management.\n",
      "Since Modern Commerce is expanding MCA, there's a whole new set of APIs that can be \n",
      "explored here: https:/ /docs.microsoft.com/rest/api/billing.\n",
      "You may have noticed that we haven't mentioned CSP in any of these scenarios. In CSP, \n",
      "the customer doesn't have access to the billing as it's managed by a partner, hence \n",
      "the APIs are not exposed. However, a transition to an Azure Plan would let the CSP \n",
      "customer use the Azure Cost Management APIs to see the retail rates.\n",
      "---------------\n",
      "Any programming or scripting language can be used to use these APIs and mix them \n",
      "together to create complete comprehensive billing solutions. In the next section, \n",
      "we will be focusing on the Azure pricing calculator, which will help the customer or \n",
      "architect to understand the cost of a deployment.\n",
      "Azure pricing calculator\n",
      "Azure provides a cost calculator for users and customers to estimate their cost and \n",
      "usage. This calculator is available at https:/ /azure.microsoft.com/pricing/calculator:\n",
      "---------------\n",
      "Azure pricing calculator | 185\n",
      "Figure 6.11: Azure pricing calculator\n",
      "Users can select multiple resources from the left menu, and they will be added to the \n",
      "calculator. In the following example, a virtual machine is added. Further configuration \n",
      "with regard to virtual machine region, operating system, type, tier, instance size, \n",
      "number of hours, and count should be provided to calculate costs:\n",
      "Figure 6.12: Providing configuration details to calculate resource costs\n",
      "---------------\n",
      "186 | Cost management for Azure solutions\n",
      "Similarly, the cost for Azure Functions, which relates to virtual machine memory size, \n",
      "execution time, and executions per seconds, is shown in Figure 6.13:\n",
      "Figure 6.13: Calculating costs for Azure Functions\n",
      " Azure provides different levels and support plans:\n",
      "• Default support: free\n",
      "• Developer support: $29 per month\n",
      "• Standard support: $300 per month\n",
      "• Profession direct: $1,000 per month\n",
      "To see a complete comparison of the support plans, please refer to https:/ /azure.\n",
      "microsoft.com/support/plans.\n",
      "You can select the support that you want depending on your requirements. Finally, the \n",
      "overall estimated cost is displayed:\n",
      "---------------\n",
      "Best practices | 187\n",
      "Figure 6.14: Cost estimation for selected support plan\n",
      "It is important that architects understand every Azure feature used in their architecture \n",
      "and solution. The success of the Azure calculator depends on the resources that are \n",
      "selected and how they are configured. Any misrepresentation would lead to biased and \n",
      "incorrect estimates and would be different than the actual billing.\n",
      "We have reached the last section of this chapter. We have covered the basics of billing, \n",
      "and now it's time to learn the best practices. Following the best practices will help you \n",
      "achieve cost optimization.\n",
      "Best practices\n",
      "Architects need to understand their architecture and the Azure components that are \n",
      "utilized. Based on the active monitoring, audits, and usage, they should determine the \n",
      "best offering from Microsoft in terms of SKU, size, and features. This section will detail \n",
      "some of the best practices to be adopted from a cost optimization perspective.\n",
      "Azure Governance\n",
      "---------------\n",
      "Azure Governance can be defined as a set of processes or mechanisms that can be \n",
      "leveraged to maintain complete control over the resources deployed in Azure. Some of \n",
      "the key points are as follows:\n",
      "• Set up a naming convention for all resource types and resource groups. Ensure \n",
      "that the naming convention is followed consistently and comprehensively across \n",
      "all resources and resource groups. This can be done by establishing Azure policies.\n",
      "---------------\n",
      "188 | Cost management for Azure solutions\n",
      "• Set up a logical organization and multiple taxonomies for resources, resource \n",
      "groups, and subscriptions by applying tags to them. Tags categorize resources \n",
      "and can also help evaluate costs from different perspectives. This can be done by \n",
      "establishing Azure policies, and multiple policies can be combined into initiatives. \n",
      "These initiatives can be applied, which in turn will apply all policies for compliance \n",
      "checks and reporting.\n",
      "• Use Azure Blueprints instead of ARM templates directly. This will ensure that \n",
      "the deployment of new environments, resources, and resource groups can be \n",
      "standardized according to corporate standards, including naming conventions and \n",
      "the use of tags.\n",
      "Compute best practices\n",
      "Compute refers to services that help in the execution of services. Some of the best \n",
      "compute practices that Azure architects should follow for optimal utilization of \n",
      "resources and cost efficiency are as follows:\n",
      "---------------\n",
      "• Leverage Azure Advisor to see the available options to save costs on your virtual \n",
      "machines and to find out whether a virtual machine is underutilized. Advisor uses \n",
      "machine learning patterns and artificial intelligence to analyze your usage and \n",
      "provide recommendations. These recommendations play an important role in cost \n",
      "optimization.\n",
      "• Use Azure Reserved Instances (RI). RIs can get you potential savings on compute \n",
      "costs by paying the cost of the virtual machine upfront or monthly. The term \n",
      "of the RI can be a year or three years. If you buy an RI, that will cut down the \n",
      "compute cost and you will only be seeing the disk, network, and license (if any) \n",
      "charges for a virtual machine. If you have five virtual machines, you can opt for five \n",
      "RIs to suppress the cost of compute completely. RIs automatically look for virtual \n",
      "machines with the matching SKU and attaches to it. Potential savings may vary \n",
      "from 20% to 40% depending on the size of the virtual machine.\n",
      "---------------\n",
      "• Using Azure Hybrid Benefit (AHUB), you can use your own Windows Server or \n",
      "SQL licenses to reduce the license cost. Combining RIs and AHUB can give you \n",
      "immense savings.\n",
      "• Choose the best location for your compute services, such as virtual machines. \n",
      "Choose a location where all the Azure features are together in the same region. \n",
      "This will avoid egress traffic.\n",
      "• Choose the optimal size for virtual machines. A large virtual machine costs more \n",
      "than a small one, and a large virtual machine might not be required at all.\n",
      "---------------\n",
      "Best practices | 189\n",
      "• Resize virtual machines during times of demand and reduce their size when \n",
      "demand subsides. Azure releases new SKUs quite frequently. If a new size suits \n",
      "your needs better, then it must be used.\n",
      "• Shut down compute services during off-hours or when they are not needed. This \n",
      "is for non-production environments.\n",
      "• Deallocate virtual machines instead of shutting them down. This will release all \n",
      "their resources, and the meter for their consumption will stop.\n",
      "• Use dev/test labs for development and testing purposes. They provide policies, \n",
      "auto-shutdown, and auto-start features.\n",
      "• With virtual machine scale sets, start with a small number of virtual machines and \n",
      "scale out when demand increases.\n",
      "• Choose the correct size (small, medium, or large) for application gateways. They \n",
      "are backed up by virtual machines and can help reduce costs.\n",
      "• Choose a Basic tier application gateway if the web application firewall is not \n",
      "needed.\n",
      "---------------\n",
      "• Choose the correct tiers for VPN gateways (Basic VPN, Standard, High \n",
      "performance, and Ultra performance).\n",
      "• Reduce network traffic between Azure regions.\n",
      "• Use a load balancer with a public IP to access multiple virtual machines rather \n",
      "than assigning a public IP to each virtual machine.\n",
      "• Monitor virtual machines and calculate performance and usage metrics. Based \n",
      "on those calculations, determine whether you want to upscale or downscale your \n",
      "virtual machines. This could result in downsizing and reducing the number of \n",
      "virtual machines.\n",
      "Considering these best practices while architecting will inevitably lead to cost savings. \n",
      "Now that we have covered compute, let's take a similar approach to storage.\n",
      "Storage best practices\n",
      "Since we are hosting our applications in the cloud, Azure Storage will be used to store \n",
      "the data related to these applications. If we don't follow the right practices, things may \n",
      "go wrong. Some of the best practices for storage are as follows:\n",
      "---------------\n",
      "• Choose an appropriate storage redundancy type (GRS, LRS, RA-GRS). GRS is \n",
      "costlier than LRS.\n",
      "---------------\n",
      "190 | Cost management for Azure solutions\n",
      "• Archive storage data to the cool or archive access tier. Keep data that is frequently \n",
      "accessed in the hot tier.\n",
      "• Remove blobs that are not required.\n",
      "• Delete virtual machine operating system disks explicitly after deleting virtual \n",
      "machines if they are not needed.\n",
      "• Storage accounts should be metered based on their size, write, read, list, and \n",
      "container operations. \n",
      "• Prefer standard disks over premium disks; use premium disks only if your business \n",
      "requires it.\n",
      "• Using CDN and caching for static files instead of fetching them from storage every \n",
      "time.\n",
      "• Azure offers reserved capacity to save costs on blob data.\n",
      "Keeping these best practices handy will help you to architect cost-effective storage \n",
      "solutions. In the next section, we are going to discuss the best practices involved in the \n",
      "deployment of PaaS services.\n",
      "PaaS best practices\n",
      "There are many PaaS services offered by Azure and if they are misconfigured, the\n",
      "---------------\n",
      "chances are that you might end up with unexpected charges in the invoice. To avoid \n",
      "this scenario, you can leverage the following best practices:\n",
      "• Choose an appropriate Azure SQL tier (Basic, Standard, Premium RS, or Premium) \n",
      "and appropriate performance levels.\n",
      "• Choose appropriately between a single database and an elastic database. If there \n",
      "are a lot of databases, it is more cost efficient to use elastic databases than single \n",
      "databases.\n",
      "• Re-architect your solution to use PaaS (serverless or microservices with \n",
      "containers) solutions instead of IaaS solutions. These PaaS solutions take away \n",
      "maintenance costs and are available on a per-minute consumption basis. If you do \n",
      "not consume these services, there is no cost, despite your code and services being \n",
      "available around the clock.\n",
      "There are resource-specific cost optimizations, and it is not possible to cover all of \n",
      "them in a single chapter. You are advised to read the documentation related to each \n",
      "feature's cost and usage.\n",
      "---------------\n",
      "Summary | 191\n",
      "General best practices\n",
      "So far, we have looked at service-specific best practices, and we will wind up this \n",
      "section with some general guidelines:\n",
      "• The cost of resources is different across regions. Try an alternate region, provided \n",
      "it's not creating any performance or latency issues.\n",
      "• EAs offer better discounts than other offers. You can speak to the Microsoft \n",
      "Account Team and see what benefits you can get if you sign up to an EA.\n",
      "• If Azure costs can be pre-paid, then you get discounts for all kinds of \n",
      "subscriptions.\n",
      "• Delete or remove unused resources. Figure out resources that are underutilized \n",
      "and reduce their SKU or size. If they aren't needed, delete them.\n",
      "• Use Azure Advisor and take its recommendations seriously.\n",
      "As mentioned earlier, these are some generic guidelines and as you architect more \n",
      "solutions, you'll be able to create a set of best practices for yourself. But to begin with,\n",
      "---------------\n",
      "you can consider these ones. Nevertheless, each component in Azure has its own best \n",
      "practices, and referring to the documentation while you are architecting will help you \n",
      "to create a cost-effective solution.\n",
      "Summary\n",
      "In this chapter, we learned the importance of cost management and administration \n",
      "when working in a cloud environment. We also covered the various Azure pricing \n",
      "options and various price optimization capabilities that Azure offers. Managing a \n",
      "project's cost is paramount primarily because the monthly expense could be very low \n",
      "but could rise if the resources are not monitored periodically. Cloud architects should \n",
      "design their applications in a cost-effective manner. They should use appropriate Azure \n",
      "resources, appropriate SKUs, tiers, and sizes, and know when to start, stop, scale up, \n",
      "scale out, scale down, scale in, transfer data, and more. Proper cost management will \n",
      "ensure that the actual expense meets the budgetary expenses.\n",
      "---------------\n",
      "In the next chapter, we will look at various Azure features related to data services, such \n",
      "as Azure SQL, Cosmos DB, and sharding.\n",
      "---------------\n",
      "Azure provides both Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) \n",
      "services. These types of services provide organizations with different levels \n",
      "and controls over storage, compute, and networks. Storage is the resource used when \n",
      "working with the storage and transmission of data. Azure provides lots of options \n",
      "for storing data, such as Azure Blob storage, Table storage, Cosmos DB, Azure SQL \n",
      "Database, Azure Data Lake Storage, and more. While some of these options are meant \n",
      "for big data storage, analytics, and presentation, there are others that are meant for \n",
      "applications that process transactions. Azure SQL is the primary resource in Azure that \n",
      "works with transactional data.\n",
      "Azure OLTP solutions\n",
      "7\n",
      "---------------\n",
      "194 | Azure OLTP solutions\n",
      "This chapter will focus on various aspects of using transactional data stores, such as \n",
      "Azure SQL Database and other open-source databases that are typically used in Online \n",
      "Transaction Processing (OLTP) systems, and will cover the following topics:\n",
      "• OLTP applications\n",
      "• Relational databases\n",
      "• Deployment models\n",
      "• Azure SQL Database\n",
      "• Single Instance\n",
      "• Elastic pools\n",
      "• Managed Instance\n",
      "• Cosmos DB\n",
      "We will start this chapter by looking at what OLTP applications are and listing the OLTP \n",
      "services of Azure and their use cases.\n",
      "OLTP applications\n",
      "As mentioned earlier, OLTP applications are applications that help in the processing and \n",
      "management of transactions. Some of the most prevalent OLTP implementations can be \n",
      "found in retail sales, financial transaction systems, and order entry. These applications \n",
      "perform data capture, data processing, data retrieval, data modification, and data\n",
      "---------------\n",
      "storage. However, it does not stop here. OLTP applications treat these data tasks as \n",
      "transactions. Transactions have a few important properties and OLTP applications \n",
      "account for these properties. These properties are grouped under the acronym ACID. \n",
      "Let's discuss these properties in detail:\n",
      "• Atomicity: This property states that a transaction must consist of statements \n",
      "and either all statements should complete successfully or no statement should be \n",
      "executed. If multiple statements are grouped together, these statements form a \n",
      "transaction. Atomicity means each transaction is treated as the lowest single unit \n",
      "of execution that either completes successfully or fails.\n",
      "• Consistency: This property focuses on the state of data in a database. It \n",
      "dictates that any change in state should be complete and based on the rules and \n",
      "constraints of the database, and that partial updates should not be allowed.\n",
      "---------------\n",
      "Azure cloud services | 195\n",
      "• Isolation: This property states that there can be multiple concurrent transactions \n",
      "executed on a system and each transaction should be treated in isolation. One \n",
      "transaction should not know about or interfere with any other transaction. If the \n",
      "transactions were to be executed in sequence, by the end, the state of data should \n",
      "be the same as before.\n",
      "• Durability: This property states that the data should be persisted and available, \n",
      "even after failure, once it is committed to the database. A committed transaction \n",
      "becomes a fact.\n",
      "Now that you know what OLTP applications are, let's discuss the role of relational \n",
      "databases in OLTP applications.\n",
      "Relational databases\n",
      "OLTP applications have generally relied on relational databases for their transaction \n",
      "management and processing. Relational databases typically come in a tabular format \n",
      "consisting of rows and columns. The data model is converted into multiple tables where\n",
      "---------------\n",
      "each table is connected to another table (based on rules) using relationships. This \n",
      "process is also known as normalization.\n",
      "There are multiple services in Azure that support OLTP applications and the \n",
      "deployment of relational databases. In the next section, we will take a look at the \n",
      "services in Azure that are related to OLTP applications.\n",
      "Azure cloud services\n",
      "A search for sql in the Azure portal provides multiple results. I have marked some of \n",
      "them to show the resources that can be used directly for OLTP applications:\n",
      "Figure 7.1: List of Azure SQL services\n",
      "---------------\n",
      "196 | Azure OLTP solutions\n",
      "Figure 7.1 shows the varied features and options available for creating SQL Server–based \n",
      "databases on Azure.\n",
      "Again, a quick search for database in the Azure portal provides multiple resources, and \n",
      "the marked ones in Figure 7.2 can be used for OLTP applications:\n",
      "Figure 7.2: List of Azure services used for OLTP applications\n",
      "Figure 7.2 shows resources provided by Azure that can host data in a variety of \n",
      "databases, including the following:\n",
      "• MySQL databases\n",
      "• MariaDB databases\n",
      "• PostgreSQL databases\n",
      "• Cosmos DB\n",
      "Next, let's discuss deployment models. \n",
      "Deployment models\n",
      "Deployment models in Azure are classified based on the level of management or \n",
      "control. It's up to the user to select which level of management or control they prefer; \n",
      "either they can go for complete control by using services such as Virtual Machines, or \n",
      "they can use managed services where things will be managed by Azure for them.\n",
      "---------------\n",
      "There are two deployment models for deploying databases on Azure:\n",
      "• Databases on Azure Virtual Machines (IaaS)\n",
      "• Databases hosted as managed services (PaaS)\n",
      "---------------\n",
      "Deployment models | 197\n",
      "We will now try to understand the difference between deployment on Azure Virtual \n",
      "Machines and managed instances. Let's start with Virtual Machines.\n",
      "Databases on Azure Virtual Machines\n",
      "Azure provides multiple stock keeping units (SKUs) for virtual machines. There are \n",
      "high-compute, high-throughput (IOPS) machines that are also available along with \n",
      "general-use virtual machines. Instead of hosting a SQL Server, MySQL, or any other \n",
      "database on on-premises servers, it is possible to deploy these databases on these \n",
      "virtual machines. The deployment and configuration of these databases are no different \n",
      "than that of on-premises deployments. The only difference is that the database is \n",
      "hosted on the cloud instead of using on-premises servers. Administrators must perform \n",
      "the same activities and steps that they normally would for an on-premises deployment. \n",
      "Although this option is great when customers want full control over their deployment,\n",
      "---------------\n",
      "there are models that can be more cost-effective, scalable, and highly available \n",
      "compared to this option, which will be discussed later in this chapter.\n",
      "The steps to deploy any database on Azure Virtual Machines are as follows:\n",
      "1. Create a virtual machine with a size that caters to the performance requirements \n",
      "of the application.\n",
      "2. Deploy the database on top of it.\n",
      "3. Configure the virtual machine and database configuration.\n",
      "This option does not provide any out-of-the-box high availability unless multiple \n",
      "servers are provisioned. It also does not provide any features for automatic scaling \n",
      "unless custom automation supports it.\n",
      "Disaster recovery is also the responsibility of the customer. Servers should be \n",
      "deployed on multiple regions connected using services like global peering, VPN \n",
      "gateways, ExpressRoute, or Virtual WAN. It is possible for these virtual machines to be \n",
      "connected to an on-premises datacenter through site-to-site VPNs or ExpressRoute\n",
      "---------------\n",
      "without having any exposure to the outside world.\n",
      "These databases are also known as unmanaged databases. On the other hand, \n",
      "databases hosted with Azure, other than virtual machines, are managed by Azure and \n",
      "are known as managed services. In the next section, we will cover these in detail.\n",
      "---------------\n",
      "198 | Azure OLTP solutions\n",
      "Databases hosted as managed services\n",
      "Managed services mean that Azure provides management services for the databases. \n",
      "These managed services include the hosting of the database, ensuring that the \n",
      "host is highly available, ensuring that the data is replicated internally for availability \n",
      "during disaster recovery, ensuring scalability within the constraint of a chosen SKU, \n",
      "monitoring the hosts and databases and generating alerts for notifications or executing \n",
      "actions, providing log and auditing services for troubleshooting, and taking care of \n",
      "performance management and security alerts.\n",
      "In short, there are a lot of services that customers get out of the box when using \n",
      "managed services from Azure, and they do not need to perform active management \n",
      "on these databases. In this chapter, we will look at Azure SQL Database in depth and \n",
      "provide information on other databases, such as MySQL and Postgres. Also, we will\n",
      "---------------\n",
      "cover non-relational databases such as Cosmos DB, which is a NoSQL database.\n",
      "Azure SQL Database\n",
      "Azure SQL Server provides a relational database hosted as a PaaS. Customers can \n",
      "provision this service, bring their own database schema and data, and connect their \n",
      "applications to it. It provides all the features of SQL Server when deployed on a virtual \n",
      "machine. These services do not provide a user interface to create tables and its schema, \n",
      "nor do they provide any querying capabilities directly. SQL Server Management Studio \n",
      "and the SQL CLI tools should be used to connect to these services and directly work \n",
      "with them. \n",
      "Azure SQL Database comes with three distinct deployment models:\n",
      "• Single Instance: In this deployment model, a single database is deployed on a \n",
      "logical server. This involves the creation of two resources on Azure: a SQL logical \n",
      "server and a SQL database.\n",
      "• Elastic pool: In this deployment mode, multiple databases are deployed on a\n",
      "---------------\n",
      "logical server. Again, this involves the creation of two resources on Azure: a SQL \n",
      "logical server and a SQL elastic database pool—this holds all the databases.\n",
      "• Managed Instance: This is a relatively new deployment model from the Azure \n",
      "SQL team. This deployment reflects a collection of databases on a logical server, \n",
      "providing complete control over the resources in terms of system databases. \n",
      "Generally, system databases are not visible in other deployment models, but they \n",
      "are available in the model. This model comes very close to the deployment of SQL \n",
      "Server on-premises:\n",
      "---------------\n",
      "Azure SQL Database | 199\n",
      "Figure 7.3: Azure SQL Database deployment models\n",
      "If you are wondering when to use what, you should look at a feature comparison \n",
      "between SQL Database and SQL Managed Instance. A complete feature comparison \n",
      "is available at https:/ /docs.microsoft.com/azure/azure-sql/database/features-\n",
      "comparison.\n",
      "Next, we will cover some of the features of SQL Database. Let's start with application \n",
      "features.\n",
      "Application features\n",
      "Azure SQL Database provides multiple application-specific features that cater to the \n",
      "different requirements of OLTP systems:\n",
      "• Columnar store: This feature allows the storage of data in a columnar format \n",
      "rather than in a row format.\n",
      "• In-memory OLTP: Generally, data is stored in back-end files in SQL, and data is \n",
      "pulled from them whenever it is needed by the application. In contrast to this, \n",
      "in-memory OLTP puts all data in memory and there is no latency in reading the \n",
      "storage for data. Storing in-memory OLTP data on SSD provides the best possible\n",
      "---------------\n",
      "performance for Azure SQL.\n",
      "• All features of on-premises SQL Server.\n",
      "The next feature we are going to discuss is high availability.\n",
      "High availability\n",
      "Azure SQL, by default, is 99.99% highly available. It has two different architectures for \n",
      "maintaining high availability based on SKUs. For the Basic, Standard, and General SKUs, \n",
      "the entire architecture is broken down into the following two layers.\n",
      "• Compute layer\n",
      "• Storage layer\n",
      "---------------\n",
      "200 | Azure OLTP solutions\n",
      "There is redundancy built in for both of these layers to provide high availability:\n",
      "Figure 7.4: Compute and storage layers in standard SKUs\n",
      "For the Premium and business-critical SKUs, both compute and storage are on the \n",
      "same layer. High availability is achieved by the replication of compute and storage \n",
      "deployed in a four-node cluster, using technology similar to SQL Server Always On \n",
      "availability groups:\n",
      "Figure 7.5: Four-node cluster deployment\n",
      "---------------\n",
      "Azure SQL Database | 201\n",
      "Now that you know how high availability is handled, let's jump to the next feature: \n",
      "backups.\n",
      "Backups\n",
      "Azure SQL Database also provides features to automatically back up databases and \n",
      "store them on storage accounts. This feature is important especially in cases where a \n",
      "database becomes corrupt or a user accidentally deletes a table. This feature is available \n",
      "at the server level, as shown in Figure 7.6:\n",
      "Figure 7.6: Backing up databases in Azure\n",
      "Architects should prepare a backup strategy so that backups can be used in times \n",
      "of need. While configuring backups, ensure that your backups occur neither too \n",
      "infrequently nor too frequently. Based on the business needs, a weekly backup or even a \n",
      "daily backup should be configured, or even more frequently than that, if required. These \n",
      "backups can be used for restoration purposes.\n",
      "Backups will help in business continuity and data recovery. You can also go for\n",
      "---------------\n",
      "geo-replication to recover the data during a region failure. In the next section, we will \n",
      "cover geo-replication.\n",
      "---------------\n",
      "202 | Azure OLTP solutions\n",
      "Geo-replication\n",
      "Azure SQL Database also provides the benefit of being able to replicate a database \n",
      "to a different region, also known as a secondary region; this is completely based on \n",
      "the plan that you are choosing. The database at the secondary region can be read by \n",
      "applications. Azure SQL Database allows readable secondary databases. This is a great \n",
      "business continuity solution as a readable database is available at any point in time. \n",
      "With geo-replication, it is possible to have up to four secondaries of a database in \n",
      "different regions or the same region. With geo-replication, it is also possible to fail over \n",
      "to a secondary database in the event of a disaster. Geo-replication is configured at the \n",
      "database level, as shown in Figure 7.7:\n",
      "Figure 7.7: Geo-replication in Azure\n",
      "---------------\n",
      "Azure SQL Database | 203\n",
      "If you scroll down on this screen, the regions that can act as secondaries are listed, as \n",
      "shown in Figure 7.8:\n",
      "Figure 7.8: List of available secondaries for geo-replication\n",
      "Before architecting solutions that involve geo-replication, we need to validate the data \n",
      "residency and compliance regulations. If customer data is not allowed to be stored \n",
      "outside a region due to compliance reasons, we shouldn't be replicating it to other \n",
      "regions.\n",
      "In the next section, we will explore scalability options.\n",
      "---------------\n",
      "204 | Azure OLTP solutions\n",
      "Scalability\n",
      "Azure SQL Database provides vertical scalability by adding more resources (such as \n",
      "compute, memory, and IOPS). This can be done by increasing the number of Database \n",
      "Throughput Units (DTUs) or compute and storage resources in the case of the vCore \n",
      "model:\n",
      "Figure 7.9: Scalability in Azure SQL Database\n",
      "We have covered the differences between DTU-based model and the vCore-based \n",
      "model later in this chapter.\n",
      "In the next section, we will cover security, which will help you understand how to build \n",
      "secure data solutions in Azure.\n",
      "Security\n",
      "Security is an important factor for any database solution and service. Azure SQL \n",
      "provides enterprise-grade security for Azure SQL, and this section will list some of the \n",
      "important security features in Azure SQL.\n",
      "Firewall\n",
      "Azure SQL Database, by default, does not provide access to any requests. Source IP \n",
      "addresses should be explicitly accepted for access to SQL Server. There is an option to\n",
      "---------------\n",
      "allow all Azure-based services access to a SQL database as well. This option includes \n",
      "virtual machines hosted on Azure.\n",
      "The firewall can be configured at the server level instead of the database level. \n",
      "The Allow access to Azure services option allows all services, including virtual \n",
      "machines, to access a database hosted on a logical server.\n",
      "---------------\n",
      "Azure SQL Database | 205\n",
      "By default, this will be turned off due to security reasons; enabling this would allow \n",
      "access from all Azure services:\n",
      "Figure 7.10: Configuring a firewall at the server level in Azure\n",
      "---------------\n",
      "206 | Azure OLTP solutions\n",
      "Azure SQL Server on dedicated networks\n",
      "Although access to SQL Server is generally available through the internet, it is possible \n",
      "for access to SQL Server to be limited to requests coming from virtual networks. This \n",
      "is a relatively new feature in Azure. This helps in accessing data within SQL Server \n",
      "from an application on another server of the virtual network without the request going \n",
      "through the internet.\n",
      "For this, a service endpoint of the Microsoft.Sql type should be added within the virtual \n",
      "network, and the virtual network should be in the same region as that of Azure SQL \n",
      "Database:\n",
      " \n",
      "Figure 7.11: Adding a Microsoft.Sql service endpoint\n",
      "---------------\n",
      "Azure SQL Database | 207\n",
      "An appropriate subnet within the virtual network should be chosen:\n",
      "Figure 7.12: Choosing a subnet for the Microsoft.Sql service\n",
      "---------------\n",
      "208 | Azure OLTP solutions\n",
      "Finally, from the Azure SQL Server configuration blade, an existing virtual \n",
      "network should be added that has a Microsoft.Sql service endpoint enabled:\n",
      "Figure 7.13: Adding a virtual network with the Microsoft.Sql service endpoint\n",
      "Encrypted databases at rest\n",
      "The databases should be in an encrypted form when at rest. At rest here means that the \n",
      "data is at the storage location of the database. Although you might not have access to \n",
      "SQL Server and its database, it is preferable to encrypt the database storage.\n",
      "Databases on a filesystem can be encrypted using keys. These keys must be stored \n",
      "in Azure Key Vault and the vault must be available in the same region as that of \n",
      "Azure SQL Server. The filesystem can be encrypted by using the Transparent \n",
      "data encryption menu item of the SQL Server configuration blade and by \n",
      "selecting Yes for Use your own key.\n",
      "---------------\n",
      "Azure SQL Database | 209\n",
      "The key is an RSA 2048 key and must exist within the vault. SQL Server will decrypt \n",
      "the data at the page level when it wants to read it and send it to the caller; then, it will \n",
      "encrypt it after writing to the database. No changes to the applications are required, \n",
      "and it is completely transparent to them:\n",
      "Figure 7.14: Transparent data encryption in SQL Server\n",
      "Dynamic data masking\n",
      "SQL Server also provides a feature that masks individual columns that contain sensitive \n",
      "data, so that no one apart from privileged users can view actual data by querying it in \n",
      "SQL Server Management Studio. Data will remain masked and will only be unmasked \n",
      "when an authorized application or user queries the table. Architects should ensure that \n",
      "sensitive data, such as credit card details, social security numbers, phone numbers, \n",
      "email addresses, and other financial details, is masked.\n",
      "Masking rules may be defined on a column in a table. There are four main types of\n",
      "---------------\n",
      "masks—you can check them out here: https:/ /docs.microsoft.com/sql/relational-\n",
      "databases/security/dynamic-data-masking?view=sql-server-ver15#defining-a-\n",
      "dynamic-data-mask.\n",
      "---------------\n",
      "210 | Azure OLTP solutions\n",
      "Figure 7.15 shows how data masking is added:\n",
      " \n",
      " Figure 7.15: Dynamic data masking in SQL Database\n",
      "Azure Active Directory integration\n",
      "Another important security feature of Azure SQL is that it can be integrated with \n",
      "Azure Active Directory (AD) for authentication purposes. Without integrating with \n",
      "Azure AD, the only authentication mechanism available to SQL Server is via username \n",
      "and password authentication—that is, SQL authentication. It is not possible to use \n",
      "integrated Windows authentication. The connection string for SQL authentication \n",
      "consists of both the username and password in plaintext, which is not secure. \n",
      "Integrating with Azure AD enables the authentication of applications with Windows \n",
      "authentication, a service principal name, or token-based authentication. It is a good \n",
      "practice to use Azure SQL Database integrated with Azure AD.\n",
      "There are other security features, such as advanced threat protection, auditing of the\n",
      "---------------\n",
      "environment, and monitoring, that should be enabled on any enterprise-level Azure \n",
      "SQL Database deployments. \n",
      "With that, we've concluded our look at the features of Azure SQL Database and can now \n",
      "move on to the types of SQL databases.\n",
      "Single Instance\n",
      "Single Instance databases are hosted as a single database on a single logical server. \n",
      "These databases do not have access to the complete features provided by SQL Server. \n",
      "Each database is isolated and portable. Single instances support the vCPU-based and \n",
      "DTU-based purchasing models that we discussed earlier.\n",
      "---------------\n",
      "Elastic pools | 211\n",
      "Another added advantage of a single database is cost-efficiency. If you are in a vCore-\n",
      "based model, you can opt for lower compute and storage resources to optimize costs. If \n",
      "you need more compute or storage power, you can always scale up. Dynamic scalability \n",
      "is a prominent feature of single instances that helps to scale resources dynamically \n",
      "based on business requirements. Single instances allow existing SQL Server customers \n",
      "to lift and shift their on-premises applications to the cloud.\n",
      "Other features include availability, monitoring, and security. \n",
      "When we started our section on Azure SQL Database, we mentioned elastic pools as \n",
      "well. You can also transition a single database to an elastic pool for resource sharing. \n",
      "If you are wondering what resource sharing and what elastic pools are, in the next \n",
      "section, we will cover this.\n",
      "Elastic pools\n",
      "An elastic pool is a logical container that can host multiple databases on a single logical\n",
      "---------------\n",
      "server. Elastic pools are available in the vCore-based and DTU-based purchasing \n",
      "models. The vCPU-based purchasing model is the default and recommended method \n",
      "of deployment, where you'll get the freedom to choose your compute and storage \n",
      "resources based on your business workloads. As shown in Figure 7.16, you can select \n",
      "how many cores and how much storage is required for your database:\n",
      "Figure 7.16: Setting up elastic pools in the vCore-based model\n",
      "---------------\n",
      "212 | Azure OLTP solutions\n",
      "Also, at the top of the preceding figure, you can see there is an option that says Looking \n",
      "for basic, standard, premium? If you select this, the model will be switched to the DTU \n",
      "model.\n",
      "The SKUs available for elastic pools in the DTU-based model are as follows:\n",
      "• Basic\n",
      "• Standard\n",
      "• Premium\n",
      "Figure 7.17 shows the maximum amounts of DTUs that can be provisioned for each SKU:\n",
      "Figure 7.17: Amount of DTUs per SKU in an elastic pool\n",
      "All the features discussed for Azure SQL single instances are available to elastic pools \n",
      "as well; however, horizontal scalability is an additional feature that enables sharding. \n",
      "Sharding refers to the vertical or horizontal partitioning of data and the storage of that \n",
      "data in separate databases. It is also possible to have autoscaling of individual databases \n",
      "in an elastic pool by consuming more DTUs than are actually allocated to that database.\n",
      "Elastic pools also provide another advantage in terms of cost. You will see in a later\n",
      "---------------\n",
      "section that Azure SQL Database is priced using DTUs, and DTUs are provisioned as \n",
      "soon as the SQL Server service is provisioned. DTUs are charged for irrespective of \n",
      "whether those DTUs are consumed. If there are multiple databases, then it is possible to \n",
      "put these databases into elastic pools and for them to share the DTUs among them.\n",
      "All information for implementing sharding with Azure SQL elastic pools has been \n",
      "provided at https:/ /docs.microsoft.com/azure/sql-database/sql-database-elastic-\n",
      "scale-introduction.\n",
      "---------------\n",
      "Managed Instance | 213\n",
      "Next, we will discuss the Managed Instance deployment option, which is a scalable, \n",
      "intelligent, cloud-based, fully managed database.\n",
      "Managed Instance\n",
      "Managed Instance is a unique service that provides a managed SQL server similar to \n",
      "what's available on on-premises servers. Users have access to master, model, and other \n",
      "system databases. Managed Instance is ideal when there are multiple databases and \n",
      "customers migrating their instances to Azure. Managed Instance consists of multiple \n",
      "databases.\n",
      "Azure SQL Database provides a new deployment model known as Azure SQL Database \n",
      "Managed Instance that provides almost 100% compatibility with the SQL Server \n",
      "Enterprise Edition Database Engine. This model provides a native virtual network \n",
      "implementation that addresses the usual security issues and is a highly recommended \n",
      "business model for on-premises SQL Server customers. Managed Instance allows\n",
      "---------------\n",
      "existing SQL Server customers to lift and shift their on-premises applications \n",
      "to the cloud with minimal application and database changes while preserving all \n",
      "PaaS capabilities at the same time. These PaaS capabilities drastically reduce the \n",
      "management overhead and total cost of ownership, as shown in Figure 7.18:\n",
      "Figure 7.18: Azure SQL Database Managed Instance\n",
      "The complete comparison between Azure SQL Database, Azure SQL Managed Instance, \n",
      "and SQL Server on an Azure virtual machine is available here: https:/ /docs.microsoft.\n",
      "com/azure/azure-sql/azure-sql-iaas-vs-paas-what-is-overview#comparison-table.\n",
      "---------------\n",
      "214 | Azure OLTP solutions\n",
      "The key features of Managed Instance are shown in the Figure 7.19:\n",
      "Figure 7.19: SQL Database Managed Instance features\n",
      "We have mentioned the terms vCPU-based pricing model and DTU-based pricing \n",
      "model at several points throughout the chapter. It's time that we took a closer look at \n",
      "these pricing models.\n",
      "---------------\n",
      "SQL database pricing | 215\n",
      "SQL database pricing\n",
      "Azure SQL previously had just one pricing model—a model based on DTUs—but an \n",
      "alternative pricing model based on vCPUs has also been launched. The pricing model is \n",
      "selected based on the customer's requirements. The DTU-based model is selected when \n",
      "the customer wants simple and preconfigured resource options. On the other hand, the \n",
      "vCore-based model offers the flexibility to choose compute and storage resources. It \n",
      "also provides control and transparency.\n",
      "Let's take a closer look at each of these models.\n",
      "DTU-based pricing\n",
      "The DTU is the smallest unit of performance measure for Azure SQL Database. Each \n",
      "DTU corresponds to a certain amount of resources. These resources include storage, \n",
      "CPU cycles, IOPS, and network bandwidth. For example, a single DTU might provide \n",
      "three IOPS, a few CPU cycles, and IO latencies of 5 ms for read operations and 10 ms for \n",
      "write operations.\n",
      "---------------\n",
      "Azure SQL Database provides multiple SKUs for creating databases, and each of these \n",
      "SKUs has defined constraints for the maximum amount of DTUs. For example, the Basic \n",
      "SKU provides just 5 DTUs with a maximum 2 GB of data, as shown in Figure 7.20:\n",
      "Figure 7.20: DTUs for different SKUs\n",
      "---------------\n",
      "216 | Azure OLTP solutions\n",
      "On the other hand, the standard SKU provides anything between 10 DTUs and 300 \n",
      "DTUs with a maximum of 250 GB of data. As you can see here, each DTU costs around \n",
      "991 rupees, or around $1.40: \n",
      "Figure 7.21: Cost summary for the selected number of DTUs in the Standard SKU\n",
      "A comparison of these SKUs in terms of performance and resources is provided by \n",
      "Microsoft and is shown in the Figure 7.22:\n",
      "Figure 7.22: SKU comparison in Azure\n",
      "---------------\n",
      "SQL database pricing | 217\n",
      "Once you provision a certain number of DTUs, the back-end resources (CPU, IOPS, and \n",
      "memory) are allocated and are charged for whether they are consumed or not. If more \n",
      "DTUs are procured than are actually needed, it leads to waste, while there would be \n",
      "performance bottlenecks if insufficient DTUs were provisioned.\n",
      "Azure provides elastic pools for this reason as well. As you know, there are multiple \n",
      "databases in an elastic pool and DTUs are assigned to elastic pools instead of individual \n",
      "databases. It is possible for all databases within a pool to share the DTUs. This means \n",
      "that if a database has low utilization and is consuming only five DTUs, there will be \n",
      "another database consuming 25 DTUs in order to compensate.\n",
      "It is important to note that, collectively, DTU consumption cannot exceed the amount \n",
      "of DTUs provisioned for the elastic pool. Moreover, there is a minimum amount of DTUs\n",
      "---------------\n",
      "that should be assigned to each database within the elastic pool, and this minimum DTU \n",
      "count is preallocated for the database.\n",
      "An elastic pool comes with its own SKUs:\n",
      "Figure 7.23: SKUs in an elastic pool\n",
      "Also, there is a limit on the maximum number of databases that can be created within \n",
      "a single elastic pool. The complete limits can be reviewed here: https:/ /docs.microsoft.\n",
      "com/azure/azure-sql/database/resource-limits-dtu-elastic-pools.\n",
      "vCPU-based pricing\n",
      "This is the new pricing model for Azure SQL. This pricing model provides options to \n",
      "procure the number of virtual CPUs (vCPUs) allocated to the server instead \n",
      "of setting the amount of DTUs required for an application. A vCPU is a logical CPU with \n",
      "attached hardware, such as storage, memory, and CPU cores.\n",
      "---------------\n",
      "218 | Azure OLTP solutions\n",
      "In this model, there are three SKUs: General Purpose, Hyperscale, and Business \n",
      "Critical, with a varied number of vCPUs and resources available. This pricing is available \n",
      "for all SQL deployment models:\n",
      "Figure 7.24: vCPU pricing for the General Purpose SKU\n",
      "How to choose the appropriate pricing model\n",
      "Architects should be able to choose an appropriate pricing model for Azure SQL \n",
      "Database. DTUs are a great mechanism for pricing where there is a usage pattern \n",
      "applicable and available for the database. Since resource availability in the DTU scheme \n",
      "of things is linear, as shown in the next diagram, it is quite possible for usage to be more \n",
      "memory-intensive than CPU-intensive. In such cases, it is possible to choose different \n",
      "levels of CPU, memory, and storage for a database. \n",
      "In DTUs, resources come packaged, and it is not possible to configure these resources \n",
      "at a granular level. With a vCPU model, it is possible to choose different levels of\n",
      "---------------\n",
      "memory and CPU for different databases. If the usage pattern for an application \n",
      "is known, using the vCPU pricing model could be a better option compared to the \n",
      "DTU model. In fact, the vCPU model also provides the benefit of hybrid licenses if an \n",
      "organization already has on-premises SQL Server licenses. There is a discount of up to \n",
      "30% provided to these SQL Server instances.\n",
      "---------------\n",
      "Azure Cosmos DB | 219\n",
      "In Figure 7.25, you can see from the left-hand graph that as the amount of DTUs \n",
      "increases, resource availability also grows linearly; however, with vCPU pricing (in \n",
      "the right-hand graph), it is possible to choose independent configurations for each \n",
      "database:\n",
      "Figure 7.25: Storage-compute graph for the DTU and vCore models\n",
      "With that, we can conclude our coverage of Azure SQL Database. We discussed \n",
      "different deployment methods, features, pricing, and plans related to Azure SQL \n",
      "Database. In the next section, we will be covering Cosmos DB, which is a NoSQL \n",
      "database service.\n",
      "Azure Cosmos DB\n",
      "Cosmos DB is Azure's truly cross-region, highly available, distributed, multi-model \n",
      "database service. Cosmos DB is for you if you would like your solution to be highly \n",
      "responsive and always available. As this is a cross-region multi-model database, we \n",
      "can deploy applications closer to the user's location and achieve low latency and high \n",
      "availability.\n",
      "---------------\n",
      "With the click of a button, throughput and storage can be scaled across any number \n",
      "of Azure regions. There are a few different database models to cover almost all \n",
      "non-relational database requirements, including:\n",
      "1. SQL (documents)\n",
      "2. MongoDB\n",
      "3. Cassandra\n",
      "4. Table\n",
      "5. Gremlin Graph\n",
      "---------------\n",
      "220 | Azure OLTP solutions\n",
      "The hierarchy of objects within Cosmos DB starts with the Cosmos DB account. An \n",
      "account can have multiple databases, and each database can have multiple containers. \n",
      "Depending on the type of database, the container might consist of documents, as in \n",
      "the case of SQL; semi-structured key-value data within Table storage; or entities and \n",
      "relationships among those entities, if using Gremlin and Cassandra to store NoSQL \n",
      "data.\n",
      "Cosmos DB can be used to store OLTP data. It accounts for ACID with regard to \n",
      "transaction data, with a few caveats.\n",
      "Cosmos DB provides for ACID requirements at the single document level. This means \n",
      "data within a document, when updated, deleted, or inserted, will have its atomicity, \n",
      "consistency, isolation, and durability maintained. However, beyond documents, \n",
      "consistency and atomicity have to be managed by the developer themselves.\n",
      "Pricing for Cosmos DB can be found here: https:/ /azure.microsoft.com/pricing/\n",
      "details/cosmos-db.\n",
      "---------------\n",
      "Figure 7.26 shows some features of Azure Cosmos DB:\n",
      "Figure 7.26: An overview of Azure Cosmos DB\n",
      "Azure Cosmos DB\n",
      "Global distribution Elastic scale-out Guaranteed low latency Five consistency models Comprehensive SLAs\n",
      "SQLTable JavaScript API for MongoDB\n",
      " Gremlin Cassandra Spark\n",
      " ETCD\n",
      "...more APIs\n",
      "coming\n",
      "Key-Value DocumentsColumn-Family Graph\n",
      "---------------\n",
      "Azure Cosmos DB | 221\n",
      "In the next section, we will cover some key features of Azure Cosmos DB.\n",
      "Features\n",
      "Some of the key benefits of Azure Cosmos DB are:\n",
      "• Global distribution: Highly responsive and highly available applications can be \n",
      "built worldwide using Azure Cosmos DB. With the help of replication, replicas of \n",
      "data can be stored in Azure regions that are close to users, hence providing less \n",
      "latency and global distribution. \n",
      "• Replication: You can opt in to or opt out of replication to a region any time you \n",
      "like. Let's say you have a replica of your data available in the East US region, and \n",
      "your organization is planning to shut down their processes in East US and migrate \n",
      "to UK South. With just a few clicks, East US can be removed, and UK South can be \n",
      "added to the account for replication.\n",
      "• Always On: Cosmos DB provides 99.999% of high availability for both read and \n",
      "write. The regional failover of a Cosmos DB account to another region can be\n",
      "---------------\n",
      "invoked via the Azure portal or programmatically. This ensures business continuity \n",
      "and disaster recovery planning for your application during a region failure.\n",
      "• Scalability: Cosmos DB offers unmatched elastic scalability for writes and reads \n",
      "all around the globe. The scalability response is massive, meaning that you can \n",
      "scale from thousands to hundreds of millions of requests/second with a single API \n",
      "call. The interesting thing is that this is done around the globe, but you need to \n",
      "pay only for throughput and storage. This level of scalability is ideal for handling \n",
      "unexpected spikes.\n",
      "• Low latency: As mentioned earlier, replicating copies of data to locations nearer \n",
      "to users drastically reduces latency; it means that users can access their data in \n",
      "milliseconds. Cosmos DB guarantees less than 10 ms of latency for both reads and \n",
      "writes all around the world.\n",
      "• TCO Savings: As Cosmos DB is a fully managed service, the level of management\n",
      "---------------\n",
      "required from the customer is low. Also, the customer doesn't have to set up \n",
      "datacenters across the globe to accommodate users from other regions.\n",
      "• SLA: It offers an SLA of 99.999% high availability.\n",
      "• Support for Open-Source Software (OSS) APIs: The support for OSS APIs \n",
      "is another added advantage of Cosmos DB. Cosmos DB implements APIs for \n",
      "Cassandra, Mongo DB, Gremlin, and Azure Table storage.\n",
      "---------------\n",
      "222 | Azure OLTP solutions\n",
      "Use case scenarios\n",
      "If your application involves high levels of data reads and writes at a global scale, then \n",
      "Cosmos DB is the ideal choice. The common types of applications that have such \n",
      "requirements include web, mobile, gaming, and Internet of Things applications. These \n",
      "applications would benefit from the high availability, low latency, and global presence of \n",
      "Cosmos DB.\n",
      "Also, the response time provided by Cosmos DB is near real time. The Cosmos DB \n",
      "SDKs can be leveraged to develop iOS and Android applications using the Xamarin \n",
      "framework.\n",
      "A couple of the popular games that use Cosmos DB are The Walking Dead: No Man's \n",
      "Land, by Next Games, and Halo 5: Guardians.\n",
      "A complete list of use case scenarios and examples can be found here: https:/ /docs.\n",
      "microsoft.com/azure/cosmos-db/use-cases.\n",
      "Cosmos DB is the go-to service in Azure for storing semi-structured data as part \n",
      "of OLTP applications. I could write a whole book on the features and capabilities of\n",
      "---------------\n",
      "Cosmos DB alone; the intention of this section was to give you an introduction to \n",
      "Cosmos DB and the role it plays in handling OLTP applications.\n",
      "Summary\n",
      "In this chapter, you learned that Azure SQL Database is one of the flagship services \n",
      "of Azure. A plethora of customers are using this service today and it provides all the \n",
      "enterprise capabilities that are needed for a mission-critical database management \n",
      "system. \n",
      "You discovered that there are multiple deployment types for Azure SQL Database, such \n",
      "as Single Instance, Managed Instance, and elastic pools. Architects should perform a \n",
      "complete assessment of their requirements and choose the appropriate deployment \n",
      "model. After they choose a deployment model, they should choose a pricing strategy \n",
      "between DTUs and vCPUs. They should also configure all the security, availability, \n",
      "disaster recovery, monitoring, performance, and scalability requirements in Azure SQL \n",
      "Database regarding data.\n",
      "---------------\n",
      "In the next chapter, we will be discussing how to build secure applications in Azure. We \n",
      "will cover the security practices and features of most services.\n",
      "---------------\n",
      "In the previous chapter, we discussed Azure data services. As we are dealing with \n",
      "sensitive data, security is a big concern. Security is, undoubtedly, the most important \n",
      "non-functional requirement for architects to implement. Enterprises put lots of \n",
      "emphasis on having their security strategy implemented correctly. In fact, security is \n",
      "one of the top concerns for almost every stakeholder in an application's development, \n",
      "deployment, and management. It becomes all the more important when an application \n",
      "is built for deployment to the cloud.\n",
      "Architecting secure \n",
      "applications on Azure\n",
      "8\n",
      "---------------\n",
      "226 | Architecting secure applications on Azure\n",
      "In order for you to understand how you can secure your applications on Azure \n",
      "depending upon the nature of the deployment, the following topics will be covered in \n",
      "this chapter:\n",
      "• Understanding security in Azure \n",
      "• Security at the infrastructure level\n",
      "• Security at the application level\n",
      "• Authentication and authorization in Azure applications\n",
      "• Working with OAuth, Azure Active Directory, and other authentication methods \n",
      "using federated identity, including third-party identity providers such as Facebook\n",
      "• Understanding managed identities and using them to access resources\n",
      "Security\n",
      "As mentioned before, security is an important element for any software or service. \n",
      "Adequate security should be implemented so that an application can only be used by \n",
      "people who are allowed to access it, and users should not be able to perform operations \n",
      "that they are not allowed to execute. Similarly, the entire request-response mechanism\n",
      "---------------\n",
      "should be built using methods that ensure that only intended parties can understand \n",
      "messages, and to make sure that it is easy to detect whether messages have been \n",
      "tampered with or not.\n",
      "For the following reasons, security in Azure is even more important. Firstly, the \n",
      "organizations deploying their applications are not in full control of the underlying \n",
      "hardware and networks. Secondly, security has to be built into every layer, including \n",
      "hardware, networks, operating systems, platforms, and applications. Any omissions \n",
      "or misconfigurations can render an application vulnerable to intruders. For example, \n",
      "you might have heard of the recent vulnerability that affected Zoom meetings that \n",
      "let hackers record meetings even when the meeting host had disabled recording for \n",
      "attendees. Sources claim that millions of Zoom accounts have been sold on the dark \n",
      "web. The company has taken the necessary action to address this vulnerability.\n",
      "---------------\n",
      "Security is a big concern nowadays, especially when hosting applications in the cloud, \n",
      "and can lead to dire consequences if mishandled. Hence, it's necessary to understand \n",
      "the best practices involved in securing your workloads. We are progressing in the area \n",
      "of DevOps, where development and operations teams collaborate effectively with the \n",
      "help of tools and practices, and security has been a big concern there as well.\n",
      "---------------\n",
      "Security | 227\n",
      "To accommodate security principles and practices as a vital part of DevOps without \n",
      "affecting the overall productivity and efficiency of the process, a new culture known as \n",
      "DevSecOps has been introduced. DevSecOps helps us to identify security issues early \n",
      "in the development stage rather than mitigating them after shipping. In a development \n",
      "process that has security as a key principle of every stage, DevSecOps reduces the cost \n",
      "of hiring security professionals at a later stage to find security flaws with software.\n",
      "Securing an application means that unknown and unauthorized entities are unable to \n",
      "access it. This also means that communication with the application is secure and not \n",
      "tampered with. This includes the following security measures:\n",
      "• Authentication: Authentication checks the identity of a user and ensures that the \n",
      "given identity can access the application or service. Authentication is performed in\n",
      "---------------\n",
      "Azure using OpenID Connect, which is an authentication protocol built on OAuth \n",
      "2.0.\n",
      "• Authorization: Authorization allows and establishes permissions that an identity \n",
      "can perform within the application or service. Authorization is performed in Azure \n",
      "using OAuth.\n",
      "• Confidentiality: Confidentiality ensures that communication between the user \n",
      "and the application remains secure. The payload exchange between entities is \n",
      "encrypted so that it will make sense only to the sender and the receiver, but \n",
      "not to others. The confidentiality of messages is ensured using symmetric and \n",
      "asymmetric encryption. Certificates are used to implement cryptography—that is, \n",
      "the encryption and decryption of messages.\n",
      "Symmetric encryption uses a single key, which is shared with the sender and \n",
      "the receiver, while asymmetric encryption uses a pair of private and public keys \n",
      "for encryption, which is more secure. SSH key pairs in Linux, which are used for\n",
      "---------------\n",
      "authentication, is a very good example of asymmetric encryption.\n",
      "• Integrity: Integrity ensures that the payload and message exchange between \n",
      "the sender and the receiver is not tampered with. The receiver receives the \n",
      "same message that was sent by the sender. Digital signatures and hashes are the \n",
      "implementation mechanisms to check the integrity of incoming messages.\n",
      "---------------\n",
      "228 | Architecting secure applications on Azure\n",
      "Security is a partnership between the service provider and the service consumer. \n",
      "Both parties have different levels of control over deployment stacks, and each should \n",
      "implement security best practices to ensure that all threats are identified and mitigated. \n",
      "We already know from Chapter 1, Getting started with Azure, that the cloud broadly \n",
      "provides three paradigms—IaaS, PaaS, and SaaS—and each of these has different levels \n",
      "of collaborative control over the deployment stack. Each party should implement \n",
      "security practices for the components under its control and within its scope. Failure \n",
      "to implement security at any layer in the stack or by any party would make the entire \n",
      "deployment and application vulnerable to attack. Every organization needs to have a \n",
      "life cycle model for security, just as for any other process. This ensures that security \n",
      "practices are continuously improved to avoid any security flaws. In the next section,\n",
      "---------------\n",
      "we'll be discussing the security life cycle and how it can be used.\n",
      "Security life cycle\n",
      "Security is often regarded as a non-functional requirement for a solution. However, \n",
      "with the growing number of cyberattacks at the moment, nowadays it is considered a \n",
      "functional requirement of every solution.\n",
      "Every organization follows some sort of application life cycle management for their \n",
      "applications. When security is treated as a functional requirement, it should follow the \n",
      "same process of application development. Security should not be an afterthought; it \n",
      "should be part of the application from the beginning. Within the overall planning phase \n",
      "for an application, security should also be planned. Depending on the nature of the \n",
      "application, different kinds and categories of threats should be identified, and, based \n",
      "on these identifications, they should be documented in terms of scope and approach to \n",
      "mitigate them. A threat modeling exercise should be undertaken to illustrate the threat\n",
      "---------------\n",
      "each component could be subject to. This will lead to designing security standards and \n",
      "policies for the application. This is typically the security design phase. The next phase is \n",
      "called the threat mitigation or build phase. In this phase, the implementation of security \n",
      "in terms of code and configuration is executed to mitigate security threats and risks.\n",
      "A system cannot be secure until it is tested. Appropriate penetration tests and other \n",
      "security tests should be performed to identify potential threat mitigation that has not \n",
      "been implemented or has been overlooked. The bugs from testing are remediated and \n",
      "the cycle continues throughout the life of the application. This process of application \n",
      "life cycle management, shown in Figure 8.1, should be followed for security:\n",
      "---------------\n",
      "Security | 229\n",
      "Figure 8.1: Security life cycle\n",
      "Planning, threat modeling, identification, mitigation, testing, and remediation are \n",
      "iterative processes that continue even when an application or service is operational. \n",
      "There should be active monitoring of entire environments and applications to \n",
      "proactively identify threats and mitigate them. Monitoring should also enable alerts and \n",
      "audit logs to help in reactive diagnosis, troubleshooting, and the elimination of threats \n",
      "and vulnerabilities.\n",
      "The security life cycle of any application starts with the planning phase, which \n",
      "eventually leads to the design phase. In the design phase, the application's architecture \n",
      "is decomposed into granular components with discrete communication and hosting \n",
      "boundaries. Threats are identified based on their interaction with other components \n",
      "within and across hosting boundaries. Within the overall architecture, threats are\n",
      "---------------\n",
      "mitigated by implementing appropriate security features, and once the mitigation \n",
      "is in place, further testing is done to verify whether the threat still exists. After the \n",
      "application is deployed to production and becomes operational, it is monitored for any \n",
      "security breaches and vulnerabilities, and either proactive or reactive remediation is \n",
      "conducted.\n",
      "As mentioned earlier, different organizations have different processes and methods to \n",
      "implement the security life cycle; likewise, Microsoft provides complete guidance and \n",
      "information about the security life cycle, which is available at https:/ /www.microsoft.\n",
      "com/securityengineering/sdl/practices. Using the practices that Microsoft has shared, \n",
      "every organization can focus on building more secure solutions. As we are progressing \n",
      "in the era of cloud computing and migrating our corporate and customer data to the \n",
      "cloud, learning how to secure that data is vital. In the next section, we will explore\n",
      "---------------\n",
      "Azure security and the different levels of security, which will help us to build secure \n",
      "solutions in Azure.\n",
      "---------------\n",
      "230 | Architecting secure applications on Azure\n",
      "Azure security\n",
      "Azure provides all its services through datacenters in multiple Azure regions. These \n",
      "datacenters are interconnected within regions, as well as across regions. Azure \n",
      "understands that it hosts mission-critical applications, services, and data for its \n",
      "customers. It must ensure that security is of the utmost importance for its datacenters \n",
      "and regions.\n",
      "Customers deploy applications to the cloud based on their belief that Azure will protect \n",
      "their applications and data from vulnerabilities and breaches. Customers will not move \n",
      "to the cloud if this trust is broken, and so Azure implements security at all layers, \n",
      "as seen in Figure 8.2, from the physical perimeter of datacenters to logical software \n",
      "components. Each layer is protected, and even the Azure datacenter team does not \n",
      "have access to them:\n",
      "Figure 8.2: Security features at different layers in Azure datacenters\n",
      "---------------\n",
      "Security is of paramount importance to both Microsoft and Azure. Microsoft ensures \n",
      "that trust is built with its customers, and it does so by ensuring that its customers' \n",
      "deployments, solutions, and data are completely secure, both physically and virtually. \n",
      "People will not use a cloud platform if it is not physically and digitally secure.\n",
      "To ensure that customers have trust in Azure, each activity in the development of \n",
      "Azure is planned, documented, audited, and monitored from a security perspective. The \n",
      "physical Azure datacenters are protected from intrusion and unauthorized access. In \n",
      "fact, even Microsoft personnel and operations teams do not have access to customer \n",
      "solutions and data. Some of the out-of-the-box security features provided by Azure are \n",
      "listed here:\n",
      "• Secure user access: A customer's deployment, solution, and data can only be \n",
      "accessed by the customer. Even Azure datacenter personnel do not have access to\n",
      "---------------\n",
      "customer artifacts. Customers can allow access to other people; however, that is \n",
      "at the discretion of the customer.\n",
      "---------------\n",
      "IaaS security | 231\n",
      "• Encryption at rest: Azure encrypts all its management data, which includes a \n",
      "variety of enterprise-grade storage solutions to accommodate different needs. \n",
      "Microsoft also provides encryption to managed services such as Azure SQL \n",
      "Database, Azure Cosmos DB, and Azure Data Lake Storage as well. Since the data \n",
      "is encrypted at rest, it cannot be read by anyone. It also provides this functionality \n",
      "to its customers, as well as those who can encrypt their data at rest.\n",
      "• Encryption at transit: Azure encrypts all data that flows from its network. It also \n",
      "ensures that its network backbone is protected from any unauthorized access.\n",
      "• Active monitoring and auditing: Azure monitors all its datacenters actively on an \n",
      "ongoing basis. It actively identifies any breach, threat, or risk, and mitigates them.\n",
      "Azure meets country-specific, local, international, and industry-specific compliance \n",
      "standards. You can explore the complete list of Microsoft compliance offerings at\n",
      "---------------\n",
      "https:/ /www.microsoft.com/trustcenter/compliance/complianceofferings. Keep this \n",
      "as a reference while deploying compliant solutions in Azure. Now that we know the key \n",
      "security features in Azure, let's go ahead and take a deep dive into IaaS security. In the \n",
      "next section, we will explore how customers can leverage the security features available \n",
      "for IaaS in Azure.\n",
      "IaaS security\n",
      "Azure is a mature platform for deploying IaaS solutions. There are lots of users of Azure \n",
      "who want complete control over their deployments, and they typically use IaaS for their \n",
      "solutions. It is important that these deployments and solutions are secure, by default \n",
      "and by design. Azure provides rich security features to secure IaaS solutions. In this \n",
      "section, some of the main features will be covered.\n",
      "Network security groups\n",
      "The bare minimum of IaaS deployment consists of virtual machines and virtual \n",
      "networks. A virtual machine might be exposed to the internet by applying a public IP\n",
      "---------------\n",
      "to its network interface, or it might only be available to internal resources. Some of \n",
      "those internal resources might, in turn, be exposed to the internet. In any case, virtual \n",
      "machines should be secured so that unauthorized requests should not even reach \n",
      "them. Virtual machines should be secured using facilities that can filter requests on the \n",
      "network itself, rather than the requests reaching a virtual machine and it having to take \n",
      "action on them.\n",
      "---------------\n",
      "232 | Architecting secure applications on Azure\n",
      "Ring-fencing is a mechanism that virtual machines use as one of their security \n",
      "mechanisms. This fence can allow or deny requests depending on their protocol, origin \n",
      "IP, destination IP, originating port, and destination port. This feature is deployed using \n",
      "the Azure network security groups (NSGs) resource. NSGs are composed of rules that \n",
      "are evaluated for both incoming and outgoing requests. Depending on the execution \n",
      "and evaluation of these rules, it is determined whether the requests should be allowed \n",
      "or denied access.\n",
      "NSGs are flexible and can be applied to a virtual network subnet or individual network \n",
      "interfaces. When applied to a subnet, the security rules are applied to all virtual \n",
      "machines hosted on the subnet. On the other hand, applying to a network interface \n",
      "affects requests to only a particular virtual machine associated with that network\n",
      "---------------\n",
      "interface. It is also possible to apply NSGs to both network subnets and network \n",
      "interfaces simultaneously. Typically, this design should be used to apply common \n",
      "security rules at the network subnet level, and unique security rules at the network \n",
      "interface level. It helps to design modular security rules.\n",
      "The flow for evaluating NSGs is shown in Figure 8.3:\n",
      "Figure 8.3: A flow diagram representing the evaluation of NSGs\n",
      "---------------\n",
      "IaaS security | 233\n",
      "When a request reaches an Azure host, depending on whether it's an inbound or \n",
      "outbound request, appropriate rules are loaded and executed against the request/\n",
      "response. If the rule matches the request/response, either the request/response \n",
      "is allowed or denied. The rule matching consists of important request/response \n",
      "information, such as the source IP address, destination IP address, source port, \n",
      "destination port, and protocol used. Additionally, NSGs support service tags. A service \n",
      "tag denotes a group of IP address prefixes from a given Azure service. Microsoft \n",
      "manages the address prefixes and automatically updates them. This eliminates the \n",
      "hassle of updating the security rules every time there is an address prefix change.\n",
      "The set of service tags available for use is available at https:/ /docs.microsoft.com/\n",
      "azure/virtual-network/service-tags-overview#available-service-tags. Service tags\n",
      "---------------\n",
      "can be used with NSGs as well as with Azure Firewall. Now that you have learned about \n",
      "how NSGs work, let's take a look at the NSG design, which will help you determine the \n",
      "primary points you should consider while creating NSG rules to improve security.\n",
      "NSG design\n",
      "The first step in designing an NSG is to ascertain the security requirements of the \n",
      "resource. The following should be determined or considered:\n",
      "• Is the resource accessible from the internet only?\n",
      "• Is the resource accessible from both internal resources and the internet?\n",
      "• Is the resource accessible from internal resources only?\n",
      "• Based on the architecture of the solution being deployed, determine the \n",
      "dependent resources, load balancers, gateways, and virtual machines used.\n",
      "• Configure a virtual network and its subnet.\n",
      "Using the results of these investigations, an adequate NSG design should be created. \n",
      "Ideally, there should be multiple network subnets for each workload and type of\n",
      "---------------\n",
      "resource. It is not recommended to deploy both load balancers and virtual machines on \n",
      "the same subnet.\n",
      "Taking project requirements into account, rules should be determined that are common \n",
      "for different virtual machine workloads and subnets. For example, for a SharePoint \n",
      "deployment, the front-end application and SQL servers are deployed on separate \n",
      "subnets, so rules for each subnet should be determined.\n",
      "After common subnet-level rules are identified, rules for individual resources should \n",
      "be identified, and these should be applied at the network interface level. It is important \n",
      "to understand that if a rule allows an incoming request on a port, that port can also be \n",
      "used for outgoing requests without any configuration.\n",
      "---------------\n",
      "234 | Architecting secure applications on Azure\n",
      "If resources are accessible from the internet, rules should be created with specific IP \n",
      "ranges and ports wherever possible, instead of allowing traffic from all the IP ranges \n",
      "(usually represented as 0.0.0.0/0). Careful functional and security testing should be \n",
      "executed to ensure that adequate and optimal NSG rules are opened and closed.\n",
      "Firewalls\n",
      "NSGs provide external security perimeters for requests. However, this does not mean \n",
      "that virtual machines should not implement additional security measures. It is always \n",
      "better to implement security both internally and externally. Virtual machines, whether \n",
      "in Linux or Windows, provide a mechanism to filter requests at the operating system \n",
      "level. This is known as a firewall in both Windows and Linux.\n",
      "It is advisable to implement firewalls for operating systems. They help build a virtual \n",
      "security wall that allows only those requests that are considered trusted. Any untrusted\n",
      "---------------\n",
      "requests are denied access. There are even physical firewall devices, but on the cloud, \n",
      "operating system firewalls are used. Figure 8.4 shows firewall configuration for a \n",
      "Windows operating system:\n",
      "Figure 8.4: Firewall configuration\n",
      "Firewalls filter network packets and identify incoming ports and IP addresses. Using the \n",
      "information from these packets, the firewall evaluates the rules and decides whether it \n",
      "should allow or deny access.\n",
      "---------------\n",
      "IaaS security | 235\n",
      "When it comes to Linux, there are different firewall solutions available. Some of the \n",
      "firewall offerings are very specific to the distribution that is being used; for example, \n",
      "SUSE uses SuSefirewall2 and Ubuntu uses ufw. The most widely used implementations \n",
      "are firewalld and iptables, which are available on every distribution.\n",
      "Firewall design\n",
      "As a best practice, firewalls should be evaluated for individual operating systems. Each \n",
      "virtual machine has a distinct responsibility within the overall deployment and solution. \n",
      "Rules for these individual responsibilities should be identified and firewalls should be \n",
      "opened and closed accordingly.\n",
      "While evaluating firewall rules, it is important to take NSG rules at both the subnet and \n",
      "individual network interface level into consideration. If this is not done properly, it is \n",
      "possible that rules are denied at the NSG level, but left open at the firewall level, and\n",
      "---------------\n",
      "vice versa. If a request is allowed at the NSG level and denied at the firewall level, the \n",
      "application will not work as intended, while security risks increase if a request is denied \n",
      "at the NSG level and allowed at the firewall level.\n",
      "A firewall helps you build multiple networks isolated by its security rules. Careful \n",
      "functional and security testing should be executed to ensure that adequate and optimal \n",
      "firewall rules are opened and closed.\n",
      "It makes the most sense to use Azure Firewall, which is a cloud-based network \n",
      "service on top of NSGs. It is very easy to set up, provides central management for \n",
      "administration, and requires zero maintenance. Azure Firewall and NSGs combined can \n",
      "provide security between virtual machines, virtual networks, and even different Azure \n",
      "subscriptions. Having said that, if a solution requires that extra level of security, we can \n",
      "consider implementing an operating system–level firewall. We'll be discussing Azure\n",
      "---------------\n",
      "Firewall in more depth in one of the upcoming sections, Azure Firewall.\n",
      "Application security groups\n",
      "NSGs are applied at the virtual network subnet level or directly to individual network \n",
      "interfaces. While it is sufficient to apply NSGs at the subnet level, there are times when \n",
      "this is not enough. There are different types of workloads available within a single \n",
      "subnet and each of them requires a different security group. It is possible to assign \n",
      "security groups to individual network interface cards (NICs) of the virtual machines, \n",
      "but it can easily become a maintenance nightmare if there is a large number of virtual \n",
      "machines.\n",
      "---------------\n",
      "236 | Architecting secure applications on Azure\n",
      "Azure has a relatively new feature known as application security groups. We can \n",
      "create application security groups and assign them directly to multiple NICs, even \n",
      "when those NICs belong to virtual machines in different subnets and resource groups. \n",
      "The functionality of application security groups is similar to NSGs, except that they \n",
      "provide an alternate way of assigning groups to network resources, providing additional \n",
      "flexibility in assigning them across resource groups and subnets. Application security \n",
      "groups can simplify NSGs; however, there is one main limitation. We can have one \n",
      "application security group in the source and destination of a security rule, but having \n",
      "multiple application security groups in a source or destination is not supported \n",
      "right now.\n",
      "One of the best practices for creating rules is to always minimize the number of \n",
      "security rules that you need, to avoid maintenance of explicit rules. In the previous\n",
      "---------------\n",
      "section, we discussed the usage of service tags with NSGs to eliminate the hassle of \n",
      "maintaining the individual IP address prefixes of each service. Likewise, when using \n",
      "application security groups, we can reduce the complexity of explicit IP addresses \n",
      "and multiple rules. This practice is recommended wherever possible. If your solution \n",
      "demands an explicit rule with an individual IP address or range of IP addresses, only \n",
      "then should you opt for it.\n",
      "Azure Firewall\n",
      "In the previous section, we discussed using Azure Firewall within a Windows/Linux \n",
      "operating system to allow or disallow requests and responses through particular \n",
      "ports and services. While operating system firewalls play an important role from a \n",
      "security point of view and must be implemented for any enterprise deployment, Azure \n",
      "provides a security resource known as Azure Firewall that has a similar functionality of \n",
      "filtering requests based on rules and determining whether a request should be allowed \n",
      "or rejected.\n",
      "---------------\n",
      "The advantage of using Azure Firewall is that it evaluates a request before it reaches \n",
      "an operating system. Azure Firewall is a network resource and is a standalone service \n",
      "protecting resources at the virtual network level. Any resources, including virtual \n",
      "machines and load balancers, that are directly associated with a virtual network can be \n",
      "protected using Azure Firewall.\n",
      "Azure Firewall is a highly available and scalable service that can protect not only \n",
      "HTTP-based requests but any kind of request coming into and going out from a virtual \n",
      "network, including FTP, SSH, and RDP. Azure Firewall can also span multiple Availability \n",
      "Zones during deployment to provide increased availability.\n",
      "---------------\n",
      "IaaS security | 237\n",
      "It is highly recommended that Azure Firewall is deployed for mission-critical workloads \n",
      "on Azure, alongside other security measures. It is also important to note that Azure \n",
      "Firewall should be used even if other services, such as Azure Application Gateway and \n",
      "Azure Front Door, are used, since all these tools have different scopes and features. \n",
      "Additionally, Azure Firewall provides support for service tags and threat intelligence. \n",
      "In the previous section, we discussed the advantages of using service tags. Threat \n",
      "intelligence can be used to generate alerts when traffic comes from or goes to known \n",
      "malicious IP addresses and domains, which are recorded in the Microsoft Threat \n",
      "Intelligence feed.\n",
      "Reducing the attack surface area\n",
      "NSGs and firewalls help with managing authorized requests to the environment. \n",
      "However, the environment should not be overly exposed to attacks. The surface area\n",
      "---------------\n",
      "of the system should be optimally enabled to achieve its functionality, but disabled \n",
      "enough that attackers cannot find loopholes and access areas that are open without \n",
      "any intended use, or open but not adequately secured. Security should be adequately \n",
      "hardened, making it difficult for any attacker to break into the system.\n",
      "Some of the configurations that should be done include the following:\n",
      "• Remove all unnecessary users and groups from the operating system.\n",
      "• Identify group membership for all users.\n",
      "• Implement group policies using directory services.\n",
      "• Block script execution unless it is signed by trusted authorities.\n",
      "• Log and audit all activities.\n",
      "• Install malware and anti-virus software, schedule scans, and update definitions \n",
      "frequently.\n",
      "• Disable or shut down services that are not required.\n",
      "• Lock down the filesystem so only authorized access is allowed.\n",
      "• Lock down changes to the registry.\n",
      "• A firewall must be configured according to the requirements.\n",
      "---------------\n",
      "• PowerShell script execution should be set to Restricted or RemoteSigned. This \n",
      "can be done using the Set-ExecutionPolicy -ExecutionPolicy Restricted or \n",
      "Set-ExecutionPolicy -ExecutionPolicy RemoteSigned PowerShell commands.\n",
      "• Enable enhanced protection through Internet Explorer.\n",
      "• Restrict the ability to create new users and groups.\n",
      "---------------\n",
      "238 | Architecting secure applications on Azure\n",
      "• Remove internet access and implement jump servers for RDP.\n",
      "• Prohibit logging into servers using RDP through the internet. Instead, use site-to-\n",
      "site VPN, point-to-site VPN, or express routes to RDP into remote machines from \n",
      "within the network.\n",
      "• Regularly deploy all security updates.\n",
      "• Run the security compliance manager tool on the environment and implement all \n",
      "of its recommendations.\n",
      "• Actively monitor the environment using Security Center and Operations \n",
      "Management Suite.\n",
      "• Deploy virtual network appliances to route traffic to internal proxies and reverse \n",
      "proxies.\n",
      "• All sensitive data, such as configuration, connection strings, and credentials, \n",
      "should be encrypted.\n",
      "The aforementioned are some of the key points that should be considered from a \n",
      "security standpoint. The list will keep on growing, and we need to constantly improve \n",
      "security to prevent any kind of security breach.\n",
      "Implementing jump servers\n",
      "---------------\n",
      "It is a good idea to remove internet access from virtual machines. It is also a good \n",
      "practice to limit remote desktop services' accessibility from the internet, but then \n",
      "how do you access the virtual machines at all? One good way is to only allow internal \n",
      "resources to RDP into virtual machines using Azure VPN options. However, there is also \n",
      "another way—using jump servers.\n",
      "Jump servers are servers that are deployed in the demilitarized zone (DMZ). This \n",
      "means it is not on the network hosting the core solutions and applications. Instead, \n",
      "it is on a separate network or subnet. The primary purpose of the jump server is to \n",
      "accept RDP requests from users and help them log in to it. From this jump server, users \n",
      "can further navigate to other virtual machines using RDP. It has access to two or more \n",
      "networks: one that has connectivity to the outside world, and another that's internal to \n",
      "the solution. The jump server implements all the security restrictions and provides a\n",
      "---------------\n",
      "secure client to connect to other servers. Normally, access to emails and the internet is \n",
      "disabled on jump servers.\n",
      "An example of deploying a jump server with virtual machine scale sets (VMSSes), \n",
      "using Azure Resource Manager templates is available at https:/ /azure.microsoft.com/\n",
      "resources/templates/201-vmss-windows-jumpbox.\n",
      "---------------\n",
      "Application security | 239\n",
      "Azure Bastion\n",
      "In the previous section, we discussed implementing jump servers. Azure Bastion is a \n",
      "fully managed service that can be provisioned in a virtual network to provide RDP/SSH \n",
      "access to your virtual machines directly in the Azure portal over TLS. The Bastion host \n",
      "will act as a jump server and eliminate the need for public IP addresses for your virtual \n",
      "machines. The concept of using Bastion is the same as implementing a jump server; \n",
      "however, since this is a managed service, it's completely managed by Azure.\n",
      "Since Bastion is a fully managed service from Azure and is hardened internally, we don't \n",
      "need to apply additional NSGs on the Bastion subnet. Also, since we are not attaching \n",
      "any public IPs to our virtual machines, they are protected against port scanning.\n",
      "Application security\n",
      "Web applications can be hosted within IaaS-based solutions on top of virtual machines,\n",
      "---------------\n",
      "and they can be hosted within Azure-provided managed services, such as App Service. \n",
      "App Service is part of the PaaS deployment paradigm, and we will look into it in the next \n",
      "section. In this section, we will look at application-level security.\n",
      "SSL/TLS\n",
      "Secure Socket layer (SSL) is now deprecated and has been replaced by Transport Layer \n",
      "security (TLS). TLS provides end-to-end security by means of cryptography. It provides \n",
      "two types of cryptography:\n",
      "• Symmetric: The same key is available to both the sender of the message and the \n",
      "receiver of the message, and it is used for both the encryption and decryption of \n",
      "the message.\n",
      "• Asymmetric: Every stakeholder has two keys—a private key and a public key. The \n",
      "private key remains on the server or with the user and remains a secret, while \n",
      "the public key is distributed freely to everyone. Holders of the public key use \n",
      "it to encrypt the message, which can only be decrypted by the corresponding\n",
      "---------------\n",
      "private key. Since the private key stays with the owner, only they can decrypt the \n",
      "message. Rivest–Shamir–Adleman (RSA) is one of the algorithms used to generate \n",
      "these pairs of public-private keys. \n",
      "• The keys are also available in certificates popularly known as X.509 certificates, \n",
      "although certificates have more details apart from just the keys and are generally \n",
      "issued by trusted certificate authorities.\n",
      "TLS should be used by web applications to ensure that message exchange between \n",
      "users and the server is secure and confidential and that identities are being protected. \n",
      "These certificates should be purchased from a trusted certificate authority instead of \n",
      "being self-signed certificates.\n",
      "---------------\n",
      "240 | Architecting secure applications on Azure\n",
      "Managed identities\n",
      "Before we take a look at managed identities, it is important to know how applications \n",
      "were built without them.\n",
      "The traditional way of application development is to use secrets, such as a username, a \n",
      "password, or SQL connection strings, in configuration files. Putting these secrets into \n",
      "configuration files makes application changes to these secrets easy and flexible without \n",
      "modifying code. It helps us stick to the \"open for extension, closed for modification\" \n",
      "principle. However, this approach has a downside from a security point of view. The \n",
      "secrets can be viewed by anyone who has access to configuration files since generally \n",
      "these secrets are listed there in plain text. There are a few hacks to encrypt them, but \n",
      "they aren't foolproof.\n",
      "A better way to use secrets and credentials within an application is to store them in \n",
      "a secrets repository such as Azure Key Vault. Azure Key Vault provides full security\n",
      "---------------\n",
      "using the hardware security module (HSM), and the secrets are stored in an encrypted \n",
      "fashion with on-demand decryption using keys stored in separate hardware. Secrets \n",
      "can be stored in Key Vault, with each secret having a display name and key. The key is in \n",
      "the form of a URI that can be used to refer to the secret from applications, as shown in \n",
      "Figure 8.5:\n",
      "Figure 8.5: Storing secrets inside a key vault\n",
      "Within application configuration files, we can refer to the secret using the name or the \n",
      "key. However, there is another challenge now. How does the application connect to and \n",
      "authenticate with the key vault?\n",
      "---------------\n",
      "Application security | 241\n",
      "Key vaults have access policies that define permissions to a user or group with regard \n",
      "to access to secrets and credentials within the key vault. The users, groups, or service \n",
      "applications that can be provided access are provisioned and hosted within Azure \n",
      "Active Directory (Azure AD). Although individual user accounts can be provided access \n",
      "using Key Vault access policies, it is a better practice to use a service principal to access \n",
      "the key vault. A service principal has an identifier, also known as an application ID or \n",
      "client ID, along with a password. The client ID, along with its password, can be used to \n",
      "authenticate with Azure Key Vault. This service principal can be allowed to access the \n",
      "secrets. The access policies for Azure Key Vault are granted in the Access policies pane \n",
      "of your key vault. In Figure 8.6, you can see that the service principal—https:/ /keyvault.\n",
      "book.com—has given access to the key vault called keyvaultbook:\n",
      "---------------\n",
      "Figure 8.6: Granted access for a service principal to access a key vault\n",
      "This brings us to another challenge: to access the key vault, we need to use the client ID \n",
      "and secret in our configuration files to connect to the key vault, get hold of the secret, \n",
      "and retrieve its value. This is almost equivalent to using a username, password, and SQL \n",
      "connection string within configuration files.\n",
      "This is where managed identities can help. Azure launched managed service identities \n",
      "and later renamed them managed identities. Managed identities are identities managed \n",
      "by Azure. In the background, managed identities also create a service principal along \n",
      "with a password. With managed identities, there is no need to put credentials in \n",
      "configuration files.\n",
      "---------------\n",
      "242 | Architecting secure applications on Azure\n",
      "Managed identities can only be used to authenticate with services that support Azure \n",
      "AD as an identity provider. Managed identities are meant only for authentication. If \n",
      "the target service does not provide role-based access control (RBAC) permission to \n",
      "the identity, the identity might not be able to perform its intended activity on the \n",
      "target service.\n",
      "Managed identities come in two flavors:\n",
      "• System-assigned managed identities\n",
      "• User-assigned managed identities\n",
      "System-assigned identities are generated by the service itself. For example, if an app \n",
      "service wants to connect to Azure SQL Database, it can generate the system-assigned \n",
      "managed identity as part of its configuration options. These managed identities also \n",
      "get deleted when the parent resource or service is deleted. As shown in Figure 8.7, \n",
      "a system-assigned identity can be used by App Service to connect to Azure SQL \n",
      "Database:\n",
      "---------------\n",
      "Figure 8.7: Enabling a system-assigned managed identity for App Service\n",
      "User-assigned managed identities are created as standalone separate identities and \n",
      "later assigned to Azure services. They can be applied and reused with multiple Azure \n",
      "services since their life cycles do not depend on the resource they are assigned to.\n",
      "---------------\n",
      "Application security | 243\n",
      "Once a managed identity is created and RBAC or access permissions are given to it on \n",
      "the target resource, it can be used within applications to access the target resources \n",
      "and services.\n",
      "Azure provides an SDK as well a REST API to talk to Azure AD and get an access token \n",
      "for managed identities, and then use the token to access and consume the target \n",
      "resources.\n",
      "The SDK comes as part of the Microsoft.Azure.Services.AppAuthentication NuGet \n",
      "package for C#. Once the access token is available, it can be used to consume the \n",
      "target resource.\n",
      "The code needed to get the access token is as follows:\n",
      "var tokenProvider = new AzureServiceTokenProvider();\n",
      "string token = await tokenProvider.GetAccessTokenAsync(\"https://vault.azure.\n",
      "net\");\n",
      "Alternatively, use this:\n",
      "string token = await tokenProvider.GetAccessTokenAsync(\"https://database.\n",
      "windows.net\");\n",
      "It should be noted that the application code needs to run in the context of App Service\n",
      "---------------\n",
      "or a function app because the identity is attached to them and is only available in code \n",
      "when it's run from within them.\n",
      "The preceding code has two different use cases. The code to access the key vault and \n",
      "Azure SQL Database is shown together.\n",
      "It is important to note that applications do not provide any information related to \n",
      "managed identities in code and is completely managed using configuration. The \n",
      "developers, individual application administrators, and operators will not come across \n",
      "any credentials related to managed identities, and, moreover, there is no mention \n",
      "of them in code either. Credential rotation is completely regulated by the resource \n",
      "provider that hosts the Azure service. The default rotation occurs every 46 days. It's up \n",
      "to the resource provider to call for new credentials if required, so the provider could \n",
      "wait for more than 46 days.\n",
      "In the next section, we will be discussing a cloud-native security information and \n",
      "event manager (SIEM): Azure Sentinel.\n",
      "---------------\n",
      "244 | Architecting secure applications on Azure\n",
      "Azure Sentinel\n",
      "Azure provides an SIEM and security orchestration automated response (SOAR) as a \n",
      "standalone service that can be integrated with any custom deployment on Azure. Figure \n",
      "8.8 shows some of the key features of Azure Sentinel:\n",
      "Figure 8.8: Key features of Azure Sentinel\n",
      "Azure Sentinel collects information logs from deployments and resources and performs \n",
      "analytics to find patterns and trends related to various security issues that are pulled \n",
      "from data sources.\n",
      "There should be active monitoring of the environment, logs should be collected, \n",
      "and information should be culled from these logs as a separate activity from code \n",
      "implementation. This is where the SIEM service comes into the picture. There are \n",
      "numerous connectors that can be used with Azure Sentinel; each of these connectors \n",
      "will be used to add data sources to Azure Sentinel. Azure Sentinel provides connectors\n",
      "---------------\n",
      "for Microsoft services such as Office 365, Azure AD, and Azure Threat Protection. The \n",
      "collected data will be fed to a Log Analytics workspace, and you can write queries to \n",
      "search these logs.\n",
      "SIEM tools such as Azure Sentinel can be enabled on Azure to get all the logs from log \n",
      "analytics and Azure Security Center, which in turn can get them from multiple sources, \n",
      "deployments, and services. SIEM can then run its intelligence on top of this collected \n",
      "data and generate insights. It can generate reports and dashboards based on discovered \n",
      "intelligence for consumption, but it can also investigate suspicious activities and \n",
      "threats, and take action on them.\n",
      "---------------\n",
      "PaaS security | 245\n",
      "While Azure Sentinel may sound very similar in functionality to Azure Security Center, \n",
      "Azure Sentinel can do much more than Azure Security Center. Its ability to collect logs \n",
      "from other avenues using connectors makes it different from Azure Security Center.\n",
      "PaaS security\n",
      "Azure provides numerous PaaS services, each with its own security features. In general, \n",
      "PaaS services can be accessed using credentials, certificates, and tokens. PaaS services \n",
      "allow the generation of short-lived security access tokens. Client applications can send \n",
      "these security access tokens to represent trusted users. In this section, we will cover \n",
      "some of the most important PaaS services that are used in almost every solution.\n",
      "Azure Private Link\n",
      "Azure Private Link provides access to Azure PaaS services as well as Azure-hosted \n",
      "customer-owned/partner-shared services over a private endpoint in your virtual \n",
      "network. While using Azure Private Link, we don't have to expose our services to the\n",
      "---------------\n",
      "public internet, and all traffic between our service and the virtual network goes via \n",
      "Microsoft's backbone network.\n",
      "Azure Private Endpoint is the network interface that helps to privately and securely \n",
      "connect to a service that is powered by Azure Private Link. Since the private endpoint \n",
      "is mapped to the instance of the PaaS service, not to the entire service, users can only \n",
      "connect to the resource. Connections to any other service are denied, and this protects \n",
      "against data leakage. Private Endpoint also lets you access securely from on-premises \n",
      "via ExpressRoute or VPN Tunnels. This eliminates the need to set up public peering or \n",
      "to pass through the public internet to reach the service.\n",
      "Azure Application Gateway\n",
      "Azure provides a Level 7 load balancer known as Azure Application Gateway that \n",
      "can not only load balance but also help in routing using values in URL. It also has a \n",
      "feature known as Web Application Firewall. Azure Application Gateway supports TLS\n",
      "---------------\n",
      "termination at the gateway, so the back-end servers will get the traffic unencrypted. \n",
      "This has several advantages, such as better performance, better utilization of the back-\n",
      "end servers, and intelligent routing of packets. In the previous section, we discussed \n",
      "Azure Firewall and how it protects resources at the network level. Web Application \n",
      "Firewall, on the other hand, protects the deployment at the application level.\n",
      "---------------\n",
      "246 | Architecting secure applications on Azure\n",
      "Any deployed application that is exposed to the internet faces numerous security \n",
      "challenges. Some of the important security threats are as follows:\n",
      "• Cross-site scripting\n",
      "• Remote code execution\n",
      "• SQL injection\n",
      "• Denial of Service (DoS) attacks\n",
      "• Distributed Denial of Service (DDoS) attacks\n",
      "There are many more, though.\n",
      "A large number of these attacks can be addressed by developers by writing defensive \n",
      "code and following best practices; however, it is not just the code that should be \n",
      "responsible for identifying these issues on a live site. Web Application Firewall \n",
      "configures rules that can identify such issues, as mentioned before, and deny requests.\n",
      "It is advised to use Application Gateway Web Application Firewall features to protect \n",
      "applications from live security threats. Web Application Firewall will either allow the \n",
      "request to pass through it or stop it, depending on how it's configured.\n",
      "Azure Front Door\n",
      "---------------\n",
      "Azure has launched a relatively new service known as Azure Front Door. The role \n",
      "of Azure Front Door is quite similar to that of Azure Application Gateway; however, \n",
      "there is a difference in scope. While Application Gateway works within a single region, \n",
      "Azure Front Door works at the global level across regions and datacenters. It has a web \n",
      "application firewall as well that can be configured to protect applications deployed \n",
      "in multiple regions from various security threats, such as SQL injection, remote code \n",
      "execution, and cross-site scripting.\n",
      "Application Gateway can be deployed behind Front Door to address connection \n",
      "draining. Also, deploying Application Gateway behind Front Door will help with the load \n",
      "balancing requirement, as Front Door can only perform path-based load balancing at \n",
      "the global level. The addition of Application Gateway to the architecture will provide \n",
      "further load balancing to the back-end servers in the virtual network.\n",
      "---------------\n",
      "PaaS security | 247\n",
      "Azure App Service Environment\n",
      "Azure App Service is deployed on shared networks behind the scenes. All SKUs of App \n",
      "Service use a virtual network, which can potentially be used by other tenants as well. \n",
      "In order to have more control and a secure App Service deployment on Azure, services \n",
      "can be hosted on dedicated virtual networks. This can be accomplished by using \n",
      "Azure App Service Environment (ASE), which provides complete isolation to run your \n",
      "App Service at a high scale. This also provides additional security by allowing you to \n",
      "deploy Azure Firewall, Application Security Groups, NSGs, Application Gateway, Web \n",
      "Application Firewall, and Azure Front Door. All App Service plans created in App Service \n",
      "Environment will be in an isolated pricing tier, and we cannot choose any other tier.\n",
      "All the logs from this virtual network and compute can then be collated in Azure Log \n",
      "Analytics and Security Center, and finally with Azure Sentinel.\n",
      "---------------\n",
      "Azure Sentinel can then provide insights and execute workbooks and runbooks to \n",
      "respond to security threats in an automated way. Security playbooks can be run in \n",
      "Azure Sentinel in response to alerts. Every security playbook comprises measures that \n",
      "need to be taken in the event of an alert. The playbooks are based on Azure Logic Apps, \n",
      "and this will give you the freedom to use and customize the built-in templates available \n",
      "with Logic Apps.\n",
      "Log Analytics\n",
      "Log Analytics is a new analytics platform for managing cloud deployments, on-premises \n",
      "datacenters, and hybrid solutions.\n",
      "It provides multiple modular solutions—a specific functionality that helps to implement \n",
      "a feature. For example, security and audit solutions help to ascertain a complete view \n",
      "of security for an organization's deployment. Similarly, there are many more solutions, \n",
      "such as automation and change tracking, that should be implemented from a security\n",
      "---------------\n",
      "perspective. Log Analytics security and audit services provide information in the \n",
      "following five categories:\n",
      "• Security domains: These provide the ability to view security records, malware \n",
      "assessments, update assessments, network security, identity and access \n",
      "information, and computers with security events. Access is also provided to the \n",
      "Azure Security Center dashboard.\n",
      "• Anti-malware assessment: This helps to identify servers that are not protected \n",
      "against malware and have security issues. It provides information about  exposure \n",
      "to potential security problems and assesses their criticality of any risk. Users can \n",
      "take proactive actions based on these recommendations. Azure Security Center \n",
      "sub-categories provide information collected by Azure Security Center.\n",
      "---------------\n",
      "248 | Architecting secure applications on Azure\n",
      "• Notable issues: This quickly identifies active issues and grades their severity.\n",
      "• Detections: This category is in preview mode. It enables the identification of \n",
      "attack patterns by visualizing security alerts.\n",
      "• Threat intelligence: This helps to identify attack patterns by visualizing the total \n",
      "number of servers with outbound malicious IP traffic, the malicious threat type, \n",
      "and a map that shows where these IPs come from.\n",
      "The preceding details, when viewed from the portal, are shown in Figure 8.9:\n",
      "Figure 8.9: Information being displayed in the Security And Audit pane of Log Analytics\n",
      "Now that you have learned about security for PaaS services, let's explore how to secure \n",
      "data stored in Azure Storage.\n",
      "Azure Storage\n",
      "Storage accounts play an important part in the overall solution architecture. Storage \n",
      "accounts can store important information, such as user personal identifiable\n",
      "---------------\n",
      "information (PII) data, business transactions, and other sensitive and confidential data. \n",
      "It is of the utmost importance that storage accounts are secure and only allow access to \n",
      "authorized users. The stored data is encrypted and transmitted using secure channels. \n",
      "Storage, as well as the users and client applications consuming the storage account \n",
      "and its data, plays a crucial role in the overall security of data. Data should be kept \n",
      "encrypted at all times. This also includes credentials and connection strings connecting \n",
      "to data stores.\n",
      "---------------\n",
      "Azure Storage | 249\n",
      "Azure provides RBAC to govern who can manage Azure storage accounts. These RBAC \n",
      "permissions are given to users and groups in Azure AD. However, when an application \n",
      "to be deployed on Azure is created, it will have users and customers that are not \n",
      "available in Azure AD. To allow users to access the storage account, Azure Storage \n",
      "provides storage access keys. There are two types of access keys at the storage account \n",
      "level—primary and secondary. Users possessing these keys can connect to the storage \n",
      "account. These storage access keys are used in the authentication step when accessing \n",
      "the storage account. Applications can access storage accounts using either primary \n",
      "or secondary keys. Two keys are provided so that if the primary key is compromised, \n",
      "applications can be updated to use the secondary key while the primary key is \n",
      "regenerated. This helps minimize application downtime. Moreover, it enhances security\n",
      "---------------\n",
      "by removing the compromised key without impacting applications. The storage key \n",
      "details, as seen on the Azure portal, are shown in Figure 8.10:\n",
      "Figure 8.10: Access keys for a storage account\n",
      "Azure Storage provides four services—blob, files, queues, and tables—in an account. \n",
      "Each of these services also provides infrastructure for their own security using secure \n",
      "access tokens.\n",
      "A shared access signature (SAS) is a URI that grants restricted access rights to Azure \n",
      "Storage services: blobs, files, queues, and tables. These SAS tokens can be shared with \n",
      "clients who should not be trusted with the entire storage account key to restrict access \n",
      "to certain storage account resources. By distributing an SAS URI to these clients, access \n",
      "to resources is granted for a specified period.\n",
      "SAS tokens exist at both the storage account and the individual blob, file, table, and \n",
      "queue levels. A storage account–level signature is more powerful and has the right to\n",
      "---------------\n",
      "allow and deny permissions at the individual service level. It can also be used instead of \n",
      "individual resource service levels.\n",
      "---------------\n",
      "250 | Architecting secure applications on Azure\n",
      "SAS tokens provide granular access to resources and can be combined as well. These \n",
      "tokens include read, write, delete, list, add, create, update, and process. Moreover, even \n",
      "access to resources can be determined while generating SAS tokens. It could be for \n",
      "blobs, tables, queues, and files individually, or a combination of them. Storage account \n",
      "keys are for the entire account and cannot be constrained for individual services—\n",
      "neither can they be constrained from the permissions perspective. It is much easier \n",
      "to create and revoke SAS tokens than it is for storage account access keys. SAS tokens \n",
      "can be created for use for a certain period of time, after which they automatically \n",
      "become invalid.\n",
      "It is to be noted that if storage account keys are regenerated, then the SAS token based \n",
      "on them will become invalid and a new SAS token should be created and shared with\n",
      "---------------\n",
      "clients. In Figure 8.11, you can see an option to select the scope, permissions, start date, \n",
      "end date, allowed IP address, allowed protocols, and signing key to create an SAS token:\n",
      "Figure 8.11: Creating an SAS token\n",
      "If we are regenerating key1, which we used to sign the SAS token in the earlier example, \n",
      "then we need to create a new SAS token with key2 or the new key1.\n",
      "---------------\n",
      "Azure Storage | 251\n",
      "Cookie stealing, script injection, and DoS attacks are common means used by attackers \n",
      "to disrupt an environment and steal data. Browsers and the HTTP protocol implement \n",
      "a built-in mechanism that ensures that these malicious activities cannot be performed. \n",
      "Generally, anything that is cross-domain is not allowed by either HTTP or browsers. A \n",
      "script running in one domain cannot ask for resources from another domain. However, \n",
      "there are valid use cases where such requests should be allowed. The HTTP protocol \n",
      "implements cross-origin resource sharing (CORS). With the help of CORS, it is possible \n",
      "to access resources across domains and make them work. Azure Storage configures \n",
      "CORS rules for blob, file, queue, and table resources. Azure Storage allows the creation \n",
      "of rules that are evaluated for each authenticated request. If the rules are satisfied, the \n",
      "request is allowed to access the resource. In Figure 8.12, you can see how to add CORS\n",
      "---------------\n",
      "rules to each of the storage services:\n",
      "Figure 8.12: Creating CORS rules for a storage account\n",
      "Data must not only be protected while in transit; it should also be protected while at \n",
      "rest. If data at rest is not encrypted, anybody who has access to the physical drive in \n",
      "the datacenter can read the data. Although the possibility of a data breach is negligible, \n",
      "customers should still encrypt their data. Storage service encryption also helps protect \n",
      "data at rest. This service works transparently and injects itself without users knowing \n",
      "about it. It encrypts data when the data is saved in a storage account and decrypts it \n",
      "automatically when it is read. This entire process happens without users performing \n",
      "any additional activity.\n",
      "Azure account keys must be rotated periodically. This will ensure that an attacker is not \n",
      "able to gain access to storage accounts.\n",
      "It is also a good idea to regenerate the keys; however, this must be evaluated with\n",
      "---------------\n",
      "regard to its usage in existing applications. If it breaks the existing application, these \n",
      "applications should be prioritized for change management, and changes should be \n",
      "applied gradually.\n",
      "---------------\n",
      "252 | Architecting secure applications on Azure\n",
      "It is always recommended to have individual service–level SAS tokens with limited \n",
      "timeframes. This token should only be provided to users who should access the \n",
      "resources. Always follow the principle of least privilege and provide only the necessary \n",
      "permissions.\n",
      "SAS keys and storage account keys should be stored in Azure Key Vault. This provides \n",
      "secure storage and access to them. These keys can be read at runtime by applications \n",
      "from the key vault, instead of storing them in configuration files.\n",
      "Additionally, you can also use Azure AD to authorize the requests to the blob and queue \n",
      "storage. We'll be using RBAC to give necessary permissions to a service principal, \n",
      "and once we authenticate the service principal using Azure AD, an OAuth 2.0 token \n",
      "is generated. This token can be added to the authorization header of your API calls to \n",
      "authorize a request against blob or queue storage. Microsoft recommends the use of\n",
      "---------------\n",
      "Azure AD authorization while working with blob and queue applications due to the \n",
      "superior security provided by Azure AD and its simplicity compared to SAS tokens.\n",
      "In the next section, we are going to assess the security options available for Azure SQL \n",
      "Database.\n",
      "Azure SQL\n",
      "SQL Server stores relational data on Azure, which is a managed relational database \n",
      "service. It is also known as a Database as a Service (DBaaS) that provides a highly \n",
      "available, scalable, performance-centric, and secure platform for storing data. It is \n",
      "accessible from anywhere, with any programming language and platform. Clients need a \n",
      "connection string comprising the server, database, and security information to connect \n",
      "to it.\n",
      "SQL Server provides firewall settings that prevent access to anyone by default. IP \n",
      "addresses and ranges should be whitelisted to access SQL Server. Architects should \n",
      "only allow IP addresses that they are confident about and that belong to customers/\n",
      "---------------\n",
      "partners. There are deployments in Azure for which either there are a lot of IP \n",
      "addresses or the IP addresses are not known, such as applications deployed in Azure \n",
      "Functions or Logic Apps. For such applications to access Azure SQL, Azure SQL allows \n",
      "whitelisting of all IP addresses to Azure services across subscriptions.\n",
      "It is to be noted that firewall configuration is at the server level and not the database \n",
      "level. This means that any changes here affect all databases within a server. In Figure \n",
      "8.13, you can see how to add clients IPs to the firewall to grant access to the server:\n",
      "---------------\n",
      "Azure SQL | 253\n",
      "Figure 8.13: Configuring firewall rules\n",
      "Azure SQL also provides enhanced security by encrypting data at rest. This ensures \n",
      "that nobody, including the Azure datacenter administrators, can view the data stored in \n",
      "SQL Server. The technology used by SQL Server for encrypting data at rest is known as \n",
      "Transparent Data Encryption (TDE). There are no changes required at the application \n",
      "level to implement TDE. SQL Server encrypts and decrypts data transparently when \n",
      "the user saves and reads data. This feature is available at the database level. We can also \n",
      "integrate TDE with Azure Key Vault to have Bring Your Own Key (BYOK). Using BYOK, \n",
      "we can enable TDE using a customer-managed key in Azure Key Vault.\n",
      "---------------\n",
      "254 | Architecting secure applications on Azure\n",
      "SQL Server also provides dynamic data masking (DDM), which is especially useful for \n",
      "masking certain types of data, such as credit card details or user PII data. Masking is \n",
      "not the same as encryption. Masking does not encrypt data, but only masks it, which \n",
      "ensures that data is not in a human-readable format. Users should mask and encrypt \n",
      "sensitive data in Azure SQL Server.\n",
      "SQL Server also provides an auditing and threat detection service for all servers. \n",
      "There are advanced data collection and intelligence services running on top of these \n",
      "databases to discover threats and vulnerabilities and alert users to them. Audit logs \n",
      "are maintained by Azure in storage accounts and can be viewed by administrators to \n",
      "be actioned. Threats such as SQL injection and anonymous client logins can generate \n",
      "alerts that administrators can be informed about over email. In Figure 8.14, you can see \n",
      "how to enable Threat Detection:\n",
      "---------------\n",
      "Figure 8.14: Enabling Threat Protection and selecting the types of threat to detect\n",
      "Data can be masked in Azure SQL. This helps us store data in a format that cannot be \n",
      "read by humans:\n",
      "---------------\n",
      "Azure SQL | 255\n",
      "Figure 8.15: Configuring the settings to mask data\n",
      "Azure SQL also provides TDE to encrypt data at rest, as shown in Figure 8.16:\n",
      "Figure 8.16: Enabling TDE\n",
      "To conduct a vulnerability assessment on SQL Server, you can leverage SQL \n",
      "Vulnerability Assessment, which is a part of the unified package for advanced SQL \n",
      "security capabilities known as Advanced Data Security. SQL Vulnerability Assessment \n",
      "can be used by customers proactively to improve the security of the database by \n",
      "discovering, tracking, and helping you to remediate potential database vulnerabilities.\n",
      "---------------\n",
      "256 | Architecting secure applications on Azure\n",
      "We have mentioned Azure Key Vault a few times in the previous sections, when we \n",
      "discussed managed identities, SQL Database, and so on. You know the purpose of Azure \n",
      "Key Vault now, and in the next section, we will be exploring some methods that can help \n",
      "secure the contents of your key vault.\n",
      "Azure Key Vault\n",
      "Securing resources using passwords, keys, credentials, certificates, and unique \n",
      "identifiers is an important element of any environment and application from the \n",
      "security perspective. They need to be protected, and ensuring that these resources \n",
      "remain secure and do not get compromised is an important pillar of security \n",
      "architecture. Management and operations that keep the secrets and keys secure, while \n",
      "making them available when needed, are important aspects that cannot be ignored. \n",
      "Typically, these secrets are used all over the place—within the source code, inside\n",
      "---------------\n",
      "configuration files, on pieces of paper, and in other digital formats. To overcome these \n",
      "challenges and store all secrets uniformly in a centralized secure storage, Azure Key \n",
      "Vault should be used.\n",
      "Azure Key Vault is well integrated with other Azure services. For example, it would \n",
      "be easy to use a certificate stored in Azure Key Vault and deploy it to an Azure virtual \n",
      "machine's certificate store. All kinds of keys, including storage keys, IoT and event \n",
      "keys, and connection strings, can be stored as secrets in Azure Key Vault. They can \n",
      "be retrieved and used transparently without anyone viewing them or storing them \n",
      "temporarily anywhere. Credentials for SQL Server and other services can also be stored \n",
      "in Azure Key Vault.\n",
      "Azure Key Vault works on a per-region basis. What this means is that an Azure Key \n",
      "Vault resource should be provisioned in the same region where the application \n",
      "and service are deployed. If a deployment consists of more than one region and\n",
      "---------------\n",
      "needs services from Azure Key Vault, multiple Azure Key Vault instances should be \n",
      "provisioned.\n",
      "An important feature of Azure Key Vault is that the secrets, keys, and certificates are \n",
      "not stored in general storage. This sensitive data is backed up by the HSM. This means \n",
      "that this data is stored in separate hardware on Azure that can only be unlocked by keys \n",
      "owned by users. To provide added security, you can also implement virtual network \n",
      "service endpoints for Azure Key Vault. This will restrict access to the key vault to \n",
      "specific virtual networks. You can also restrict access to an IPv4 address range.\n",
      "---------------\n",
      "Authentication and authorization using OAuth | 257\n",
      "In the Azure Storage section, we discussed using Azure AD to authorize requests to \n",
      "blobs and queues. It was mentioned that we use an OAuth token, which is obtained \n",
      "from Azure AD, to authenticate API calls. In the next section, you will learn how \n",
      "authentication and authorization are done using OAuth. Once you have completed \n",
      "the next section, you will be able to relate it to what we discussed in the Azure Storage \n",
      "section.\n",
      "Authentication and authorization using OAuth\n",
      "Azure AD is an identity provider that can authenticate users based on already available \n",
      "users and service principals available within the tenant. Azure AD implements \n",
      "the OAuth protocol and supports authorization on the internet. It implements an \n",
      "authorization server and services to enable the OAuth authorization flow, implicit as \n",
      "well as client credential flows. These are different well-documented OAuth interaction\n",
      "---------------\n",
      "flows between client applications, authorization endpoints, users, and protected \n",
      "resources.\n",
      "Azure AD also supports single sign-on (SSO), which adds security and ease when \n",
      "signing in to applications that are registered with Azure AD. You can use OpenID \n",
      "Connect, OAuth, SAML, password-based, or the linked or disabled SSO method when \n",
      "developing new applications. If you are unsure of which to use, refer to the flowchart \n",
      "from Microsoft here: https:/ /docs.microsoft.com/azure/active-directory/manage-\n",
      "apps/what-is-single-sign-on#choosing-a-single-sign-on-method.\n",
      "Web applications, JavaScript-based applications, and native client applications (such \n",
      "as mobile and desktop applications) can use Azure AD for both authentication and \n",
      "authorization. There are social media platforms, such as Facebook, Twitter, and so on, \n",
      "that support the OAuth protocol for authorization.\n",
      "One of the easiest ways to enable authentication for web applications using Facebook is\n",
      "---------------\n",
      "shown next. There are other methods that use security binaries, but that is outside the \n",
      "scope of this book.\n",
      "In this walkthrough, an Azure App Service will be provisioned along with an App Service \n",
      "Plan to host a custom web application. A valid Facebook account will be needed as a \n",
      "prerequisite in order to redirect users to it for authentication and authorization.\n",
      "---------------\n",
      "258 | Architecting secure applications on Azure\n",
      "A new resource group can be created using the Azure portal, as shown in Figure 8.17:\n",
      "Figure 8.17: Creating a new resource group\n",
      "After the resource group has been created, a new app service can be created using the \n",
      "portal, as shown in Figure 8.18:\n",
      "Figure 8.18: Creating a new application\n",
      "---------------\n",
      "Authentication and authorization using OAuth | 259\n",
      "It is important to note the URL of the web application because it will be needed later \n",
      "when configuring the Facebook application.\n",
      "Once the web application is provisioned in Azure, the next step is to create a new \n",
      "application in Facebook. This is needed to represent your web application within \n",
      "Facebook and to generate appropriate client credentials for the web application. This is \n",
      "the way Facebook knows about the web application.\n",
      "Navigate to developers.facebook.com and log in using the appropriate credentials. \n",
      "Create a new application by selecting the Create App option under My Apps in the \n",
      "top-right corner, as shown in Figure 8.19:\n",
      "Figure 8.19: Creating a new application from the Facebook developer portal\n",
      "The web page will prompt you to provide a name for the web application to create a \n",
      "new application within Facebook:\n",
      "Figure 8.20: Adding a new application\n",
      "---------------\n",
      "Add a new Facebook Login product and click on Set Up to configure login for the \n",
      "custom web application to be hosted on Azure App Service:\n",
      "---------------\n",
      "260 | Architecting secure applications on Azure\n",
      "Figure 8.21: Adding Facebook login to the application\n",
      "The Set Up button provides a few options, as shown in Figure 8.22, and these options \n",
      "configure the OAuth flow, such as authorization flow, implicit flow, or client credential \n",
      "flow. Select the Web option because that is what needs Facebook authorization:\n",
      "Figure 8.22: Selecting the platform\n",
      "---------------\n",
      "Authentication and authorization using OAuth | 261\n",
      "Provide the URL of the web application that we noted earlier after provisioning the web \n",
      "application on Azure:\n",
      "Figure 8.23: Providing the site URL to the application\n",
      "Click on the Settings item in the menu on the left and provide the OAuth redirect \n",
      "URL for the application. Azure already has well-defined callback URLs for each of the \n",
      "popular social media platforms, and the one used for Facebook is domain name/.auth/\n",
      "login/facebook/callback:\n",
      "Figure 8.24: Adding OAuth redirect URIs\n",
      "Go to the Basic settings from the menu on the left and note the values for App ID and \n",
      "App Secret. These are needed to configure the Azure App Services authentication/\n",
      "authorization:\n",
      "---------------\n",
      "262 | Architecting secure applications on Azure\n",
      "Figure 8.25: Finding the App ID and App Secret\n",
      "In the Azure portal, navigate back to the Azure App Service created in the first few \n",
      "steps of this section and navigate to the authentication/authorization blade. Switch on \n",
      "App Services Authentication, select Log in with Facebook for authentication, and click \n",
      "on the Facebook item from the list:\n",
      "Figure 8.26: Enabling Facebook authentication in App Service\n",
      "On the resultant page, provide the already noted app ID and app secret, and also \n",
      "select the scope. The scope decides the information shared by Facebook with the web \n",
      "application:\n",
      "---------------\n",
      "Authentication and authorization using OAuth | 263\n",
      "Figure 8.27: Selecting the scope \n",
      "Click OK and click the Save button to save the authentication/authorization settings.\n",
      "Now, if a new incognito browser session is initiated and you go to the custom web \n",
      "application, the request should get redirected to Facebook. As you might have seen \n",
      "on other websites, when you use Log in with Facebook, you will be asked to give your \n",
      "credentials: \n",
      "Figure 8.28: Logging in to the website using Facebook\n",
      "---------------\n",
      "264 | Architecting secure applications on Azure\n",
      "Once you have entered your credentials, a user consent dialog box will ask for \n",
      "permission for data from Facebook to be shared with the web application:\n",
      "Figure 8.29: User consent to share your information with the application\n",
      "If consent is provided, the web page from the web application should appear:\n",
      "Figure 8.30: Accessing the landing page\n",
      "---------------\n",
      "Security monitoring and auditing | 265\n",
      "A similar approach can be used to protect your web application using Azure AD, Twitter, \n",
      "Microsoft, and Google. You can also integrate your own identity provider if required.\n",
      "The approach shown here illustrates just one of the ways to protect a website using \n",
      "credentials stored elsewhere and the authorization of external applications to access \n",
      "protected resources. Azure also provides JavaScript libraries and .NET assemblies to \n",
      "use the imperative programming method to consume the OAuth endpoints provided by \n",
      "Azure AD and other social media platforms. You are recommended to use this approach \n",
      "for greater control and flexibility for authentication and authorization within your \n",
      "applications.\n",
      "So far, we have discussed security features and how they can be implemented. It is also \n",
      "relevant to have monitoring and auditing in place. Implementing an auditing solution will\n",
      "---------------\n",
      "help your security team to audit the logs and take precautionary measures. In the next \n",
      "section, we will be discussing the security monitoring and auditing solutions in Azure.\n",
      "Security monitoring and auditing\n",
      "Every activity in your environment, from emails to changing a firewall, can be \n",
      "categorized as a security event. From a security standpoint, it's necessary to have a \n",
      "central logging system to monitor and track the changes made. During an audit, if \n",
      "you find suspicious activity, you can discover what the flaw in the architecture is and \n",
      "how it can be remediated. Also, if you had a data breach, the logs will help security \n",
      "professionals to understand the pattern of an attack and how it was executed. Also, \n",
      "necessary preventive measures can be taken to avoid similar incidents in the future. \n",
      "Azure provides the following two important security resources to manage all security \n",
      "aspects of the Azure subscription, resource groups, and resources:\n",
      "• Azure Monitor\n",
      "• Azure Security Center\n",
      "---------------\n",
      "Of these two security resources, we will first explore Azure Monitor.\n",
      "Azure Monitor\n",
      "Azure Monitor is a one-stop shop for monitoring Azure resources. It provides \n",
      "information about Azure resources and their state. It also offers a rich query interface, \n",
      "using information that can be sliced and diced using data at the levels of subscription, \n",
      "resource group, individual resource, and resource type. Azure Monitor collects \n",
      "data from numerous data sources, including metrics and logs from Azure, customer \n",
      "applications, and the agents running in virtual machines. Other services, such as Azure \n",
      "Security Center and Network Watcher, also ingest data to the Log Analytics workspace, \n",
      "which can be analyzed from Azure Monitor. You can use REST APIs to send custom data \n",
      "to Azure Monitor.\n",
      "---------------\n",
      "266 | Architecting secure applications on Azure\n",
      "Azure Monitor can be used through the Azure portal, PowerShell, the CLI, and \n",
      "REST APIs:\n",
      "Figure 8.31: Exploring activity logs\n",
      "The following logs are those provided by Azure Monitor:\n",
      "• Activity log: This shows all management-level operations performed on resources. \n",
      "It provides details about the creation time, the creator, the resource type, and the \n",
      "status of resources.\n",
      "• Operation log (classic): This provides details of all operations performed on \n",
      "resources within a resource group and subscription.\n",
      "• Metrics: This gets performance information for individual resources and sets \n",
      "alerts on them.\n",
      "• Diagnostic settings: This helps us to configure the effects logs by setting up Azure \n",
      "Storage for storing logs, streaming logs in real time to Azure Event Hubs, and \n",
      "sending them to Log Analytics.\n",
      "• Log search: This helps integrate Log Analytics with Azure Monitor.\n",
      "---------------\n",
      "Security monitoring and auditing | 267\n",
      "Azure Monitor can identify security-related incidents and take appropriate action. It is \n",
      "important that only authorized individuals should be allowed to access Azure Monitor, \n",
      "since it might contain sensitive information.\n",
      "Azure Security Center\n",
      "Azure Security Center, as the name suggests, is a one-stop shop for all security needs. \n",
      "There are generally two activities related to security—implementing security and \n",
      "monitoring for any threats and breaches. Security Center has been built primarily to \n",
      "help with both these activities. Azure Security Center enables users to define their \n",
      "security policies and get them implemented on Azure resources. Based on the current \n",
      "state of Azure resources, Azure Security Center provides security recommendations \n",
      "to harden the solution and individual Azure resources. The recommendations include \n",
      "almost all Azure security best practices, including the encryption of data and disks,\n",
      "---------------\n",
      "network protection, endpoint protection, access control lists, the whitelisting of \n",
      "incoming requests, and the blocking of unauthorized requests. The resources range \n",
      "from infrastructure components, such as load balancers, network security groups, and \n",
      "virtual networks, to PaaS resources, such as Azure SQL and Storage. Here is an excerpt \n",
      "from the Overview pane of Azure Security Center, which shows the overall secure score \n",
      "of the subscription, resource security hygiene, and more:\n",
      " \n",
      "Figure 8.32: Azure Security Center overview\n",
      "---------------\n",
      "268 | Architecting secure applications on Azure\n",
      "Azure Security Center is a rich platform that provides recommendations for multiple \n",
      "services, as shown in Figure 8.33. Also, these recommendations can be exported to CSV \n",
      "files for reference:\n",
      " \n",
      "Figure 8.33: Azure Security Center recommendations\n",
      "As was mentioned at the beginning of this section, monitoring and auditing are crucial \n",
      "in an enterprise environment. Azure Monitor can have multiple data sources and can \n",
      "be used to audit logs from these sources. Azure Security Center gives continuous \n",
      "assessments and prioritized security recommendations along with the overall secure \n",
      "score.\n",
      "Summary\n",
      "Security is always an important aspect of any deployment or solution. It has become \n",
      "much more important and relevant because of deployment to the cloud. Moreover, \n",
      "there is an increasing threat of cyberattacks. In these circumstances, security has \n",
      "become a focal point for organizations. No matter the type of deployment or solution,\n",
      "---------------\n",
      "whether it's IaaS, PaaS, or SaaS, security is needed across all of them. Azure datacenters \n",
      "are completely secure, and they have a dozen international security certifications. \n",
      "They are secure by default. They provide IaaS security resources, such as NSGs, \n",
      "network address translation, secure endpoints, certificates, key vaults, storage, virtual \n",
      "machine encryption, and PaaS security features for individual PaaS resources. Security \n",
      "has a complete life cycle of its own and it should be properly planned, designed, \n",
      "implemented, and tested, just like any other application functionality.\n",
      "---------------\n",
      "Summary | 269\n",
      "We discussed operating system firewalls and Azure Firewall and how they can be \n",
      "leveraged to increase the overall security landscape of your solution. We also explored \n",
      "new Azure services, such as Azure Bastion, Azure Front Door, and Azure Private Link.\n",
      "Application security was another key area, and we discussed performing authentication \n",
      "and authorization using OAuth. We did a quick demo of how to create an app service \n",
      "and integrate Facebook login. Facebook was just an example; you could use Google, \n",
      "Twitter, Microsoft, Azure AD, or any custom identity provider.\n",
      "We also explored the security options offered by Azure SQL, which is a managed \n",
      "database service provided by Azure. We discussed the implementation of security \n",
      "features, and in the final section, we concluded with monitoring and auditing \n",
      "with Azure Monitor and Azure Security Center. Security plays a vital role in your \n",
      "environment. An architect should always design and architect solutions with security as\n",
      "---------------\n",
      "one of the main pillars of the architecture; Azure provides many options to achieve this.\n",
      "Now that you know how to secure your data in Azure, in the next chapter, we will focus \n",
      "on big data solutions from Hadoop, followed by Data Lake Storage, Data Lake Analytics, \n",
      "and Data Factory.\n",
      "---------------\n",
      "In the previous chapter, you learned about the various security strategies that can be \n",
      "implemented on Azure. With a secure application, we manage vast amounts of data. \n",
      "Big data has been gaining significant traction over the last few years. Specialized tools, \n",
      "software, and storage are required to handle it. Interestingly, these tools, platforms, \n",
      "and storage options were not available as services a few years back. However, with new \n",
      "cloud technology, Azure provides numerous tools, platforms, and resources to create \n",
      "big data solutions easily. This chapter will detail the complete architecture for ingesting, \n",
      "cleaning, filtering, and visualizing data in a meaningful way.\n",
      "Azure Big Data \n",
      "solutions\n",
      "9\n",
      "---------------\n",
      "272 | Azure Big Data solutions\n",
      "The following topics will be covered in this chapter:\n",
      "• Big data overview\n",
      "• Data integration\n",
      "• Extract-Transform-Load (ETL)\n",
      "• Data Factory\n",
      "• Data Lake Storage\n",
      "• Tools ecosystems such as Spark, Databricks, and Hadoop \n",
      "• Databricks\n",
      "Big data\n",
      "With the influx of cheap devices—such as Internet of Things devices and hand-held \n",
      "devices—the amount of data that is being generated and captured has increased \n",
      "exponentially. Almost every organization has a great deal of data and they are ready \n",
      "to purchase more if needed. When large quantities of data arrive in multiple different \n",
      "formats and on an ever-increasing basis, then we can say we are dealing with big data. \n",
      "In short, there are three key characteristics of big data:\n",
      "• Volume: By volume, we mean the quantity of data both in terms of size (in GB, TB, \n",
      "and PB, for instance) and in terms of the number of records (as in a million rows\n",
      "---------------\n",
      "in a hierarchical data store, 100,000 images, half a billion JSON documents, and so \n",
      "on).\n",
      "• Velocity: Velocity refers to the speed at which data arrives or is ingested. If data \n",
      "does not change frequently or new data does not arrive frequently, the velocity \n",
      "of data is said to be low, while if there are frequent updates and a lot of new data \n",
      "arrives on an ongoing basis frequently, it is said to have high velocity.\n",
      "• Variety: Variety refers to different kinds and formats of data. Data can come \n",
      "from different sources in different formats. Data can arrive as structured data (as \n",
      "in comma-separated files, JSON files, or hierarchical data), as semi-structured \n",
      "databases (as in schema-less NoSQL documents), or as unstructured data (such as \n",
      "binary large objects (blobs), images, PDFs, and so on). With so many variants, it's \n",
      "important to have a defined process for processing ingested data. \n",
      "In the next section, we will check out the general big data process.\n",
      "---------------\n",
      "Big data | 273\n",
      "Process for big data\n",
      "When data comes from multiple sources in different formats and at different speeds, it \n",
      "is important to set out a process of storing, assimilating, filtering, and cleaning data in a \n",
      "way that helps us to work with that data more easily and make the data useful for other \n",
      "processes. There needs to be a well-defined process for managing data. The general \n",
      "process for big data that should be followed is shown in Figure 9.1:\n",
      "Figure 9.1: Big data process\n",
      "There are four main stages of big data processing. Let's explore them in detail:\n",
      "• Ingest: This is the process of bringing and ingesting data into the big data \n",
      "environment. Data can come from multiple sources, and connectors should be \n",
      "used to ingest that data within the big data platform.\n",
      "• Store: After ingestion, data should be stored in the data pool for long-term \n",
      "storage. The storage should be there for both historical as well as live data and\n",
      "---------------\n",
      "must be capable of storing structured, semi-structured, and non-structured \n",
      "data. There should be connectors to read the data from data sources, or the data \n",
      "sources should be able to push data to storage.\n",
      "• Analysis: After data is read from storage, it should be analyzed, a process that \n",
      "requires filtering, grouping, joining, and transforming data to gather insights.\n",
      "• Visualize: The analysis can be sent as reports using multiple notification platforms \n",
      "or used to generate dashboards with graphs and charts.\n",
      "Previously, the tools needed to capture, ingest, store, and analyze big data were not \n",
      "readily available for organizations due to the involvement of expensive hardware and \n",
      "large investments. Also, no platform was available to process them. With the advent of \n",
      "the cloud, it has become easier for organizations to capture, ingest, store, and perform \n",
      "big data analytics using their preferred choice of tools and frameworks. They can\n",
      "---------------\n",
      "pay the cloud provider to use their infrastructure and avoid any capital expenditure. \n",
      "Moreover, the cost of the cloud is very cheap compared to any on-premises solution.\n",
      "Ingest Store Analyze Visualize\n",
      "---------------\n",
      "274 | Azure Big Data solutions\n",
      "Big data demands an immense amount of compute, storage, and network resources. \n",
      "Generally, the amount of resources required is not practical to have on a single machine \n",
      "or server. Even if, somehow, enough resources are made available on a single server, \n",
      "the time it takes to process an entire big data pool is considerably large, since each \n",
      "job is done in sequence and each step has a dependency upon the prior step. There is \n",
      "a need for specialized frameworks and tools that can distribute work across multiple \n",
      "servers and eventually bring back the results from them and present to the user after \n",
      "appropriately combining the results from all the servers. These tools are specialized \n",
      "big data tools that help in achieving availability, scalability, and distribution out of the \n",
      "box to ensure that a big data solution can be optimized to run quickly with built-in \n",
      "robustness and stability.\n",
      "---------------\n",
      "The two prominent Azure big data services are HD Insights and Databricks. Let's go \n",
      "ahead and explore the various tools available in the big data landscape.\n",
      "Big data tools\n",
      "There are many tools and services in the big data space, and we are going to cover some \n",
      "of them in this chapter. \n",
      "Azure Data Factory \n",
      "Azure Data Factory is the flagship ETL service in Azure. It defines incoming data (in \n",
      "terms of its format and schema), transforms data according to business rules and \n",
      "filters, augments existing data, and finally transfers data to a destination store that is \n",
      "readily consumable by other downstream services. It is able to run pipelines (containing \n",
      "ETL logic) on Azure, as well as custom infrastructure, and can also run SQL Server \n",
      "Integration Services packages.\n",
      "Azure Data Lake Storage\n",
      "Azure Data Lake Storage is enterprise-level big data storage that is resilient, highly \n",
      "available, and secure out of the box. It is compatible with Hadoop and can scale to\n",
      "---------------\n",
      "petabytes of data storage. It is built on top of Azure storage accounts and hence gets all \n",
      "of the benefits of storage account directly. The current version is called Gen2, after the \n",
      "capabilities of both Azure Storage and Data Lake Storage Gen1 were combined.\n",
      "---------------\n",
      "Big data tools | 275\n",
      "Hadoop\n",
      "Hadoop was created by the Apache software foundation and is a distributed, scalable, \n",
      "and reliable framework for processing big data that breaks big data down into smaller \n",
      "chunks of data and distributes them within a cluster. A Hadoop cluster comprises two \n",
      "types of servers—masters and slaves. The master server contains the administrative \n",
      "components of Hadoop, while the slaves are the ones where the data processing \n",
      "happens. Hadoop is responsible for the logical partition data between slaves; slaves \n",
      "perform all transformation on data, gather insights, and pass them back to master \n",
      "nodes who will collate them to generate the final output. Hadoop can scale to \n",
      "thousands of servers, with each server providing compute and storage for the jobs. \n",
      "Hadoop is available as a service using the HDInsight service in Azure.\n",
      "There are three main components that make up the Hadoop core system:\n",
      "---------------\n",
      "HDFS: Hadoop Distributed File System is a file system for the storage of big data. \n",
      "It is a distributed framework that helps by breaking down large big data files into \n",
      "smaller chunks and placing them on different slaves in a cluster. HDFS is a fault-\n",
      "tolerant file system. This means that although different chunks of data are made \n",
      "available to different slaves in the cluster, there is also the replication of data \n",
      "between the slaves to ensure that in the event of any slave's failure, that data will \n",
      "also be available on another server. It also provides fast and efficient access to data \n",
      "to the requestor.\n",
      "MapReduce: MapReduce is another important framework that enables Hadoop to \n",
      "process data in parallel. This framework is responsible for processing data stored \n",
      "within HDFS slaves and mapping them to the slaves. After the slaves are done \n",
      "processing, the \"reduce\" part brings information from each slave and collates them\n",
      "---------------\n",
      "together as the final output. Generally, both HDFS and MapReduce are available \n",
      "on the same node, such that the data does not need to travel between slaves and \n",
      "higher efficiency can be achieved when processing them.\n",
      "YARN: Yet Another Resource Negotiator (YARN) is an important Hadoop \n",
      "architectural component that helps in scheduling jobs related to applications and \n",
      "resource management within a cluster. YARN was released as part of Hadoop 2.0, \n",
      "with many casting it as the successor to MapReduce as it is more efficient in terms \n",
      "of batch processing and resource allocation.\n",
      "---------------\n",
      "276 | Azure Big Data solutions\n",
      "Apache Spark \n",
      "Apache Spark is a distributed, reliable analytics platform for large-scale data processing. \n",
      "It provides a cluster that is capable of running transformation and machine learning \n",
      "jobs on large quantities of data in parallel and bringing a consolidated result back to the \n",
      "client. It comprises master and worker nodes, where the master nodes are responsible \n",
      "for dividing and distributing the actions within jobs and data between worker nodes, \n",
      "as well as consolidating the results from all worker nodes and returning the results \n",
      "to the client. An important thing to remember while using Spark is that the logic or \n",
      "calculations should be easily parallelized, and the amount of data is too large to fit on \n",
      "one machine. Spark is available in Azure as a service from HDInsight and Databricks.\n",
      "Databricks\n",
      "Databricks is built on top of Apache Spark. It is a Platform as a Service where a managed\n",
      "---------------\n",
      "Spark cluster is made available to users. It provides lots of added features, such as a \n",
      "complete portal to manage Spark cluster and its nodes, as well as helping to create \n",
      "notebooks, schedule and run jobs, and provide security and support for multiple users. \n",
      "Now, it's time to learn how to integrate data from multiple sources and work with them \n",
      "together using the tools we've been talking about.\n",
      "Data integration\n",
      "We are well aware of how integration patterns are used for applications; applications \n",
      "that are composed of multiple services are integrated together using a variety of \n",
      "patterns. However, there is another paradigm that is a key requirement for many \n",
      "organizations, which is known as data integration. The surge in data integration has \n",
      "primarily happened during the last decade, when the generation and availability of data \n",
      "has become incredibly high. The velocity, variety, and volume of data being generated \n",
      "has increased drastically, and there is data almost everywhere.\n",
      "---------------\n",
      "Every organization has many different types of applications, and they all generate data \n",
      "in their own proprietary format. Often, data is also purchased from the marketplace. \n",
      "Even during mergers and amalgamations of organizations, data needs to be migrated \n",
      "and combined.\n",
      "Data integration refers to the process of bringing data from multiple sources and \n",
      "generating a new output that has more meaning and usability.\n",
      "---------------\n",
      "ETL | 277\n",
      "There is a definite need for data integration in the following scenarios:\n",
      "• Migrating data from a source or group of sources to a target destination. This is \n",
      "needed to make data available in different formats to different stakeholders and \n",
      "consumers.\n",
      "• Getting insights from data. With the rapidly increasing availability of data, \n",
      "organizations want to derive insights from it. They want to create solutions \n",
      "that provide insights; data from multiple sources should be merged, cleaned, \n",
      "augmented, and stored in a data warehouse. \n",
      "• Generating real-time dashboards and reports.\n",
      "• Creating analytics solutions.\n",
      "Application integration has a runtime behavior when users are consuming the \n",
      "application—for example, in the case of credit card validation and integration. On the \n",
      "other hand, data integration happens as a back-end exercise and is not directly linked \n",
      "to user activity. \n",
      "Let's move on to understanding the ETL process with Azure Data Factory.\n",
      "ETL\n",
      "---------------\n",
      "A very popular process known as ETL helps in building a target data source to house \n",
      "data that is consumable by applications. Generally, the data is in a raw format, and to \n",
      "make it consumable, the data should go through the following three distinct phases:\n",
      "• Extract: During this phase, data is extracted from multiple places. For instance, \n",
      "there could be multiple sources and they all need to be connected together in \n",
      "order to retrieve the data. Extract phases typically use data connectors consisting \n",
      "of connection information related to the target data source. They might also have \n",
      "temporary storage to bring the data from the data source and store it for faster \n",
      "retrieval. This phase is responsible for the ingestion of data.\n",
      "• Transform: The data that is available after the extract phase might not be \n",
      "directly consumable by applications. This could be for a variety of reasons; for \n",
      "example, the data might have irregularities, there might be missing data, or there\n",
      "---------------\n",
      "might be erroneous data. Or, there might even be data that is not needed at all. \n",
      "Alternatively, the format of the data might not be conducive to consumption by \n",
      "the target applications. In all of these cases, transformation has to be applied to \n",
      "the data in such a way that it can be efficiently consumed by applications.\n",
      "---------------\n",
      "278 | Azure Big Data solutions\n",
      "• Load: After transformation, data should be loaded to the target data source in a \n",
      "format and schema that enables faster, easier, and performance-centric availability \n",
      "for applications. Again, this typically consists of data connectors for destination \n",
      "data sources and loading data into them.\n",
      "Next, let's cover how Azure Data Factory relates to the ETL process.\n",
      "A primer on Azure Data Factory\n",
      "Azure Data Factory is a fully managed, highly available, highly scalable, and easy-to-use \n",
      "tool for creating integration solutions and implementing ETL phases. Data Factory helps \n",
      "you to create new pipelines in a drag and drop fashion using a user interface, without \n",
      "writing any code; however, it still provides features to allow you to write code in your \n",
      "preferred language.\n",
      "There are a few important concepts to learn about before using the Data Factory \n",
      "service, which we will be exploring in more detail in the following sections:\n",
      "---------------\n",
      "• Activities: Activities are individual tasks that enable the running and processing \n",
      "of logic within a Data Factory pipeline. There are multiple types of activities. \n",
      "There are activities related to data movement, data transformation, and control \n",
      "activities. Each activity has a policy through which it can decide the retry \n",
      "mechanism and retry interval.\n",
      "• Pipelines: Pipelines in Data Factory are composed of groups of activities and \n",
      "are responsible for bringing activities together. Pipelines are the workflows and \n",
      "orchestrators that enable the running of the ETL phases. Pipelines allow the \n",
      "weaving together of activities and allow the declaration of dependencies between \n",
      "them. By using dependencies, it is possible to run some tasks in parallel and other \n",
      "tasks in sequence.\n",
      "• Datasets: Datasets are the sources and destinations of data. These could be Azure \n",
      "storage accounts, Data Lake Storage, or a host of other sources.\n",
      "---------------\n",
      "• Linked services: These are services that contain the connection and connectivity \n",
      "information for datasets and are utilized by individual tasks for connecting to \n",
      "them. \n",
      "• Integration runtime: The main engine that is responsible for the running of Data \n",
      "Factory is called the integration runtime. The integration runtime is available on \n",
      "the following three configurations:\n",
      "• Azure: In this configuration, Data Factory runs on the compute resources that are \n",
      "provided by Azure.\n",
      "---------------\n",
      "A primer on Azure Data Lake Storage  | 279\n",
      "• Self-hosted: Data Factory, in this configuration, runs when you bring your own \n",
      "compute resources. This could be through on-premises or cloud-based virtual \n",
      "machine servers.\n",
      "• Azure SQL Server Integration Services (SSIS): This configuration allows the \n",
      "running of traditional SSIS packages written using SQL Server.\n",
      "• Versions: Data Factory comes in two different versions. It is important to \n",
      "understand that all new developments will happen on V2, and that V1 will stay as it \n",
      "is, or fade out at some point. V2 is preferred for the following reasons:\n",
      "It provides the capability to run SQL Server integration packages.\n",
      "It has enhanced functionalities compared to V1.\n",
      "It comes with enhanced monitoring, which is missing in V1.\n",
      "Now that you have a fair understanding of Data Factory, let's get into the various \n",
      "storage options available on Azure.\n",
      "A primer on Azure Data Lake Storage\n",
      "---------------\n",
      "Azure Data Lake Storage provides storage for big data solutions. It is specially designed \n",
      "for storing the large amounts of data that are typically needed in big data solutions. It is \n",
      "an Azure-provided managed service. Customers need to bring their data and store it in \n",
      "a data lake.\n",
      "There are two versions of Azure Data Lake Storage: version 1 (Gen1) and the current \n",
      "version, version 2 (Gen2). Gen2 has all the functionality of Gen1, but one particular \n",
      "difference is that it is built on top of Azure Blob storage.\n",
      "As Azure Blob storage is highly available, can be replicated multiple times, is disaster-\n",
      "ready, and is low in cost, these benefits are transferred to Data Lake Storage Gen2. Data \n",
      "Lake Storage Gen2 can store any kind of data, including relational, non-relational, file \n",
      "system–based, and hierarchical data.\n",
      "Creating a Data Lake Storage Gen2 instance is as simple as creating a new storage \n",
      "account. The only change that needs to be done is enabling the hierarchical namespace\n",
      "---------------\n",
      "from the Advanced tab of your storage account. It is important to note that there is \n",
      "no direct migration or conversion from a general storage account to Azure Data Lake \n",
      "Storage or vice versa. Also, general storage accounts are for storing files, while Data \n",
      "Lake Storage is optimized for reading and ingesting large quantities of data.\n",
      "Next, we will look into the process and main phases while working with big data. These \n",
      "are distinct phases and each is responsible for different activities on data.\n",
      "---------------\n",
      "280 | Azure Big Data solutions\n",
      "Migrating data from Azure Storage to Data Lake Storage Gen2\n",
      "In this section, we will be migrating data from Azure Blob storage to another Azure \n",
      "container of the same Azure Blob storage instance, and we will also migrate data to \n",
      "an Azure Data Lake Storage Gen2 instance using an Azure Data Factory pipeline. \n",
      "The following sections outline the steps that need to be taken to create such an  \n",
      "end-to-end solution.\n",
      "Preparing the source storage account\n",
      "Before we can create Azure Data Factory pipelines and use them for migration, we need \n",
      "to create a new storage account, consisting of a number of containers, and upload the \n",
      "data files. In the real world, these files and the storage connection would already be \n",
      "prepared. The first step for creating a new Azure storage account is to create a new \n",
      "resource group or choose an existing resource group within an Azure subscription.\n",
      "Provisioning a new resource group\n",
      "---------------\n",
      "Every resource in Azure is associated with a resource group. Before we provision an \n",
      "Azure storage account, we need to create a resource group that will host the storage \n",
      "account. The steps for creating a resource group are given here. It is to be noted that a \n",
      "new resource group can be created while provisioning an Azure storage account or an \n",
      "existing resource group can be used:\n",
      "1. Navigate to the Azure portal, log in, and click on + Create a resource; then, search \n",
      "for Resource group.\n",
      "2. Select Resource group from the search results and create a new resource group. \n",
      "Provide a name and choose an appropriate location. Note that all the resources \n",
      "should be hosted in the same resource group and location so that it is easy to \n",
      "delete them.\n",
      "After provisioning the resource group, we will provision a storage account within it.\n",
      "---------------\n",
      "Migrating data from Azure Storage to Data Lake Storage Gen2 | 281\n",
      "Provisioning a storage account\n",
      "In this section, we will go through the steps of creating a new Azure storage account. \n",
      "This storage account will fetch the data source from which data will be migrated. \n",
      "Perform the following steps to create a storage account:\n",
      "1. Click on + Create a resource and search for Storage Account. Select Storage \n",
      "Account from the search results and then create a new storage account.\n",
      "2. Provide a name and location, and then select a subscription based on the resource \n",
      "group that was created earlier.\n",
      "3. Select StorageV2 (general purpose v2) for Account \n",
      "kind, Standard for Performance, and Locally-redundant storage \n",
      "(LRS) for Replication, as demonstrated in Figure 9.2:\n",
      "Figure 9.2: Configuring the storage account\n",
      "---------------\n",
      "282 | Azure Big Data solutions\n",
      "4. Now create a couple of containers within the storage account. The rawdata \n",
      "container contains the files that will be extracted by the Data Factory pipeline \n",
      "and will act as the source dataset, while finaldata will contain files that the Data \n",
      "Factory pipelines will write data to and will act as the destination dataset:\n",
      "Figure 9.3: Creating containers\n",
      "5. Upload a data file (this file is available with the source code) to \n",
      "the rawdata container, as shown in Figure 9.4:\n",
      "Figure 9.4: Uploading a data file\n",
      "After completing these steps, the source data preparation activities are complete. Now \n",
      "we can focus on creating a Data Lake Storage instance.\n",
      "---------------\n",
      "Migrating data from Azure Storage to Data Lake Storage Gen2 | 283\n",
      "Provisioning the Data Lake Storage Gen2 service\n",
      "As we already know, the Data Lake Storage Gen2 service is built on top of the \n",
      "Azure storage account. Because of this, we will be creating a new storage account \n",
      "in the same way that we did earlier—with the only difference being the selection \n",
      "of Enabled for Hierarchical namespace in the Advanced tab of the new Azure storage \n",
      "account. This will create the new Data Lake Storage Gen2 service:\n",
      "Figure 9.5: Creating a new storage account\n",
      "After the creation of the data lake, we will focus on creating a new Data Factory \n",
      "pipeline.\n",
      "---------------\n",
      "284 | Azure Big Data solutions\n",
      "Provisioning Azure Data Factory \n",
      "Now that we have provisioned both the resource group and Azure storage account, it's \n",
      "time to create a new Data Factory resource:\n",
      "1. Create a new Data Factory pipeline by selecting V2 and by providing a name and \n",
      "location, along with a resource group and subscription selection.\n",
      "Data Factory has three different versions, as shown in Figure 9.6. We've already \n",
      "discussed V1 and V2:\n",
      "Figure 9.6: Selecting the version of Data Factory\n",
      "---------------\n",
      "Migrating data from Azure Storage to Data Lake Storage Gen2 | 285\n",
      "2. Once the Data Factory resource is created, click on the Author & Monitor link \n",
      "from the central pane.\n",
      "This will open another window, consisting of the Data Factory designer for the \n",
      "pipelines.\n",
      "The code of the pipelines can be stored in version control repositories such that it can \n",
      "be tracked for code changes and promote collaboration between developers. If you \n",
      "missed setting up the repository settings in these steps, that can be done later. \n",
      "The next section will focus on configuration related to version control repository \n",
      "settings if your Data Factory resource was created without any repository settings \n",
      "being configured.\n",
      "Repository settings\n",
      "Before creating any Data Factory artifacts, such as datasets and pipelines, it is a good \n",
      "idea to set up the code repository for hosting files related to Data Factory:\n",
      "1. From the Authoring page, click on the Manage button and then Git Configuration\n",
      "---------------\n",
      "in the left menu. This will open another pane; click on the Set up code repository \n",
      "button in this pane:\n",
      "Figure 9.7: Setting up a Git repository\n",
      "---------------\n",
      "286 | Azure Big Data solutions\n",
      "2. From the resultant blade, select any one of the types of repositories that you want \n",
      "to store Data Factory code files in. In this case, let's select Azure DevOps Git:\n",
      "Figure 9.8: Selecting the appropriate Git repository type\n",
      "3. Create a new repository or reuse an existing repository from Azure DevOps. You \n",
      "should already have an account in Azure DevOps. If not, visit, https:/ /dev.azure.\n",
      "com and use the same account used for the Azure portal to login and create a new \n",
      "organization and project within it. Refer to Chapter 13, Integrating Azure DevOps, \n",
      "to learn more about creating organizations and projects in Azure DevOps.  \n",
      "Now, we can move back to the Data Factory authoring window and start creating \n",
      "artifacts for our new pipeline.\n",
      "In the next section, we will prepare the datasets that will be used within our Data \n",
      "Factory pipelines.\n",
      "---------------\n",
      "Migrating data from Azure Storage to Data Lake Storage Gen2 | 287\n",
      "Data Factory datasets\n",
      "Now we can go back to the Data Factory pipeline. First, create a new dataset that will \n",
      "act as the source dataset. It will be the first storage account that we create and upload \n",
      "the sample product.csv file to:\n",
      "1. Click on + Datasets -> New DataSet from the left menu and select Azure Blob \n",
      "Storage as data store and delimitedText as the format for the source file. Create \n",
      "a new linked service by providing a name and selecting an Azure subscription \n",
      "and storage account. By default, AutoResolveIntegrationRuntime is used for the \n",
      "runtime environment, which means Azure will provide the runtime environment \n",
      "on Azure-managed compute. Linked services provide multiple authentication \n",
      "methods, and we are using the shared access signature (SAS) uniform resource \n",
      "locator (URI) method. It is also possible to use an account key, service principal, \n",
      "and managed identity as authentication methods:\n",
      "---------------\n",
      "Figure 9.9: Implementing the authentication method\n",
      "---------------\n",
      "288 | Azure Big Data solutions\n",
      "2. Then, on the resultant lower pane in the General tab, click on the Open properties \n",
      "link and provide a name for the dataset:\n",
      " \n",
      "Figure 9.10: Naming the dataset\n",
      "3. From the Connection tab, provide details about the container, the blob file \n",
      "name in the storage account, the row delimiter, the column delimiter, and other \n",
      "information that will help Data Factory to read the source data appropriately. \n",
      "The Connection tab, after configuration, should look similar to Figure 9.11. Notice \n",
      "that the path includes the name of the container and the name of the file:\n",
      "Figure 9.11: Configuring the connection\n",
      "---------------\n",
      "Migrating data from Azure Storage to Data Lake Storage Gen2 | 289\n",
      "4. At this point, if you click on the Preview data button, it shows preview data \n",
      "from the product.csv file. On the Schema tab, add two columns and name them \n",
      "ProductID and ProductPrice. The schema helps in providing an identifier to the \n",
      "columns and also mapping the source columns in the source dataset to the target \n",
      "columns in the target dataset, when the names are not the same.\n",
      "Now that the first dataset is created, let's create the second one.\n",
      "Creating the second dataset\n",
      "Create a new dataset and linked service for the destination blob storage account in the \n",
      "same way that you did before. Note that the storage account is the same as the source \n",
      "but the container is different. Ensure that the incoming data has schema information \n",
      "associated with it as well, as shown in Figure 9.12:\n",
      "Figure 9.12: Creating the second dataset\n",
      "Next, we will create a third dataset.\n",
      "Creating a third dataset\n",
      "---------------\n",
      "Create a new dataset for the Data Lake Storage Gen2 instance as the target dataset. To \n",
      "do this, select the new dataset and then select Azure Data Lake Storage Gen2.\n",
      "Give the new dataset a name and create a new linked service in the Connection tab. \n",
      "Choose Use account key as the authentication method and the rest of the configuration \n",
      "will be auto-filled after selecting the storage account name. Then, test the connection \n",
      "by clicking on the Test connection button. Keep the default configuration for the rest of \n",
      "the tabs, as shown in Figure 9.13:\n",
      "---------------\n",
      "290 | Azure Big Data solutions\n",
      "Figure 9.13: Configuration in Connection tabs\n",
      "Now that we have the connection to source data and also connections to both the \n",
      "source and destination data stores, it's time to create the pipelines that will contain the \n",
      "logic of the data transformation.\n",
      "---------------\n",
      "Migrating data from Azure Storage to Data Lake Storage Gen2 | 291\n",
      "Creating a pipeline\n",
      "After all the datasets are created, we can create a pipeline that will consume those \n",
      "datasets. The steps for creating a pipeline are given next:\n",
      "1. Click on the + Pipelines => New Pipeline menu from the left menu to create \n",
      "a new pipeline. Then, drag and drop the Copy Data activity from the Move & \n",
      "Transform menu, as demonstrated in Figure 9.14:\n",
      "Figure 9.14: Pipeline menu\n",
      "---------------\n",
      "292 | Azure Big Data solutions\n",
      "2. The resultant General tab can be left as it is, but the Source tab should be \n",
      "configured to use the source dataset that we configured earlier:\n",
      "Figure 9.15: Source tab\n",
      "3. The Sink tab is used to configure the destination data store and dataset, and it \n",
      "should be configured to use the target dataset that we configured earlier:\n",
      "Figure 9.16: Sink tab\n",
      "4. On the Mapping tab, map the columns from the source to the destination dataset \n",
      "columns, as shown in Figure 9.17:\n",
      "Figure 9.17: Mapping tab\n",
      "---------------\n",
      "Migrating data from Azure Storage to Data Lake Storage Gen2 | 293\n",
      "Adding one more Copy Data activity\n",
      "Within our pipeline, we can add multiple activities, each responsible for a particular \n",
      "transformation task. The task looked at in this section is responsible for copying data \n",
      "from the Azure storage account to Azure Data Lake Storage:\n",
      "1. Add another Copy Data activity from the left activity menu to migrate data to Data \n",
      "Lake Storage; both of the copy tasks will run in parallel:\n",
      "Figure 9.18: Copy Data activities\n",
      "The configuration for the source is the Azure Blob storage account that contains \n",
      "the product.csv file.\n",
      "The sink configuration will target the Data Lake Storage Gen2 account.\n",
      "2. The rest of the configuration can be left in the default settings for the second \n",
      "Copy Data activity.\n",
      "After the authoring of the pipeline is complete, it can be published to a version control \n",
      "repository such as GitHub. \n",
      "Next, we will look into creating a solution using Databricks and Spark.\n",
      "---------------\n",
      "294 | Azure Big Data solutions\n",
      "Creating a solution using Databricks\n",
      "Databricks is a platform for using Spark as a service. We do not need to provision \n",
      "master and worker nodes on virtual machines. Instead, Databricks provides us with a \n",
      "managed environment consisting of master and worker nodes and also manages them. \n",
      "We need to provide the steps and logic for the processing of data, and the rest is taken \n",
      "care of by the Databricks platform.\n",
      "In this section, we will go through the steps of creating a solution using Databricks. We \n",
      "will be downloading sample data to analyze.\n",
      "The sample CSV has been downloaded from https:/ /ourworldindata.org/coronavirus-\n",
      "source-data, although it is also provided with the code of this book. The URL mentioned \n",
      "before will have more up-to-date data; however, the format might have changed, and so \n",
      "it is recommended to use the file available with the code samples of this book:\n",
      "---------------\n",
      "1. The first step in creating a Databricks solution is to provision it from the Azure \n",
      "portal. There is a 14-day evaluation SKU available along with two other SKUs—\n",
      "standard and premium. The premium SKU has Azure Role-Based Access Control at \n",
      "the level of notebooks, clusters, jobs, and tables:\n",
      "Figure 9.19: Azure portal—Databricks service\n",
      "---------------\n",
      "Creating a solution using Databricks | 295\n",
      "2. After the Data bricks workspace is provisioned, click on the Launch workspace \n",
      "button from the Overview pane. This will open a new browser window and will \n",
      "eventually log you in to the Databricks portal.\n",
      "3. From the Databricks portal, select Clusters from the left menu and create a new \n",
      "cluster, as shown in Figure 9.20:\n",
      "Figure 9.20: Creating a new cluster\n",
      "4. Provide the name, the Databricks runtime version, the number of worker types, \n",
      "the virtual machine size configuration, and the driver type server configuration.\n",
      "5. The creation of the cluster might take a few minutes. After the creation of the \n",
      "cluster, click on Home, select a user from its context menu, and create a new \n",
      "notebook:\n",
      "---------------\n",
      "296 | Azure Big Data solutions\n",
      "Figure 9.21: Selecting a new notebook\n",
      "6. Provide a name to the notebook, as shown next:\n",
      "Figure 9.22: Creating a notebook\n",
      "7. Create a new storage account, as shown next. This will act as storage for the raw \n",
      "COVID data in CSV format:\n",
      "Figure 9.23: Creating a new storage account\n",
      "---------------\n",
      "Creating a solution using Databricks | 297\n",
      "8. Create a container for storing the CSV file, as shown next:\n",
      "Figure 9.24: Creating a container\n",
      "9. Upload the owid-covid-data.csv file to this container.\n",
      "Once you have completed the preceding steps, the next task is to load the data.\n",
      "Loading data\n",
      "The second major step is to load the COVID data within the Databricks workspace. This \n",
      "can be done in two main ways:\n",
      "• Mount the Azure storage container in Databricks and then load the files available \n",
      "within the mount.\n",
      "• Load the data directly from the storage account. This approach has been used in \n",
      "the following example.\n",
      "The following steps should be performed to load and analyze data using Databricks:\n",
      "1. The first step is to connect and access the storage account. The key for the \n",
      "storage account is needed, which is stored within the Spark configuration. Note \n",
      "that the key here is \"fs.azure.account.key.coronadatastorage.blob.core.windows.\n",
      "net\" and the value is the associated key:\n",
      "---------------\n",
      "spark.conf.set(\"fs.azure.account.key.coronadatastorage.blob.core.windows.\n",
      "net\",\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx==\")\n",
      "2. The key for the Azure storage account can be retrieved by navigating to the \n",
      "settings and the Access Keys property of the storage account in the portal.\n",
      "The next step is to load the file and read the data within the CSV file. The schema \n",
      "should be inferred from the file itself instead of being provided explicitly. There is \n",
      "also a header row, which is represented using the option in the next command.\n",
      "The file is referred to using the following format: wasbs://{{container}}@{{storage \n",
      "account name}}.blob.core.windows.net/{{filename}}.\n",
      "---------------\n",
      "298 | Azure Big Data solutions\n",
      "3. The read method of the SparkSession object provides methods to read files. To \n",
      "read CSV files, the csv method should be used along with its required parameters, \n",
      "such as the path to the CSV file. There are additional optional parameters that can \n",
      "be supplied to customize the reading process of the data files. There are multiple \n",
      "types of file formats, such as JSON, Optimized Row Columnar (ORC), and Parquet, \n",
      "and relational databases such as SQL Server and MySQL, NoSQL data stores such \n",
      "as Cassandra and MongoDB, and big data platforms such as Apache Hive that can \n",
      "all be used within Spark. Let's take a look at the following command to understand \n",
      "the implementation of Spark DataFrames:\n",
      "coviddata = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").\n",
      "option(\"header\", \"true\").load(\"wasbs://coviddata@coronadatastorage.blob.\n",
      "core.windows.net/owid-covid-data.csv\")\n",
      "Using this command creates a new object of the DataFrame type in Spark. Spark\n",
      "---------------\n",
      "provides Resilient Distributed Dataset (RDD) objects to manipulate and work \n",
      "with data. RDDs are low-level objects and any code written to work with them \n",
      "might not be optimized. DataFrames are higher-level constructs over RDDs and \n",
      "provide optimization to access and work with them RDDs. It is better to work with \n",
      "DataFrames than RDDs.\n",
      "DataFrames provide data in row-column format, which makes it easier to visualize \n",
      "and work with data. Spark DataFrames are similar to pandas DataFrames, with the \n",
      "difference being that they are different implementations.\n",
      "4. The following command shows the data in a DataFrame. It shows all the rows and \n",
      "columns available within the DataFrame:\n",
      "coviddata.show()\n",
      "You should get a similar output to what you can see in Figure 9.25:\n",
      "Figure 9.25: The raw data in a DataFrame\n",
      "5. The schema of the loaded data is inferred by Spark and can be checked using the \n",
      "following command:\n",
      "coviddata.printSchema()\n",
      "This should give you a similar output to this:\n",
      "---------------\n",
      "Creating a solution using Databricks | 299\n",
      "Figure 9.26: Getting the schema of the DataFrame for each column\n",
      "6. To count the number of rows within the CSV file, the following command can be \n",
      "used, and its output shows that there are 19,288 rows in the file:\n",
      "coviddata.count()\n",
      "Figure 9.27: Finding the count of records in a DataFrame\n",
      "7. The original DataFrame has more than 30 columns. We can also select a subset of \n",
      "the available columns and work with them directly, as shown next:\n",
      "CovidDataSmallSet = coviddata.select(\"location\",\"date\", \"new_cases\", \"new_\n",
      "deaths\")\n",
      "CovidDataSmallSet.show()\n",
      "The output of the code will be as shown in Figure 9.28: \n",
      "Figure 9.28: Selecting a few columns from the overall columns\n",
      "---------------\n",
      "300 | Azure Big Data solutions\n",
      "8. It is also possible to filter data using the filter method, as shown next:\n",
      "CovidDataSmallSet.filter(\" location == 'United States' \").show()\n",
      "9. It is also possible to add multiple conditions together using the AND (&) or OR (|) \n",
      "operators:\n",
      "CovidDataSmallSet.filter((CovidDataSmallSet.location == 'United States') | \n",
      "(CovidDataSmallSet.location == 'Aruba')).show()\n",
      "10. To find out the number of rows and other statistical details, such as the mean, \n",
      "maximum, minimum, and standard deviation, the describe method can be used:\n",
      "CovidDataSmallSet.describe().show()\n",
      "Upon using the preceding command, you'll get a similar output to this:\n",
      "Figure 9.29: Showing each column's statistics using the describe method\n",
      "11. It is also possible to find out the percentage of null or empty data within specified \n",
      "columns. A couple of examples are shown next:\n",
      "from pyspark.sql.functions import  col\n",
      "(coviddata.where(col(\"diabetes_prevalence\").isNull()).count() * 100)/\n",
      "coviddata.count()\n",
      "---------------\n",
      "The output shows 5.998548320199087, which means 95% of the data is null. We \n",
      "should remove such columns from data analysis. Similarly, running the same \n",
      "command on the total_tests_per_thousand column returns 73.62090418913314, \n",
      "which is much better than the previous column.\n",
      "12. To drop some of the columns from the DataFrame, the next command can be used:\n",
      "coviddatanew=coviddata.drop(\"iso_code\").drop(\"total_tests\").drop(\"total_\n",
      "tests\").drop(\"new_tests\").drop(\"total_tests_per_thousand\").drop(\"new_\n",
      "tests_per_thousand\").drop(\"new_tests_smoothed\").drop(\"new_tests_smoothed_\n",
      "per_thousand \")\n",
      "---------------\n",
      "Creating a solution using Databricks | 301\n",
      "13. At times, you will need to have an aggregation of data. In such scenarios, you can \n",
      "perform the grouping of data, as shown here:\n",
      "coviddatanew = coviddata.groupBy('location').agg({'date': 'max'})\n",
      "This will display the data from the groupBy statement:\n",
      "Figure 9.30: Data from the groupby statement\n",
      "14. As you can see in the max (date) column, the dates are mostly the same for all the \n",
      "countries, we can use this value to filter the records and get a single row for each \n",
      "country representing the maximum date:\n",
      "coviddatauniquecountry = coviddata.filter(\"date='2020-05-23 00:00:00'\")\n",
      "coviddatauniquecountry.show()\n",
      "15. If we take a count of records for the new DataFrame, we get 209.\n",
      "We can save the new DataFrame into another CSV file, which may be needed by \n",
      "other data processors:\n",
      "coviddatauniquecountry.rdd.saveAsTextFile(\"dbfs:/mnt/coronadatastorage/\n",
      "uniquecountry.csv\")\n",
      "We can check the newly created file with the following command:\n",
      "---------------\n",
      "%fs ls /mnt/coronadatastorage/\n",
      "---------------\n",
      "302 | Azure Big Data solutions\n",
      "The mounted path will be displayed as shown in Figure 9.31:\n",
      "Figure 9.31: The mounted path within the Spark nodes\n",
      "16. It is also possible to add the data into the Databricks catalog using the \n",
      "createTempView or createOrReplaceTempView method within the Databricks catalog. \n",
      "Putting data into the catalog makes it available in a given context. To add data \n",
      "into the catalog, the createTempView or createOrReplaceTempView method of the \n",
      "DataFrame can be used, providing a new view for the table within the catalog:\n",
      "coviddatauniquecountry.createOrReplaceTempView(\"corona\")\n",
      "17. Once the table is in the catalog, it is accessible from your SQL session, as shown \n",
      "next:\n",
      "spark.sql(\"select * from corona\").show()\n",
      "The data from the SQL statement will apear as shown in Figure 9.32:\n",
      "Figure 9.32: Data from the SQL statement\n",
      "---------------\n",
      "Summary | 303\n",
      "18. It possible to perform an additional SQL query against the table, as shown next:\n",
      "spark.sql(\"select * from corona where location in ('India','Angola') order \n",
      "by location\").show()\n",
      "That was a small glimpse of the possibilities with Databricks. There are many more \n",
      "features and services within it that could not be covered within a single chapter. Read \n",
      "more about it at https:/ /azure.microsoft.com/services/databricks.\n",
      "Summary\n",
      "This chapter dealt with the Azure Data Factory service, which is responsible for \n",
      "providing ETL services in Azure. Since it is a platform as a service, it provides unlimited \n",
      "scalability, high availability, and easy-to-configure pipelines. Its integration with Azure \n",
      "DevOps and GitHub is also seamless. We also explored the features and benefits of \n",
      "using Azure Data Lake Storage Gen2 to store any kind of big data. It is a cost-effective, \n",
      "highly scalable, hierarchical data store for handling big data, and is compatible with\n",
      "---------------\n",
      "Azure HDInsight, Databricks, and the Hadoop ecosystem.\n",
      "By no means did we have a complete deep dive into all the topics mentioned in this \n",
      "chapter. It was more about the possibilities in Azure, especially with Databricks and \n",
      "Spark. There are multiple technologies in Azure related to big data, including HDInsight, \n",
      "Hadoop, Spark and its related ecosystem, and Databricks, which is a Platform as a \n",
      "Service environment for Spark with added functionality. In the next chapter, you will \n",
      "learn about the serverless computing capabilities in Azure.\n",
      "---------------\n",
      "In the previous chapter, you learned about various big data solutions available on Azure. \n",
      "In this chapter, you will learn how serverless technology can help you deal with a large \n",
      "amount of data. \n",
      "Serverless is one of the hottest buzzwords in technology these days, and everyone \n",
      "wants to ride this bandwagon. Serverless brings a lot of advantages in overall \n",
      "computing, software development processes, infrastructure, and technical \n",
      "implementation. There is a lot going on in the industry: at one end of the spectrum is \n",
      "Infrastructure as a Service (IaaS), and at the other is serverless. In between the two \n",
      "are Platform as a Service (PaaS) and containers. I have met many developers and it \n",
      "seems to me that there is some confusion among them about IaaS, PaaS, containers, \n",
      "and serverless computing. Also, there is much confusion about use cases, applicability, \n",
      "architecture, and implementation for the serverless paradigm. Serverless is a new\n",
      "---------------\n",
      "paradigm that is changing not only technology but also the culture and processes \n",
      "within organizations.\n",
      "Serverless in Azure – \n",
      "Working with Azure \n",
      "Functions\n",
      "10\n",
      "---------------\n",
      "306 | Serverless in Azure – Working with Azure Functions\n",
      "We will begin this chapter by introducing serverless, and will cover the following topics \n",
      "as we progress:\n",
      "• Functions as a Service\n",
      "• Azure Functions\n",
      "• Azure Durable Functions\n",
      "• Azure Event Grid\n",
      "Serverless\n",
      "Serverless refers to a deployment model in which users are responsible for only their \n",
      "application code and configuration. In serverless computing, customers do not have to \n",
      "bother about bringing their own underlying platform and infrastructure and, instead, \n",
      "can concentrate on solving their business problems.\n",
      "Serverless does not mean that there are no servers. Code and configuration will \n",
      "always need compute, storage, and networks to run. However, from the customer's \n",
      "perspective, there is no visibility of such compute, storage, and networks. They do not \n",
      "care about the underlying platform and infrastructure. They do not need to manage or \n",
      "monitor infrastructure and the platform. Serverless provides an environment that can\n",
      "---------------\n",
      "scale up and down, in and out, automatically, without the customer even knowing about \n",
      "it. All operations related to platforms and infrastructures happen behind the scenes \n",
      "and are executed by the cloud provider. Customers are provided with performance-\n",
      "related service-level agreements (SLAs) and Azure ensures that it meets those SLAs \n",
      "irrespective of the total demand.\n",
      "Customers are required to only bring in their code; it is the responsibility of the cloud \n",
      "provider to provide the infrastructure and platform needed to run the code. Let's go \n",
      "ahead and dive into the various advantages of Azure Functions.\n",
      "The advantages of Azure Functions\n",
      "Serverless computing is a relatively new paradigm that helps organizations convert \n",
      "large functionalities into smaller, discrete, on-demand functions that can be invoked \n",
      "and executed through automated triggers and scheduled jobs. They are also known \n",
      "as Functions as a Service (FaaS), in which organizations can focus on their domain\n",
      "---------------\n",
      "challenges instead of the underlying infrastructure and platform. FaaS also helps in \n",
      "devolving solution architectures into smaller, reusable functions, thereby increasing \n",
      "return on investment.\n",
      "---------------\n",
      "The advantages of Azure Functions | 307\n",
      "There is a plethora of serverless compute platforms available. Some of the important \n",
      "ones are listed here:\n",
      "• Azure Functions\n",
      "• AWS Lambda\n",
      "• IBM OpenWhisk\n",
      "• Iron.io\n",
      "• Google Cloud Functions\n",
      "In fact, every few days it feels like there is a new platform/framework being introduced, \n",
      "and it is becoming increasingly difficult for enterprises to decide on the framework \n",
      "that works best for them. Azure provides a rich serverless environment known as Azure \n",
      "Functions, and what follows are some of the features that it supports:\n",
      "• Numerous ways to invoke a function—manually, on a schedule, or based on an \n",
      "event.\n",
      "• Numerous types of binding support.\n",
      "• The ability to run functions synchronously as well as asynchronously.\n",
      "• The ability to execute functions based on multiple types of triggers.\n",
      "• The ability to run both long- and short-duration functions. However, large and \n",
      "long-running functions are not recommended as they may lead to unexpected \n",
      "timeouts.\n",
      "---------------\n",
      "• The ability to use proxy features for different function architectures.\n",
      "• Multiple usage models including consumption, as well as the App Service model.\n",
      "• The ability to author functions using multiple languages, such as JavaScript, \n",
      "Python, and C#.\n",
      "• Authorization based on OAuth.\n",
      "• The Durable Functions extension helps in writing stateful functions.\n",
      "• Multiple authentication options, including Azure AD, Facebook, Twitter, and other \n",
      "identity providers.\n",
      "• The ability to easily configure inbound and outbound parameters.\n",
      "• Visual Studio integration for authoring Azure functions.\n",
      "• Massive parallelism.\n",
      "Let's take a look at FaaS and what roles it plays in serverless architecture.\n",
      "---------------\n",
      "308 | Serverless in Azure – Working with Azure Functions\n",
      "FaaS\n",
      "Azure provides FaaS. These are serverless implementations from Azure. With Azure \n",
      "Functions, code can be written in any language the user is comfortable with and Azure \n",
      "Functions will provide a runtime to execute it. Based on the language chosen, an \n",
      "appropriate platform is provided for users to bring their own code. Functions are a unit \n",
      "of deployment and can automatically be scaled out and in. When dealing with functions, \n",
      "users cannot view the underlying virtual machines and platform, but Azure Functions \n",
      "provides a small window to view them via the Kudu Console.\n",
      "There are two main components of Azure Functions:\n",
      "• The Azure Functions runtime\n",
      "• Azure Functions binding and triggers\n",
      "Let's learn about these components in detail.\n",
      "The Azure Functions runtime\n",
      "The core of Azure Functions is its runtime. The precursor to Azure Functions was \n",
      "Azure WebJobs. The code for Azure WebJobs also forms the core for Azure Functions.\n",
      "---------------\n",
      "There are additional features and extensions added to Azure WebJobs to create Azure \n",
      "Functions. The Azure Functions runtime is the magic that makes functions work. Azure \n",
      "Functions is hosted within Azure App Service. Azure App Service loads the Azure \n",
      "runtime and either waits for an external event or a manual activity to occur. On arrival \n",
      "of a request or the occurrence of a trigger, App Service loads the incoming payload, \n",
      "reads the function's function.json file to find the function's bindings and trigger, maps \n",
      "the incoming data to incoming parameters, and invokes the function with parameter \n",
      "values. Once the function completes its execution, the value is again passed back to \n",
      "the Azure Functions runtime by way of an outgoing parameter defined as a binding in \n",
      "the function.json file. The function runtime returns the values to the caller. The Azure \n",
      "Functions runtime acts as the glue that enables the entire performance of functions.\n",
      "---------------\n",
      "The current Azure runtime version is ~3. It is based on the .NET Core 3.1 framework. \n",
      "Prior to this, version ~2 was based on the .NET Core 2.2 framework. The first version, ~1, \n",
      "was based on the .NET 4.7 framework.\n",
      "There were substantial changes from version 1 to 2 because of changes in the \n",
      "underlying framework itself. However, there are very few breaking changes from \n",
      "version 2 to 3 and most functions written in version 2 would continue to run on version \n",
      "3 as well. However, it is recommended that adequate testing is done after migrating \n",
      "from version 2 to 3. There were also breaking changes from version 1 to 2 with regard to \n",
      "triggers and bindings. Triggers and bindings are now available as extensions, with each \n",
      "one in a different assembly in versions 2 and 3.\n",
      "---------------\n",
      "FaaS | 309\n",
      "Azure Functions bindings and triggers\n",
      "If the Azure Functions runtime is the brain of Azure Functions, then Azure Functions \n",
      "bindings and triggers are its heart. Azure Functions promote loose coupling and high \n",
      "cohesion between services using triggers and bindings. Applications written targeting \n",
      "non-serverless environments implement code using imperative syntax for incoming and \n",
      "outgoing parameters and return values. Azure Functions uses a declarative mechanism \n",
      "to invoke functions using triggers and configures the flow of data using bindings.\n",
      "Binding refers to the process of creating a connection between the incoming data \n",
      "and the Azure function along with mapping the data types. The connection could be \n",
      "in a single direction from the runtime to Azure Functions and vice versa or could be \n",
      "multi-directional—the binding can transmit data between the Azure runtime and Azure \n",
      "Functions in both directions. Azure Functions defines bindings declaratively.\n",
      "---------------\n",
      "Triggers are a special type of binding through which functions can be invoked based on \n",
      "external events. Apart from invoking a function, triggers also pass the incoming data, \n",
      "payload, and metadata to the function.\n",
      "Bindings are defined in the function.json file as follows:\n",
      "{ \n",
      "  \"bindings\": [ \n",
      "    { \n",
      "      \"name\": \"checkOut\", \n",
      "      \"type\": \"queueTrigger\", \n",
      "      \"direction\": \"in\", \n",
      "      \"queueName\": \"checkout-items\", \n",
      "      \"connection\": \"AzureWebJobsDashboard\" \n",
      "    }, \n",
      "    { \n",
      "      \"name\": \"Orders\", \n",
      "      \"type\": \"table\", \n",
      "      \"direction\": \"out\", \n",
      "      \"tableName\": \"OrderDetails\", \n",
      "      \"connection\": \"<<Connection to table storage account>>\" \n",
      "   } \n",
      "  ], \n",
      "  \"disabled\": false \n",
      "}\n",
      "---------------\n",
      "310 | Serverless in Azure – Working with Azure Functions\n",
      "In this example, a trigger is declared that invokes the function whenever there is a new \n",
      "item in the storage queue. The type is queueTrigger, the direction is inbound, queueName \n",
      "is checkout-items, and details about the target storage account connection and table \n",
      "name are also shown. All these values are important for the functioning of this binding. \n",
      "The checkOut name can be used within the function's code as a variable.\n",
      "Similarly, a binding for the return value is declared. Here, the return value is \n",
      "named Orders and the data is the output from Azure Functions. The binding writes the \n",
      "return data into Azure Table Storage using the connection string provided.\n",
      "Both bindings and triggers can be modified and authored using the Integrate tab in \n",
      "Azure Functions. In the backend, the function.json file is updated. The checkOut trigger \n",
      "is declared as shown here:\n",
      "Figure 10.1: The Triggers section of the Integrate tab\n",
      "---------------\n",
      "FaaS | 311\n",
      "The Orders output is shown next:\n",
      "Figure 10.2: Adding output details for the storage account\n",
      "The authors of Azure functions do not need to write any plumbing code to get data \n",
      "from multiple sources. They just decide the type of data expected from the Azure \n",
      "runtime. This is shown in the next code segment. Notice that the checkout is available \n",
      "as a string to the function. Multiple data types can be used as binding for functions. For \n",
      "example, a queue binding can provide the following:\n",
      "• A plain old CLR (Common Language Runtime) object (POCO)\n",
      "• A string\n",
      "• A byte\n",
      "• CloudQueueMessage\n",
      "---------------\n",
      "312 | Serverless in Azure – Working with Azure Functions\n",
      "The author of the function can use any one of these data types, and the Azure Functions \n",
      "runtime will ensure that a proper object is sent to the function as a parameter. The \n",
      "following is a code snippet for accepting string data and the Functions runtime will \n",
      "encapsulate incoming data into a string data type before invoking the function. If the \n",
      "runtime is unable to cast the incoming data to a string, it will generate an exception:\n",
      "using System; \n",
      "public static void Run(string checkOut, TraceWriter log) \n",
      "{ \n",
      "    log.Info($\"C# Queue trigger function processed: { checkOut }\"); \n",
      "}\n",
      "It is also important to know that, in Figure 10.2, the storage account names are \n",
      "AzureWebJobsStorage and AzureWebJobsDashboard. Both of these are keys defined in the \n",
      "appSettings section and contain storage account connection strings. These storage \n",
      "accounts are used internally by Azure Functions to maintain its state and the status of \n",
      "function execution.\n",
      "---------------\n",
      "For more information on Azure bindings and triggers, refer to https:/ /docs.microsoft.\n",
      "com/azure/azure-functions/functions-bindings-storage-queue.\n",
      "Now that we have a fair understanding of Azure bindings and triggers, let's check out \n",
      "the various configuration options offered by Azure Functions.\n",
      "Azure Functions configuration\n",
      "Azure Functions provides configuration options at multiple levels. It provides \n",
      "configuration for the following:\n",
      "• The platform itself\n",
      "• Functions App Services\n",
      "These settings affect every function contained by them. More information about these \n",
      "settings are available at https:/ /docs.microsoft.com/azure/azure-functions/functions-\n",
      "how-to-use-azure-function-app-settings.\n",
      "---------------\n",
      "FaaS | 313\n",
      "Platform configuration\n",
      "Azure functions are hosted within Azure App Service, so they get all of its features. \n",
      "Diagnostic and monitoring logs can be configured easily using platform features. \n",
      "Furthermore, App Service provides options for assigning SSL certificates, using a \n",
      "custom domain, authentication, and authorization as part of its networking features. \n",
      "Although customers are not concerned about the infrastructure, operating system, \n",
      "file system, or platform on which functions actually execute, Azure Functions provides \n",
      "the necessary tooling to peek within the underlying system and make changes. The \n",
      "in-portal console and the Kudu Console are the tools used for this purpose. They \n",
      "provide a rich editor to author Azure functions and edit their configuration.\n",
      "Azure Functions, just like App Service, lets you store the configuration information \n",
      "within the web.config application settings section, which can be read on demand. Some\n",
      "---------------\n",
      "of the platform features of function apps are shown in Figure 10.3:\n",
      "Figure 10.3: Platform features of a function app\n",
      "These platform features can be used to configure authentication, custom domains, SSL, \n",
      "and so on. Also, the Platform Features tab provides an overview of the development \n",
      "tools that can be used with the function app. In the next section, we will take a look at \n",
      "the function app settings that are available in the platform features.\n",
      "---------------\n",
      "314 | Serverless in Azure – Working with Azure Functions\n",
      "App Service function settings\n",
      "These settings affect all functions. Application settings can be managed here. Proxies \n",
      "in Azure Functions can be enabled and disabled. We will discuss proxies later in this \n",
      "chapter. They also help in changing the edit mode of a function application and the \n",
      "deployment to slots:\n",
      "Figure 10.4: Function app settings\n",
      "Budget is a very important aspect of the success of any project. Let's explore the various \n",
      "plans offered for Azure Functions.\n",
      "Azure Functions cost plans\n",
      "Azure Functions is based on the Azure App Service and provides a pocket-friendly \n",
      "model for users. There are three cost models.\n",
      "---------------\n",
      "FaaS | 315\n",
      "A consumption plan\n",
      "This is based on the per-second consumption and execution of functions. This plan \n",
      "calculates the cost based on the compute usage during the actual consumption and \n",
      "execution of the function. If a function is not executed, there is no cost associated \n",
      "with it. However, it does not mean that performance is compromised in this plan. \n",
      "Azure functions will automatically scale out and in based on demand, to ensure basic \n",
      "minimum performance levels are maintained. A function execution is allowed 10 \n",
      "minutes for completion.\n",
      "One of the major drawbacks of this plan is that if there is no consumption of functions \n",
      "for a few seconds, the function might get cold and the next request that comes up \n",
      "might face a short delay in getting a response as the function is idle. This phenomenon \n",
      "is called a cold start. However, there are workarounds that can keep functions warm \n",
      "even when there are no legitimate requests. This can be done by writing a scheduled\n",
      "---------------\n",
      "function that keeps invoking the target function to keep it warm.\n",
      "A premium plan\n",
      "This is a relatively new plan and provides lots of benefits compared to both App Service \n",
      "and a consumption plan. In this plan, there are no cold starts for Azure functions. \n",
      "Functions can be associated with a private network and customers can choose their \n",
      "own virtual machine sizes for executing functions. It provides numerous out-of-the-\n",
      "box facilities that were not possible previously with the other two types of plans.\n",
      "An App Service plan\n",
      "This plan provides functions with completely dedicated virtual machines in the \n",
      "backend, and so the cost is directly proportional to the cost of the virtual machine and \n",
      "its size. There is a fixed cost associated with this plan, even if functions are not invoked. \n",
      "Function code can run for as long as necessary. Although there is no time restriction, \n",
      "the default limit is set to 30 minutes. This can be changed by changing the value in the\n",
      "---------------\n",
      "hosts.json file. Within the App Service plan, the function runtime goes idle if not used \n",
      "for a few minutes and can be activated only using an HTTP trigger. There is an Always \n",
      "On setting that can be used to prevent the function runtime from going idle. Scaling is \n",
      "either manual or based on autoscale settings.\n",
      "Along with the flexible pricing option, Azure also offers various hosting options for \n",
      "architecture deployment.\n",
      "---------------\n",
      "316 | Serverless in Azure – Working with Azure Functions\n",
      "Azure Functions destination hosts\n",
      "The Azure Functions runtime can be hosted on Windows as well as on Linux hosts. \n",
      "PowerShell Core, Node.js, Java, Python, and .NET Core-based functions can run on \n",
      "both Windows as well as Linux operating systems. It is important to know which type \n",
      "of underlying operating system is required for the functions because this configuration \n",
      "setting is tied to the function app and in turn to all functions that are contained in it. \n",
      "Also, it is possible to run functions within Docker containers. This is because Azure \n",
      "provides Docker images that have a pre-built function runtime installed in them and \n",
      "functions can be hosted using such images. Now, Docker images can be used to create \n",
      "containers within Kubernetes Pods and hosted on Azure Kubernetes Service, Azure \n",
      "Container Instances, or on unmanaged Kubernetes clusters. These images can be\n",
      "---------------\n",
      "stored within Docker Hub, Azure Container Registry, or any other global as well as \n",
      "private image repositories.\n",
      "To have a clearer understanding, let's look into some of the most prominent use cases \n",
      "for Azure Functions.\n",
      "Azure Functions use cases\n",
      "Azure Functions has many implementations. Let's have a look at some of these use \n",
      "cases.\n",
      "Implementing microservices\n",
      "Azure Functions helps in breaking down large applications into smaller, discrete \n",
      "functional code units. Each unit is treated independently of others and evolves in its \n",
      "own life cycle. Each such code unit has its own compute, hardware, and monitoring \n",
      "requirements. Each function can be connected to all other functions. These units \n",
      "are woven together by orchestrators to build complete functionality. For example, \n",
      "in an e-commerce application, there can be individual functions (code units), each \n",
      "responsible for listing catalogs, recommendations, categories, subcategories, shopping\n",
      "---------------\n",
      "carts, checkouts, payment types, payment gateways, shipping addresses, billing \n",
      "addresses, taxes, shipping charges, cancellations, returns, emails, SMS, and so on. \n",
      "Some of these functions are brought together to create use cases for e-commerce \n",
      "applications, such as product browsing and checkout flow.\n",
      "Integration between multiple endpoints\n",
      "Azure Functions can build overall application functionality by integrating multiple \n",
      "functions. The integration can be based on the triggering of events or it could be \n",
      "on a push basis. This helps in decomposing large monolithic applications into small \n",
      "components.\n",
      "---------------\n",
      "FaaS | 317\n",
      "Data processing\n",
      "Azure Functions can be used for processing incoming data in batches. It can help in \n",
      "processing data in multiple formats, such as XML, CSV, JSON, and TXT. It can also run \n",
      "conversion, enrichment, cleaning, and filtering algorithms. In fact, multiple functions \n",
      "can be used, each doing either conversion or enrichment, cleaning or filtering. Azure \n",
      "Functions can also be used to incorporate advanced cognitive services, such as optical \n",
      "character recognition (OCR), computer vision, and image manipulation and conversion. \n",
      "This is ideal if you want to process API responses and convert them.\n",
      "Integrating legacy applications\n",
      "Azure Functions can help in integrating legacy applications with newer protocols \n",
      "and modern applications. Legacy applications might not be using industry-\n",
      "standard protocols and formats. Azure Functions can act as a proxy for these legacy \n",
      "applications, accepting requests from users or other applications, converting the\n",
      "---------------\n",
      "data into a format understood by a legacy application, and talking to it on protocols it \n",
      "understands. This opens a world of opportunity for integrating and bringing old and \n",
      "legacy applications into the mainstream portfolio.\n",
      "Scheduled jobs\n",
      "Azure Functions can be used to execute continuously or periodically for certain \n",
      "application functions. These application functions can perform tasks such as \n",
      "periodically taking backups, restoring, running batch jobs, exporting and importing \n",
      "data, and bulk emailing.\n",
      "Communication gateways\n",
      "Azure Functions can be used in communication gateways when using notification \n",
      "hubs, SMS, email, and so on. For example, you can use Azure Functions to send a push \n",
      "notification to Android and iOS devices using Azure Notification Hubs.\n",
      "Azure functions are available in different types, which must be selected based on their \n",
      "relationship to optimizing workloads. Let's have a closer look at them.\n",
      "---------------\n",
      "318 | Serverless in Azure – Working with Azure Functions\n",
      "Types of Azure functions\n",
      "Azure functions can be categorized into three different types:\n",
      "• On-demand functions: These are functions that are executed when they \n",
      "are explicitly called or invoked. Examples of such functions include HTTP-based \n",
      "functions and webhooks.\n",
      "• Scheduled functions: These functions are like timer jobs and execute \n",
      "functions on fixed intervals. \n",
      "• Event-based functions: These functions are executed based on external events. \n",
      "For example, uploading a new file to Azure Blob storage generates an event that \n",
      "could start the execution of Azure functions.\n",
      "In the following section, you will learn how to create an event-driven function that will \n",
      "be connected to an Azure Storage account.\n",
      "Creating an event-driven function\n",
      "In this example, an Azure function will be authored and connected to an Azure Storage \n",
      "account. The Storage account has a container for holding all Blob files. The name of the\n",
      "---------------\n",
      "Storage account is incomingfiles and the container is orders, as shown in Figure 10.5:\n",
      "Figure 10.5: Storage account details\n",
      "---------------\n",
      "Creating an event-driven function | 319\n",
      "Perform the following steps to create a new Azure function from the Azure portal: \n",
      "1. Click on the + button beside the Functions menu on the left.\n",
      "2. Select In-Portal from the resultant screen and click on the Continue button.\n",
      "3. Select Azure Blob Storage trigger, as shown in Figure 10.6:\n",
      "Figure 10.6: Selecting Azure Blob Storage trigger\n",
      "Right now, this Azure function does not have connectivity to the Storage account. Azure \n",
      "functions need connection information for the Storage account, and that is available \n",
      "from the Access keys tab in the Storage account. The same information can be obtained \n",
      "using the Azure Functions editor environment. In fact, that environment allows the \n",
      "creation of a new Storage account from the same editor environment.\n",
      "The Azure Blob Storage trigger can be added using the New button beside the Storage \n",
      "account connection input type. It allows the selection of an existing Storage account\n",
      "---------------\n",
      "or the creation of a new Storage account. Since I already have a couple of Storage \n",
      "accounts, I am reusing them, but you should create a separate Azure Storage account. \n",
      "Selecting a Storage account will update the settings in the appSettings section with the \n",
      "connection string added to it.\n",
      "Ensure that a container already exists within the Blob service of the target Azure \n",
      "Storage account. The path input refers to the path to the container. In this case, the \n",
      "orders container already exists within the Storage account. The Create button shown \n",
      "here will provision the new function monitoring the Storage account container:\n",
      "---------------\n",
      "320 | Serverless in Azure – Working with Azure Functions\n",
      "Figure 10.7: Creating a function that monitors the Storage account container\n",
      "The code for the storagerelatedfunctions function is as follows:\n",
      "public static void Run(Stream myBlob, TraceWriter log) \n",
      "{ \n",
      "    log.Info($\"C# Blob trigger function Processed blob\\n  \\n Size {myBlob.\n",
      "Length} Bytes\"); \n",
      "}\n",
      "The bindings are shown here:\n",
      "{ \n",
      "  \"bindings\": [ \n",
      "    { \n",
      "      \"name\": \"myBlob\", \n",
      "      \"type\": \"blobTrigger\", \n",
      "      \"direction\": \"in\", \n",
      "      \"path\": \"orders\", \n",
      "      \"connection\": \"azureforarchitead2b_STORAGE\" \n",
      "    } \n",
      "  ], \n",
      "  \"disabled\": false \n",
      "}\n",
      "---------------\n",
      "Function Proxies | 321\n",
      "Now, uploading any blob file to the orders container should trigger the function:\n",
      " \n",
      "Figure 10.8: C# Blob trigger function processed blob\n",
      "In the next section, we will dive into Azure Function Proxies, which will help you to \n",
      "efficiently handle the requests and responses of your APIs.\n",
      "Function Proxies\n",
      "Azure Function Proxies is a relatively new addition to Azure Functions. Function \n",
      "Proxies helps in hiding the details of Azure functions and exposing completely different \n",
      "endpoints to customers. Function Proxies can receive requests on endpoints, modify \n",
      "the content, body, headers, and URL of the request by changing the values, and \n",
      "augment them with additional data and pass it internally to Azure functions. Once they \n",
      "get a response from these functions, they can again convert, override, and augment the \n",
      "response and send it back to the client.\n",
      "It also helps in invoking different functions for CRUD (create, read, delete, and update)\n",
      "---------------\n",
      "operations using different headers, thereby breaking large functions into smaller \n",
      "ones. It provides a level of security by not exposing the original function endpoint and \n",
      "also helps in changing the internal function implementation and endpoints without \n",
      "impacting its caller. Function Proxies helps by providing clients with a single function \n",
      "URL and then invoking multiple Azure functions in the backend to complete workflows. \n",
      "More information about Azure Function Proxies can be found at https:/ /docs.microsoft.\n",
      "com/azure/azure-functions/functions-proxies.\n",
      "In the next section, we will cover Durable Functions in detail.\n",
      "---------------\n",
      "322 | Serverless in Azure – Working with Azure Functions\n",
      "Durable Functions\n",
      "Durable Functions is one of the latest additions to Azure Functions. It allows architects \n",
      "to write stateful workflows in an Orchestrator function, which is a new function type. \n",
      "As a developer, you can choose to code it or use any form of IDE. Some advantages of \n",
      "using Durable Functions are:\n",
      "• Function output can be saved to local variables and you can call other functions \n",
      "synchronously and asynchronously.\n",
      "• The state is preserved for you.\n",
      "The following is the basic mechanism for invoking Durable Functions:\n",
      "Figure 10.9: Mechanism for invoking Durable Functions\n",
      "Azure Durable Functions can be invoked by any trigger provided by Azure Functions. \n",
      "These triggers include HTTP, Blob storage, Table Storage, Service Bus queues, and \n",
      "more. They can be triggered manually by someone with access to them, or by an \n",
      "application. Figure 10.9 shows a couple of triggers as an example. These are also\n",
      "---------------\n",
      "known as starter Durable Functions. The starter durable functions invoke the durable \n",
      "orchestrator trigger, which contains the main logic for orchestration, and orchestrates \n",
      "the invocation of activity functions.\n",
      "---------------\n",
      "Durable Functions | 323\n",
      "The code written within the durable orchestrator must be deterministic. This means \n",
      "that no matter the number of times the code is executed, the values returned by it \n",
      "should remain the same. The Orchestrator function is a long-running function by \n",
      "nature. This means it can be hydrated, state-serialized, and it goes to sleep after it \n",
      "calls a durable activity function. This is because it does not know when the durable \n",
      "activity function will complete and does not want to wait for it. When the durable \n",
      "activity function finishes its execution, the Orchestrator function is executed again. \n",
      "The function execution starts from the top and executes until it either calls another \n",
      "durable activity function or finishes the execution of the function. It has to re-execute \n",
      "the lines of code that it already executed earlier and should get the same results that \n",
      "it got earlier. Note that the code written within the durable orchestrator must be\n",
      "---------------\n",
      "deterministic. This means that no matter the number of times the code is executed, the \n",
      "values returned by it should remain the same. \n",
      "Let me explain this with the help of an example. If we use a general .NET Core \n",
      "datetime class and return the current date time, it will result in a new value every \n",
      "time we execute the function. The Durable Functions context object provides \n",
      "CurrentUtcDateTime, which will return the same datetime value during re-execution that \n",
      "it returned the first time.\n",
      "These orchestration functions can also wait for external events and enable scenarios \n",
      "related to human hand-off. This concept will be explained later in this section. \n",
      "These activity functions can be called with or without a retry mechanism. Durable \n",
      "Functions can help to solve many challenges and provides features to write functions \n",
      "that can do the following:\n",
      "• Execute long-running functions\n",
      "• Maintain state\n",
      "• Execute child functions in parallel or sequence\n",
      "• Recover from failure easily\n",
      "---------------\n",
      "• Orchestrate the execution of functions in a workflow\n",
      "Now that you have a fair understanding of the inner workings of a durable function, let's \n",
      "explore how to create a durable function in Visual Studio.\n",
      "---------------\n",
      "324 | Serverless in Azure – Working with Azure Functions\n",
      "Steps for creating a durable function using Visual Studio\n",
      "The following are the steps to create a durable function:\n",
      "1. Navigate to the Azure portal and click on Resource groups in the left menu.\n",
      "2. Click on the +Add button in the top menu to create a new resource group.\n",
      "3. Provide the resource group information on the resultant form and click on the \n",
      "Create button, as shown here:\n",
      "Figure 10.10: Creating a resource group\n",
      "4. Navigate to the newly created resource group and add a new function app by \n",
      "clicking on the +Add button in the top menu and search for function app in the \n",
      "resultant search box.\n",
      "5. Select Function App and click on the Create button. Fill in the resultant function \n",
      "app form and click on the Create button. You can also reuse the function app we \n",
      "created earlier.\n",
      "6. Once the function app is created, we will get into our local development\n",
      "---------------\n",
      "environment with visual studio 2019 installed on it. We will get started with Visual \n",
      "Studio and create a new project of type Azure functions, provide it with a name, \n",
      "and select Azure Functions v3 (.NET core) for Function runtime.\n",
      "7. After the project is created, we need to add the DurableTask NuGet package to \n",
      "the project for working with Durable Functions. The version used at the time of \n",
      "writing this chapter is 2.2.2:\n",
      "Figure 10.11: Adding a DurableTask NuGet package\n",
      "---------------\n",
      "Durable Functions | 325\n",
      "8. Now, we can code our durable functions within Visual Studio. Add a new function, \n",
      "provide it with a name, and select the Durable Functions Orchestration trigger \n",
      "type:\n",
      "Figure 10.12: Selecting a Durable Functions Orchestration trigger \n",
      "9. Visual Studio generates the boilerplate code for Durable Functions, and we are \n",
      "going to use it to learn about Durable Functions. Durable Functions activities are \n",
      "functions that are invoked by the main Orchestrator function. There is generally \n",
      "one main Orchestrator function and multiple Durable Functions activities. Once \n",
      "the extension is installed, provide a name for the function and write code that \n",
      "does something useful, such as sending an email or an SMS, connecting to external \n",
      "systems and executing logic, or executing services using their endpoints, such as \n",
      "cognitive services.\n",
      "Visual Studio generates three sets of functions in a single line of code:\n",
      "---------------\n",
      "• HttpStart: This is the starter function. This means that it is responsible for \n",
      "starting the durable function orchestration. The code generated consists of an \n",
      "HTTP trigger starter function; however, it could be any trigger-based function, \n",
      "such as BlobTrigger, a ServiceBus queue, or a trigger-based function.\n",
      "• RunOrchestrator: This is the main durable orchestration function. It is \n",
      "responsible for accepting parameters from the starter function and in \n",
      "turn, invokes multiple durable task functions. Each durable task function is \n",
      "responsible for a functionality and these durable tasks can be invoked either in \n",
      "parallel or in sequence depending on the need. \n",
      "• SayHello: This is the durable task function that is invoked from the durable \n",
      "function orchestrator to do a particular job.\n",
      "---------------\n",
      "326 | Serverless in Azure – Working with Azure Functions\n",
      "10. The code for the starter function (HttpStart) is shown next. This function has a \n",
      "trigger of type HTTP and it accepts an additional binding of type DurableClient. \n",
      "This DurableClient object helps in invoking the Orchestrator function:\n",
      "Figure 10.13: Code for the starter function\n",
      "11. The code for the Orchestrator function (RunOrchestrator) is shown next. This \n",
      "function has a trigger of type OrchestrationTrigger and accepts a parameter of \n",
      "type IDurableOrchestrationContext. This context object helps in invoking durable \n",
      "tasks:\n",
      "Figure 10.14: Code for orchestrator trigger function\n",
      "---------------\n",
      "Durable Functions | 327\n",
      "12. The code for the durable task function (HelloFunction) is shown next. This \n",
      "function has a trigger of type ActivityTrigger and accepts a parameter that \n",
      "can be any type needed for it to execute its functionality. It has a return value \n",
      "of type string and the function is responsible for returning a string value to the \n",
      "orchestration function:\n",
      "Figure 10.15: Code for the durable task function\n",
      "Next, we can execute the function locally, which will start a storage emulator if one's \n",
      "not already started, and will provide a URL for the HTTP trigger function:\n",
      "Figure 10.16: Starting the storage emulator\n",
      "---------------\n",
      "328 | Serverless in Azure – Working with Azure Functions\n",
      "We are going to invoke this URL using a tool known as Postman (this can be \n",
      "downloaded from https:/ /www.getpostman.com/). We just need to copy the URL and \n",
      "execute it in Postman. This activity is shown in Figure 10.17:\n",
      "Figure 10.17: Invoking URLs using Postman\n",
      "Notice that five URLs are generated when you start the orchestrator:\n",
      "• The statusQueryGetUri URL is used to find the current status of the orchestrator. \n",
      "Clicking this URL on Postman opens a new tab, and if we execute this request, it \n",
      "shows the status of the workflow:\n",
      "Figure 10.18: Current status of the orchestrator\n",
      "• The terminatePostUri URL is used for stopping an already running Orchestrator \n",
      "function.\n",
      "• The sendEventPostUri URL is used to post an event to a suspended durable \n",
      "function. Durable functions can be suspended if they are waiting for an external \n",
      "event. This URL is used in those cases.\n",
      "---------------\n",
      "Creating a connected architecture with functions | 329\n",
      "• The purgeHistoryDeleteUri URL is used to delete the history maintained by \n",
      "Durable Functions for a particular invocation from its Table Storage account.\n",
      "Now that you know how to work with Durable Functions using Visual Studio, let's cover \n",
      "another aspect of Azure functions: chaining them together.\n",
      "Creating a connected architecture with functions\n",
      "A connected architecture with functions refers to creating multiple functions, whereby \n",
      "the output of one function triggers another function and provides data for the next \n",
      "function to execute its logic. In this section, we will continue with the previous scenario \n",
      "of the Storage account. In this case, the output of the function being triggered using \n",
      "Azure Storage Blob files will write the size of the file to Azure Cosmos DB.\n",
      "The configuration of Cosmos DB is shown next. By default, there are no collections \n",
      "created in Cosmos DB.\n",
      "---------------\n",
      "A collection will automatically be created when creating a function that will be triggered \n",
      "when Cosmos DB gets any data:\n",
      "Figure 10.19: Creating an Azure Cosmos DB account\n",
      "---------------\n",
      "330 | Serverless in Azure – Working with Azure Functions\n",
      "Let's follow the below steps to retrieve data for the next function from the output of \n",
      "one function.\n",
      "1. Create a new database, testdb, within Cosmos DB, and create a new collection \n",
      "named testcollection within it. You need both the database and the collection \n",
      "name when configuring Azure functions: \n",
      " Figure 10.20: Adding a container\n",
      "2. Create a new function that will have a Blob Storage trigger and output CosmosDB \n",
      "binding. The value returned from the function will be the size of the data for \n",
      "the uploaded file. This returned value will be written to Cosmos DB. The output \n",
      "binding will write to the Cosmos DB collection. Navigate to the Integrate tab  \n",
      "and click on the New Output button below the Outputs label and select  \n",
      "Azure Cosmos DB:\n",
      "---------------\n",
      "Creating a connected architecture with functions | 331\n",
      "Figure 10.21: Binding output to Azure Cosmos DB\n",
      "3. Provide the appropriate names for the database and collection (check the \n",
      "checkbox to create the collection if it does not exist), click on the New button \n",
      "to select your newly created Azure Cosmos DB, and leave the parameter name \n",
      "as outputDocument:\n",
      "Figure 10.22: Newly created Azure Cosmos DB\n",
      "---------------\n",
      "332 | Serverless in Azure – Working with Azure Functions\n",
      "4. Modify the function as shown in Figure 10.23:\n",
      " Figure 10.23: Modifying the function\n",
      "5. Now, uploading a new file to the orders collection in the Azure Storage account \n",
      "will execute a function that will write to the Azure Cosmos DB collection. Another \n",
      "function can be written with the newly created Azure Cosmos DB account as a \n",
      "trigger binding. It will provide the size of files and the function can act on it. This \n",
      "is shown here:\n",
      "Figure 10.24: Writing a trigger binding function\n",
      "This section covered how the output of one function can be used to retrieve data for \n",
      "the next function. In the next section, you will learn about how to enable serverless \n",
      "eventing by understanding about Azure Event Grid.\n",
      "Azure Event Grid\n",
      "Azure Event Grid is a relatively new service. It has also been referred to as a serverless \n",
      "eventing platform. It helps with the creation of applications based on events (also\n",
      "---------------\n",
      "known as event-driven design). It is important to understand what events are and how \n",
      "we dealt with them prior to Event Grid. An event is something that happened – that \n",
      "is, an activity that changed the state of a subject. When a subject undergoes a change in \n",
      "its state, it generally raises an event.\n",
      "---------------\n",
      "Azure Event Grid | 333\n",
      "Events typically follow the publish/subscribe pattern (also popularly known as the pub/\n",
      "sub pattern), in which a subject raises an event due to its state change, and that event \n",
      "can then be subscribed to by multiple interested parties, also known as subscribers. \n",
      "The job of the event is to notify the subscribers of such changes and also provide them \n",
      "with data as part of its context. The subscribers can take whatever action they deem \n",
      "necessary, which varies from subscriber to subscriber.\n",
      "Prior to Event Grid, there was no service that could be described as a real-time event \n",
      "platform. There were separate services, and each provided its own mechanism for \n",
      "handling events.\n",
      "For example, Log Analytics, also known as Operations Management Suite (OMS), \n",
      "provides an infrastructure for capturing environment logs and telemetry on which \n",
      "alerts can be generated. These alerts can be used to execute a runbook, a webhook, or\n",
      "---------------\n",
      "a function. This is near to real time, but they are not completely real time. Moreover, \n",
      "it was quite cumbersome to trap individual logs and act on them. Similarly, there \n",
      "is Application Insights, which provides similar features to Log Analytics but for \n",
      "applications instead.\n",
      "There are other logs, such as activity logs and diagnostic logs, but again, they rely on \n",
      "similar principles as other log-related features. Solutions are deployed on multiple \n",
      "resource groups in multiple regions, and events raised from any of these should be \n",
      "available to the resources that are deployed elsewhere.\n",
      "Event Grid removes all barriers, and as a result, events can be generated by most \n",
      "resources (they are increasingly becoming available), and even custom events can be \n",
      "generated. These events can then be subscribed to by any resource, in any region, and \n",
      "in any resource group within the subscription.\n",
      "Event Grid is already laid down as part of the Azure infrastructure, along with data\n",
      "---------------\n",
      "centers and networks. Events raised in one region can easily be subscribed to by \n",
      "resources in other regions, and since these networks are connected, it is extremely \n",
      "efficient for the delivery of events to subscribers.\n",
      "Event Grid\n",
      "Event Grid lets you create applications with event-based architecture. There are \n",
      "publishers of events and there are consumers of events; however, there can be multiple \n",
      "subscribers for the same event.\n",
      "The publisher of an event can be an Azure resource, such as Blob storage, Internet of \n",
      "Things (IoT) hubs, and many others. These publishers are also known as event sources. \n",
      "These publishers use out-of-the-box Azure topics to send their events to Event Grid. \n",
      "There is no need to configure either the resource or the topic. The events raised by \n",
      "Azure resources are already using topics internally to send their events to Event Grid. \n",
      "Once the event reaches the grid, it can be consumed by the subscribers.\n",
      "---------------\n",
      "334 | Serverless in Azure – Working with Azure Functions\n",
      "The subscribers, or consumers, are resources who are interested in events and want to \n",
      "execute an action based on these events. These subscribers provide an event handler \n",
      "when they subscribe to the topic. The event handlers can be Azure functions, custom \n",
      "webhooks, logic apps, or other resources. Both the event sources and subscribers that \n",
      "execute event handlers are shown in Figure 10.25:\n",
      "Figure 10.25: The Event Grid architecture\n",
      "When an event reaches a topic, multiple event handlers can be executed \n",
      "simultaneously, each taking its own action.\n",
      "It is also possible to raise a custom event and send a custom topic to Event Grid. Event \n",
      "Grid provides features for creating custom topics, and these topics are automatically \n",
      "attached to Event Grid. These topics know the storage for Event Grid and automatically \n",
      "send their messages to it. Custom topics have two important properties, as follows:\n",
      "---------------\n",
      "• An endpoint: This is the endpoint of the topic. Publishers and event sources use \n",
      "this endpoint to send and publish their events to Event Grid. In other words, \n",
      "topics are recognized using their endpoints.\n",
      "• Keys: Custom topics provide a couple of keys. These keys enable security for \n",
      "the consumption of the endpoint. Only publishers with these keys can send and \n",
      "publish their messages to Event Grid.\n",
      "Media Services\n",
      "Blob Storage\n",
      "Azure Subscriptions\n",
      "Resource Groups\n",
      "IoT Hub\n",
      "Event Hubs\n",
      "Service Bus\n",
      "Custom Topics\n",
      "Azure Functions\n",
      "Logic Apps\n",
      "Azure Automation\n",
      "Queue Storage\n",
      "Webhooks\n",
      "Hybrid Connections\n",
      "Event Hubs\n",
      "Event Sources Event Handlers\n",
      "Event SubscriptionsTopics\n",
      "---------------\n",
      "Azure Event Grid | 335\n",
      "Each event has an event type and it is recognized by it. For example, Blob \n",
      "storage provides event types, such as blobAdded and blobDeleted. Custom topics \n",
      "can be used to send a custom-defined event, such as a custom event of the \n",
      "KeyVaultSecretExpired type.\n",
      "On the other hand, subscribers have the ability to accept all messages or only get \n",
      "events based on filters. These filters can be based on the event type or other properties \n",
      "within the event payload.\n",
      "Each event has at least the following five properties:\n",
      "• id: This is the unique identifier for the event.\n",
      "• eventType: This is the event type.\n",
      "• eventTime: This is the date and time when the event was raised.\n",
      "• subject: This is a short description of the event.\n",
      "• data: This is a dictionary object and contains either resource-specific data or any \n",
      "custom data (for custom topics).\n",
      "Currently, Event Grid's functionalities are not available with all resources; however,\n",
      "---------------\n",
      "Azure is continually adding more and more resources with Event Grid functionality.\n",
      "To find out more about the resources that can raise events related to Event Grid and \n",
      "handlers that can handle these events, please go to https:/ /docs.microsoft.com/azure/\n",
      "event-grid/overview.\n",
      "Resource events\n",
      "In this section, the following steps are provided to create a solution in which events \n",
      "that are raised by Blob storage are published to Event Grid and ultimately routed to an \n",
      "Azure function:\n",
      "1. Log in to the Azure portal using the appropriate credentials and create a new \n",
      "Storage account in an existing or a new resource group. The Storage account \n",
      "should be either StorageV2 or Blob storage. As demonstrated in Figure 10.26, Event \n",
      "Grid will not work with StorageV1:\n",
      "---------------\n",
      "336 | Serverless in Azure – Working with Azure Functions\n",
      "Figure 10.26: Creating a new storage account\n",
      "2. Create a new function app or reuse an existing function app to create an Azure \n",
      "function. The Azure function will be hosted within the function app.\n",
      "3. Create a new function using the Azure Event Grid trigger template. Install the \n",
      "Microsoft.Azure.WebJobs.Extensions.EventGrid extension if it's not already \n",
      "installed, as shown in Figure 10.27:\n",
      "---------------\n",
      "Azure Event Grid | 337\n",
      "Figure 10.27: Installing extensions for an Azure Event Grid trigger\n",
      "4. Name the StorageEventHandler function and create it. The following default \n",
      "generated code will be used as the event handler:\n",
      "Figure 10.28: Event handler code\n",
      "The subscription to Storage events can be configured either from the Azure \n",
      "Functions user interface (UI) by clicking on Add Event Grid subscription, or from \n",
      "the storage account itself.\n",
      "---------------\n",
      "338 | Serverless in Azure – Working with Azure Functions\n",
      "5. Click on the Add Event Grid subscription link in the Azure Functions UI to add a \n",
      "subscription to the events raised by the storage account created in the previous \n",
      "step. Provide a meaningful name for the subscription, and then choose Event \n",
      "Schema followed by Event Grid Schema. Set Topic Types as Storage Accounts, \n",
      "set an appropriate Subscription, and the resource group containing the storage \n",
      "account:\n",
      "Figure 10.29: Creating an Event Grid subscription\n",
      "Ensure that the Subscribe to all event types checkbox is checked and click on the \n",
      "Create button (it should be enabled as soon as a storage account is selected).\n",
      "---------------\n",
      "Azure Event Grid | 339\n",
      "6. If we now navigate to the storage account in the Azure portal and click on \n",
      "the Events link in the left-hand menu, the subscription for the storage account \n",
      "should be visible:\n",
      "Figure 10.30:  Event subscription list\n",
      "---------------\n",
      "340 | Serverless in Azure – Working with Azure Functions\n",
      "7. Upload a file to the Blob storage after creating a container, and the Azure \n",
      "function should be executed. The upload action will trigger a new event of \n",
      "the blobAdded type and send it to the Event Grid topic for storage accounts. As \n",
      "shown in Figure 10.31, the subscription is already set to get all the events from this \n",
      "topic, and the function gets executed as part of the event handler:\n",
      "Figure 10.31:  Triggering a new event\n",
      "In this section, you learned how events raised by Blob storage can be routed to an Azure \n",
      "function. In the next section, you will learn how to leverage custom events.\n",
      "Custom events\n",
      "In this example, instead of using out-of-box resources to generate events, custom \n",
      "events will be used. We will use PowerShell to create this solution and reuse the same \n",
      "Azure function that was created in the last exercise as the handler:\n",
      "1. Log in and connect to your Azure subscription of choice using Login-AzAccount\n",
      "---------------\n",
      "and Set-AzContext cmdlet.\n",
      "2. The next step is to create a new Event Grid topic in Azure in a resource group. \n",
      "The New-AzEventGridTopic cmdlet is used to create a new topic:\n",
      "New-AzEventGridTopic -ResourceGroupName CustomEventGridDemo -Name \n",
      "\"KeyVaultAssetsExpiry\" -Location \"West Europe\"\n",
      "---------------\n",
      "Azure Event Grid | 341\n",
      "3. Once the topic is created, its endpoint URL and key should be retrieved as they \n",
      "are needed to send and publish the event to it. The Get-AzEventGridTopic and \n",
      "Get-AzEventGridTopicKey cmdlets are used to retrieve these values. Note that Key1 \n",
      "is retrieved to connect to the endpoint:\n",
      "$topicEndpoint = (Get-AzEventGridTopic -ResourceGroupName containers -Name \n",
      "KeyVaultAssetsExpiry).Endpoint \n",
      " \n",
      "$keys = (Get-AzEventGridTopicKey -ResourceGroupName containers -Name \n",
      "KeyVaultAssetsExpiry).Key1\n",
      "4. A new hash table is created with all five important Event Grid event properties. A \n",
      "new id property is generated for the ID, the subject property is set to Key vault \n",
      "Asset Expiry, eventType is set to Certificate Expiry, eventTime is set to the \n",
      "current time, and data contains information regarding the certificate:\n",
      "$eventgridDataMessage = @{ \n",
      "id = [System.guid]::NewGuid() \n",
      "subject = \"Key Vault Asset Expiry\" \n",
      "eventType = \"Certificate Expiry\"\n",
      "---------------\n",
      "eventTime = [System.DateTime]::UtcNow \n",
      "data = @{ \n",
      "CertificateThumbprint = \"sdfervdserwetsgfhgdg\" \n",
      "ExpiryDate = \"1/1/2019\" \n",
      "Createdon = \"1/1/2018\" \n",
      "} \n",
      "}\n",
      "5. Since Event Grid data should be published in the form of a JSON array, the payload \n",
      "is converted in the JSON array. The \"[\",\"]\" square brackets represent a JSON \n",
      "array:\n",
      "$finalBody = \"[\" + $(ConvertTo-Json $eventgridDataMessage) + \"]\"\n",
      "6. The event will be published using the HTTP protocol, and the appropriate \n",
      "header information has to be added to the request. The request is sent using the \n",
      "application/JSON content type and the key belonging to the topic is assigned \n",
      "to the aeg-sas-key header. It is mandatory to name the header and key set to \n",
      "aeg-sas-key:\n",
      "$header = @{ \n",
      "\"contentType\" = \"application/json\" \n",
      "\"aeg-sas-key\" = $keys}\n",
      "---------------\n",
      "342 | Serverless in Azure – Working with Azure Functions\n",
      "7. A new subscription is created to the custom topic with a name, the resource \n",
      "group containing the topic, the topic name, the webhook endpoint, and the actual \n",
      "endpoint that acts as the event handler. The event handler in this case is the Azure \n",
      "function:\n",
      "New-AzEventGridSubscription -TopicName KeyVaultAssetsExpiry \n",
      "-EventSubscriptionName \"customtopicsubscriptionautocar\" -ResourceGroupName \n",
      "CustomEventGridDemo -EndpointType webhook ' \n",
      "-Endpoint \"https://durablefunctiondemoapp.\n",
      "azurewebsites.net/runtime/webhooks/\n",
      "EventGrid?functionName=StorageEventHandler&code=0aSw6sxvtFmafXHvt7iOw/\n",
      "Dsb8o1M9RKKagzVchTUkwe9EIkzl4mCg==' \n",
      "-Verbose\n",
      "The URL of the Azure function is available from the Integrate tab, as shown in \n",
      "Figure 10.31:\n",
      "Figure 10.32: Event Grid Subscription URL in the Integrate tab\n",
      "8. By now, both the subscriber (event handler) and the publisher have been\n",
      "---------------\n",
      "configured. The next step is to send and publish an event to the custom topic. \n",
      "The event data was already created in the previous step and, by using the Invoke-\n",
      "WebRequest cmdlet, the request is sent to the endpoint along with the body and the \n",
      "header:\n",
      "Invoke-WebRequest -Uri $topicEndpoint -Body $finalBody -Headers $header \n",
      "-Method Post\n",
      "The API call will trigger the event and the Event Grid will message the endpoint we \n",
      "configured, which is the function app. With this activity, we are winding up this chapter.\n",
      "---------------\n",
      "Summary | 343\n",
      "Summary\n",
      "The evolution of functions from traditional methods has led to the design of the loosely \n",
      "coupled, independently evolving, self-reliant serverless architecture that was only a \n",
      "concept in earlier days. Functions are a unit of deployment and provide an environment \n",
      "that does not need to be managed by the user at all. All they have to care about is \n",
      "the code written for the functionality. Azure provides a mature platform for hosting \n",
      "functions and integrating them seamlessly, based on events or on demand. Nearly every \n",
      "resource in Azure can participate in an architecture composed of Azure functions. The \n",
      "future is functions, as more and more organizations want to stay away from managing \n",
      "infrastructures and platforms. They want to offload this to cloud providers. Azure \n",
      "Functions is an essential feature to master for every architect dealing with Azure.\n",
      "This chapter went into the details of Azure Functions, Functions as a Service, Durable\n",
      "---------------\n",
      "Functions, and Event Grid. The next chapter will focus on Azure Logic Apps, and we will \n",
      "build a complete end-to-end solution combining multiple serverless services along with \n",
      "other Azure services, such as Azure Key Vault and Azure Automation.\n",
      "---------------\n",
      "This chapter continues from the previous chapter and will go into further depth about \n",
      "serverless services available within Azure. In the previous chapter, you learned in detail \n",
      "about Azure Functions, functions as a service, Durable Functions, and Event Grid. Going \n",
      "forward, this chapter will focus on understanding Logic Apps and then move on to \n",
      "creating a complete end-to-end serverless solution that combines multiple serverless \n",
      "and other kinds of services, such as Key Vault and Azure Automation.\n",
      "Azure solutions \n",
      "using Azure Logic \n",
      "Apps, Event Grid, and \n",
      "Functions\n",
      "11\n",
      "---------------\n",
      "346 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "In this chapter, we will further explore Azure services by covering the following topics:\n",
      "• Azure Logic Apps\n",
      "• Creating an end-to-end solution using serverless technologies\n",
      "Azure Logic Apps  \n",
      "Logic Apps is a serverless workflow offering from Azure. It has all the features of \n",
      "serverless technologies, such as consumption-based costing and unlimited scalability. \n",
      "Logic Apps helps us to build a business process and workflow solution with ease using \n",
      "the Azure portal. It provides a drag-and-drop UI to create and configure workflows.\n",
      "Using Logic Apps is the preferred way to integrate services and data, create business \n",
      "projects, and create a complete flow of logic. There are several important concepts that \n",
      "should be understood before building a logic app.\n",
      "Activities\n",
      "An activity is a single unit of work. Examples of activities include converting XML\n",
      "---------------\n",
      "to JSON, reading blobs from Azure Storage, and writing to a Cosmos DB document \n",
      "collection. Logic Apps provides a workflow definition consisting of multiple co-related \n",
      "activities in a sequence. There are two types of activity in Logic Apps:\n",
      "• Trigger: A trigger refers to the initiation of an activity. All logic apps have a single \n",
      "trigger that forms the first activity. It is the trigger that creates an instance of the \n",
      "logic app and starts the execution. Examples of triggers are the arrival of Event \n",
      "Grid messages, an email, an HTTP request, or a schedule.\n",
      "• Actions: Any activity that is not a trigger is a step activity, and each of them is \n",
      "responsible to perform one task. Steps are connected to each other in a workflow. \n",
      "Each step will have an action that needs to be completed before going to the next \n",
      "step.\n",
      "Connectors\n",
      "Connectors are Azure resources that help connect a logic app to external services.\n",
      "---------------\n",
      "These services can be in the cloud or on-premises. For example, there is a connector \n",
      "for connecting logic apps to Event Grid. Similarly, there is another connector to \n",
      "connect to Office 365 Exchange. Almost all types of connectors are available in Logic \n",
      "Apps, and they can be used to connect to services. Connectors contain connection \n",
      "information and also logic to connect to external services using this connection \n",
      "information.\n",
      "---------------\n",
      "Azure Logic Apps   | 347\n",
      "The entire list of connectors is available at https:/ /docs.microsoft.com/connectors.\n",
      "Now that you know about connectors, you need to understand how they can be aligned \n",
      "in a step-by-step manner to make the workflow work as expected. In the next section, \n",
      "we will be focusing on the workings of a logic app.\n",
      "The workings of a logic app\n",
      "Let's create a Logic Apps workflow that gets triggered when an email account receives \n",
      "an email. It replies to the sender with a default email and performs sentiment analysis \n",
      "on the content of the email. For sentiment analysis, the Text Analytics resource from \n",
      "Cognitive Services should be provisioned before creating the logic app:\n",
      "1. Navigate to the Azure portal, log in to your account, and create a Text Analytics \n",
      "resource in a resource group. Text Analytics is part of Cognitive Services and \n",
      "has features such as sentiment analysis, key phrase extraction, and language\n",
      "---------------\n",
      "detection. You can find the service in the Azure portal, as shown in Figure 11.1:\n",
      "Figure 11.1: Navigating to the Text Analytics service from the Azure portal\n",
      "2. Provide the Name, Location, Subscription, Resource group, and Pricing tier \n",
      "values. We'll be using the free tier (F0 tier) of this service for this demo.\n",
      "3. Once the resource is provisioned, navigate to the Overview page, and copy the \n",
      "endpoint URL. Store it in a temporary location. This value will be required when \n",
      "configuring the logic app.\n",
      "---------------\n",
      "348 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "4. Navigate to the Keys page and copy the value from Key 1 and store it in a \n",
      "temporary location. This value will be needed when configuring the logic app.\n",
      "5. The next step is to create a logic app. To create a logic app, navigate to the \n",
      "resource group in the Azure portal in which the logic app should be created. \n",
      "Search for Logic App and create it by providing Name, Location, Resource group, \n",
      "and Subscription values.\n",
      "6. After the logic app has been created, navigate to the resource, click on Logic app \n",
      "designer in the left-hand menu, and then select the When a new email is received \n",
      "in Outlook.com template to create a new workflow. The template provides a \n",
      "head start by adding boilerplate triggers and activities. This will add an Office 365 \n",
      "Outlook trigger automatically to the workflow.\n",
      "7. Click on the Sign in button on the trigger; it will open a new Internet Explorer\n",
      "---------------\n",
      "window. Then, sign in to your account. After successfully signing in, a new Office \n",
      "365 mail connector will be created, containing the connection information to the \n",
      "account.\n",
      "8. Click on the Continue button and configure the trigger with a 3-minute poll \n",
      "frequency, as shown in Figure 11.2:\n",
      "Figure 11.2: Configuring the trigger with a 3-minute poll frequency\n",
      "---------------\n",
      "Azure Logic Apps   | 349\n",
      "9. Click on Next step to add another action and type the keyword variable in \n",
      "the search bar. Then, select the Initialize variable action, as demonstrated in \n",
      "Figure 11.3:\n",
      "Figure 11.3: Adding the Initialize variable action\n",
      "---------------\n",
      "350 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "10. Next, configure the variable action. When the Value box is clicked on, a pop-up \n",
      "window appears that shows Dynamic content and Expression. Dynamic content \n",
      "refers to properties that are available to the current action and are filled with \n",
      "runtime values from previous actions and triggers. Variables help in keeping \n",
      "workflows generic. From this window, select Body from Dynamic content:\n",
      "Figure 11.4: Configuring the variable action\n",
      "11. Add another action by clicking on Add step, typing outlook in the search bar, and \n",
      "then selecting the Reply to email action:\n",
      "Figure 11.5: Adding the Reply to email action\n",
      "---------------\n",
      "Azure Logic Apps   | 351\n",
      "12. Configure the new action. Ensure that Message Id is set with the dynamic \n",
      "content, Message Id, and then type the reply in the Comment box that you'd like \n",
      "to send to the recipient:\n",
      "Figure 11.6: Configuring the Reply to email action\n",
      "13. Add another action, type text analytics in the search bar, and then select Detect \n",
      "Sentiment (preview):\n",
      "Figure 11.7: Adding the Detect Sentiment (preview) action\n",
      "---------------\n",
      "352 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "14. Configure the sentiment action as shown in Figure 11.8—both the endpoint and key \n",
      "values should be used here. Now click on the Create button, as demonstrated in \n",
      "Figure 11.8:\n",
      "Figure 11.8: Configuring the Detect Sentiment (preview) action\n",
      "15. Provide the text to the action by adding dynamic content and selecting the \n",
      "previously created variable, emailContent. Then, click on Show advanced options \n",
      "and select en for Language:\n",
      "Figure 11.9: Selecting the language for the sentiment action\n",
      "16. Next, add a new action by selecting Outlook, and then select Send an email. This \n",
      "action sends the original recipient the email content with the sentiment score \n",
      "in its subject. It should be configured as shown in Figure 11.10. If the score is not \n",
      "visible in the dynamic content window, click on the See more link beside it:\n",
      "---------------\n",
      "Azure Logic Apps   | 353\n",
      "Figure 11.10: Adding the Send an email action\n",
      "17. Save the logic app, navigate back to the overview page, and click on Run trigger. \n",
      "The trigger will check for new emails every 3 minutes, reply to the senders, \n",
      "perform sentiment analysis, and send an email to the original recipient. A sample \n",
      "email with negative connotations is sent to the given email ID:\n",
      "Figure 11.11: Sample email\n",
      "---------------\n",
      "354 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "18. After a few seconds, the logic app executes, and the sender gets the following \n",
      "reply:\n",
      "Figure 11.12: Reply email to the original sender \n",
      "19. The original recipient gets an email with the sentiment score and the original \n",
      "email text, as shown in Figure 11.13:\n",
      "Figure 11.13: HTML view of the email message\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 355\n",
      "From the activity, we were able to understand the workings of a logic app. The app was \n",
      "triggered when an email was received in the inbox of the user and the process followed \n",
      "the sequence of steps that were given in the logic app. In the next section, you will learn \n",
      "how to create an end-to-end solution using serverless technologies.\n",
      "Creating an end-to-end solution using serverless technologies\n",
      "In this section, we will create an end-to-end solution comprising serverless \n",
      "technologies that we discussed in the previous sections. The following example will give \n",
      "you an idea of how workflows can be intelligently implemented to avoid management \n",
      "overhead. In the next activity, we will create a workflow to notify the users when the \n",
      "keys, secrets, and certificates get stored in Azure Key Vault. We will take this as a \n",
      "problem statement, figure out a solution, architect the solution, and implement it.\n",
      "The problem statement\n",
      "---------------\n",
      "The problem that we are going to solve here is that users and organizations are not \n",
      "notified regarding the expiration of secrets in their key vault, and applications stop \n",
      "working when they expire. Users are complaining that Azure does not provide the \n",
      "infrastructure to monitor Key Vault secrets, keys, and certificates.\n",
      "Solution\n",
      "The solution to this problem is to combine multiple Azure services and integrate them \n",
      "so that users can be proactively notified of the expiration of secrets. The solution will \n",
      "send notifications using two channels—email and SMS.\n",
      "The Azure services used to create this solution include the following:\n",
      "• Azure Key Vault\n",
      "• Azure Active Directory (Azure AD)\n",
      "• Azure Event Grid\n",
      "• Azure Automation\n",
      "• Logic Apps\n",
      "• Azure Functions\n",
      "• SendGrid\n",
      "• Twilio SMS\n",
      "Now that you know the services that will be used as part of the solution, let's go ahead \n",
      "and create an architecture for this solution.\n",
      "---------------\n",
      "356 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "Architecture\n",
      "In the previous section, we explored the list of services that will be used in the solution. \n",
      "If we want to implement the solution, the services should be laid out in the proper \n",
      "order. The architecture will help us to develop the workflow and take a step closer to \n",
      "the solution.\n",
      "The architecture of the solution comprises multiple services, as shown in Figure 11.14:\n",
      "Figure 11.14: Solution architecture\n",
      "Let's go through each of these services and understand their roles and the functionality \n",
      "that they provide in the overall solution.\n",
      "Azure Automation\n",
      "Azure Automation provides runbooks, and these runbooks can be executed to run logic \n",
      "using PowerShell, Python, and other scripting languages. Scripts can be executed either \n",
      "on-premises or in the cloud, which provides rich infrastructure and facilities to create \n",
      "scripts. These kinds of scripts are known as runbooks. Typically, runbooks implement\n",
      "---------------\n",
      "a scenario such as stopping or starting a virtual machine, or creating and configuring \n",
      "storage accounts. It is quite easy to connect to the Azure environment from runbooks \n",
      "with the help of assets such as variables, certificates, and connections.\n",
      "In the current solution, we want to connect to Azure Key Vault, read all the secrets \n",
      "and keys stored within it, and fetch their expiry dates. These expiry dates should be \n",
      "compared with today's date and, if the expiry date is within a month, the runbook \n",
      "should raise a custom event on Event Grid using an Event Grid custom topic.\n",
      "An Azure Automation runbook using a PowerShell script will be implemented to achieve \n",
      "this. Along with the runbook, a scheduler will also be created that will execute the \n",
      "runbook once a day at 12.00 AM.\n",
      "PowerShell\n",
      "runbookAzure\n",
      "Automation\n",
      "schedule Certificates\n",
      "Administrator/\n",
      "stakeholder\n",
      "groups\n",
      "Event Grid\n",
      "Key Vault Secrets\n",
      "logic app\n",
      "email\n",
      "Access\n",
      "Subscribe\n",
      "sPublishes\n",
      "SMS\n",
      "Runs\n",
      "once\n",
      "every day\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 357\n",
      "A custom Azure Event Grid topic\n",
      "Once the runbook identifies that a secret or key is going to expire within a month, it will \n",
      "raise a new custom event and publish it to the custom topic created specifically for this \n",
      "purpose. Again, we will go into the details of the implementation in the next section.\n",
      "Azure Logic Apps\n",
      "A logic app is a serverless service that provides workflow capabilities. Our logic app will \n",
      "be configured to be triggered as and when an event is published on the custom Event \n",
      "Grid topic. After it is triggered, it will invoke the workflow and execute all the activities \n",
      "in it one after another. Generally, there are multiple activities, but for the purpose of \n",
      "this example, we will invoke one Azure function that will send both email and SMS \n",
      "messages. In a full-blown implementation, these notification functions should be \n",
      "implemented separately in separate Azure functions.\n",
      "Azure Functions\n",
      "---------------\n",
      "Azure Functions is used to notify users and stakeholders about the expiration of secrets \n",
      "and keys using email and SMS. SendGrid is used to send emails, while Twilio is used to \n",
      "send SMS messages from Azure Functions.  \n",
      "In the next section, we will take a look at the prerequisites before implementing the \n",
      "solution.\n",
      "Prerequisites\n",
      "You will need an Azure subscription with contributor rights at the very least. As \n",
      "we are only deploying services to Azure and no external services are deployed, the \n",
      "subscription is the only prerequisite. Let's go ahead and implement the solution.\n",
      "Implementation\n",
      "A key vault should already exist. If not, one should be created.\n",
      "This step should be performed if a new Azure Key Vault instance needs to be \n",
      "provisioned. Azure provides multiple ways in which to provision resources. Prominent \n",
      "among them are Azure PowerShell and the Azure CLI. The Azure CLI is a command-line \n",
      "interface that works across platforms. The first task will be to provision a key vault in\n",
      "---------------\n",
      "Azure. In this implementation, we will use Azure PowerShell to provision the key vault.\n",
      "Before Azure PowerShell can be used to create a key vault, it is important to log into \n",
      "Azure so that subsequent commands can be executed successfully to create the key \n",
      "vault.\n",
      "---------------\n",
      "358 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "Step 1: Provisioning an Azure Key Vault instance\n",
      "The first step is to prepare the environment for the sample. This involves logging into \n",
      "the Azure portal, selecting an appropriate subscription, and then creating a new Azure \n",
      "resource group and a new Azure Key Vault resource:\n",
      "1. Execute the Connect-AzAccount command to log into Azure. It will prompt for \n",
      "credentials in a new window.\n",
      "2. After a successful login, if there are multiple subscriptions available for the \n",
      "login ID provided, they will all be listed. It is important to select an appropriate \n",
      "subscription—this can be done by executing the Set-AzContext cmdlet:\n",
      "Set-AzContext -SubscriptionId xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxx\n",
      "3. Create a new resource group in your preferred location. In this case, the name of \n",
      "the resource group is IntegrationDemo and it is created in the West Europe region:\n",
      "---------------\n",
      "New-AzResourceGroup -Name IntegrationDemo -Location \"West Europe\" -Verbose\n",
      "4. Create a new Azure Key Vault resource—the name of the vault, in this case, \n",
      "is keyvaultbook, and it is enabled for deployment, template deployment, disk \n",
      "encryption, soft delete, and purge protection:\n",
      "New-AzKeyVault -Name keyvaultbook -ResourceGroupName \n",
      "IntegrationDemo -Location \"West Europe\" -EnabledForDeployment \n",
      "-EnabledForTemplateDeployment -EnabledForDiskEncryption \n",
      "-EnablePurgeProtection -Sku Standard - Verbose\n",
      "Please note that the key vault name needs to be unique. You may not be able to use the \n",
      "same name for two key vaults. The preceding command, when executed successfully, \n",
      "will create a new Azure Key Vault resource. The next step is to provide access to a \n",
      "service principal on the key vault.\n",
      "Step 2: Creating a service principal\n",
      "Instead of using an individual account to connect to Azure, Azure provides service \n",
      "principals, which are, in essence, service accounts that can be used to connect to\n",
      "---------------\n",
      "Azure Resource Manager and run activities. Adding a user to an Azure directory/tenant \n",
      "makes them available everywhere, including in all resource groups and resources, due \n",
      "to the nature of security inheritance in Azure. Access must be explicitly revoked from \n",
      "resource groups for users if they are not allowed to access them. Service principals \n",
      "help by assigning granular access and control to resource groups and resources, and, if \n",
      "required, they can be given access to the subscription scope. They can also be assigned \n",
      "granular permissions, such as reader, contributor, or owner permissions.\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 359\n",
      "In short, service principals should be the preferred mechanism to consume Azure \n",
      "services. They can be configured either with a password or with a certificate key. \n",
      "Service principals can be created using the New-AzAdServicePrinicipal command, as \n",
      "shown here:\n",
      "$sp = New-AzADServicePrincipal -DisplayName \"keyvault-book\" -Scope \"/\n",
      "subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" -Role Owner -StartDate \n",
      "([datetime]::Now) -EndDate $([datetime]::now.AddYears(1)) -Verbose\n",
      "The important configuration values are the scope and role. The scope determines the \n",
      "access area for the service application—it is currently shown at the subscription level. \n",
      "Valid values for scope are as follows:\n",
      "/subscriptions/{subscriptionId} \n",
      "/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName} \n",
      "/subscriptions/{subscriptionId}/resourcegroups/{resourceGroupName}/providers/\n",
      "{resourceProviderNamespace}/{resourceType}/{resourceName}\n",
      "---------------\n",
      "/subscriptions/{subscriptionId}/resourcegroups/{resourceGroupName}/\n",
      "providers/{resourceProviderNamespace}/{parentResourcePath}/{resourceType}/\n",
      "{resourceName}\n",
      "The role provides permissions to the assigned scope. The valid values are as follows:\n",
      "• Owner\n",
      "• Contributor\n",
      "• Reader\n",
      "• Resource-specific permissions\n",
      "In the preceding command, owner permissions have been provided to the newly \n",
      "created service principal.\n",
      "We can also use certificates if needed. For simplicity, we will proceed with the \n",
      "password.\n",
      "With the service principal we created, the secret will be hidden. To find out the secret, \n",
      "you can try the following commands:\n",
      "$BSTR = [System.Runtime.InteropServices.Marshal]::SecureStringToBSTR($sp. \n",
      "Secret) \n",
      "$UnsecureSecret = [System.Runtime.InteropServices.\n",
      "Marshal]::PtrToStringAuto($BSTR)\n",
      "$UnsecureSecret will have your secret key.\n",
      "---------------\n",
      "360 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "Along with the service principal, an Application Directory application will be created. \n",
      "The application acts as the global representation of our application across directories \n",
      "and the principal is like a local representation of the application. We can create multiple \n",
      "principals using the same application in a different directory. We can get the details \n",
      "of the application created using the Get-AzAdApplication command. We will save the \n",
      "output of this command to a variable, $app, as we will need this later:\n",
      "$app = Get-AzAdApplication -DisplayName $sp.DisplayName\n",
      "We have now created a service principal using a secret; another secure way of creating \n",
      "one is using certificates. In the next section, we will create a service principal using \n",
      "certificates.\n",
      "Step 3: Creating a service principal using certificates\n",
      "To create a service principal using certificates, the following steps should be executed:\n",
      "---------------\n",
      "1. Create a self-signed certificate or purchase a certificate: A self-signed certificate \n",
      "is used to create this example end-to-end application. For real-life deployments, a \n",
      "valid certificate should be purchased from a certificate authority.\n",
      "To create a self-signed certificate, the following command can be run. The \n",
      "self-signed certificate is exportable and stored in a personal folder on the local \n",
      "machine—it also has an expiry date:\n",
      "$currentDate = Get-Date \n",
      "$expiryDate = $currentDate.AddYears(1) \n",
      "$finalDate = $expiryDate.AddYears(1) \n",
      "$servicePrincipalName = \"https://automation.book.com\" \n",
      "$automationCertificate = New-SelfSignedCertificate -DnsName \n",
      "$servicePrincipalName -KeyExportPolicy Exportable -Provider \"Microsoft \n",
      "Enhanced RSA and AES Cryptographic Provider\" -NotAfter $finalDate \n",
      "-CertStoreLocation \"Cert:\\LocalMachine\\My\"\n",
      "2. Export the newly created certificate: The new certificate must be exported to the\n",
      "---------------\n",
      "filesystem so that later, it can be uploaded to other destinations, such as Azure AD, \n",
      "to create a service principal.\n",
      "The commands used to export the certificate to the local filesystem are shown \n",
      "next. Please note that this certificate has both public and private keys, and so \n",
      "while it is exported, it must be protected using a password, and the password \n",
      "must be a secure string:\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 361\n",
      "$securepfxpwd = ConvertTo-SecureString -String 'password' -AsPlainText \n",
      "-Force # Password for the private key PFX certificate \n",
      "$cert1 = Get-Item -Path Cert:\\LocalMachine\\My\\$($automationCertificate.\n",
      "Thumbprint) \n",
      "Export-PfxCertificate -Password $securepfxpwd -FilePath \" C:\\\n",
      "azureautomation.pfx\" -Cert $cert1\n",
      "The Get-Item cmdlet reads the certificate from the certificate store and stores \n",
      "it in the $cert1 variable. The Export-PfxCertificate cmdlet actually exports the \n",
      "certificate in the certificate store to the filesystem. In this case, it is in the C:\\book \n",
      "folder.\n",
      "3. Read the content from the newly generated PFX file: An object of \n",
      "X509Certificate2 is created to hold the certificate in memory, and the data is \n",
      "converted to a Base64 string using the System.Convert function:\n",
      "$newCert = New-Object System.Security.Cryptography.X509Certificates.\n",
      "X509Certificate2 -ArgumentList \"C:\\azureautomation.pfx\", $securepfxpwd\n",
      "---------------\n",
      "$newcertdata = [System.Convert]::ToBase64String($newCert.GetRawCertData())\n",
      "We will be using this same principal to connect to Azure from the Azure \n",
      "Automation account. It is important that the application ID, tenant ID, subscription \n",
      "ID, and certificate thumbprint values are stored in a temporary location so that \n",
      "they can be used to configure subsequent resources:\n",
      "$adAppName = \"azure-automation-sp\" \n",
      "$ServicePrincipal = New-AzADServicePrincipal -DisplayName $adAppName \n",
      "-CertValue $newcertdata -StartDate $newCert.NotBefore -EndDate $newCert.\n",
      "NotAfter\n",
      "Sleep 10 \n",
      "New-AzRoleAssignment -ServicePrincipalName $ServicePrincipal.ApplicationId \n",
      "-RoleDefinitionName Owner -Scope /subscriptions/xxxxx-xxxxxxx-xxxxxx-\n",
      "xxxxxxx  \n",
      "We have our service principal ready. The key vault we created doesn't have an access \n",
      "policy set, which means no user or application will be able to access the vault. In the \n",
      "next step, we will grant permissions to the Application Directory application we created\n",
      "---------------\n",
      "to access the key vault.\n",
      "Step 4: Creating a key vault policy\n",
      "At this stage, we have created the service principal and the key vault. However, the \n",
      "service principal still does not have access to the key vault. This service principal will \n",
      "be used to query and list all the secrets, keys, and certificates from the key vault, and it \n",
      "should have the necessary permissions to do so.\n",
      "---------------\n",
      "362 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "To provide the newly created service principal permission to access the key vault, we \n",
      "will go back to the Azure PowerShell console and execute the following command:\n",
      "Set-AzKeyVaultAccessPolicy -VaultName keyvaultbook -ResourceGroupName \n",
      "IntegrationDemo -ObjectId $ServicePrincipal.Id -PermissionsToKeys get,list,create \n",
      "-PermissionsToCertificates get,list,import -PermissionsToSecrets get,list -Verbose\n",
      "Referring to the previous command block, take a look at the following points:\n",
      "• Set-AzKeyVaultAccessPolicy provides access permissions to users, groups, and \n",
      "service principals. It accepts the key vault name and the service principal object \n",
      "ID. This object is different from the application ID. The output of the service \n",
      "principal contains an Id property, as shown here:\n",
      "Figure 11.15: Finding the object ID of the service principal\n",
      "• PermissionsToKeys provides access to keys in the key vault, and the get, list, and\n",
      "---------------\n",
      "create permissions are provided to this service principal. There is no write or \n",
      "update permission provided to this principal.\n",
      "• PermissionsToSecrets provides access to secrets in the key vault, and the get and \n",
      "list permissions are provided to this service principal. There is no write or update \n",
      "permission provided to this principal.\n",
      "• PermissionsToCertificates provides access to secrets in the key vault, and get, \n",
      "import, and list permissions are provided to this service principal. There is no \n",
      "write or update permission provided to this principal.\n",
      "At this point, we have configured the service principal to work with the Azure key vault. \n",
      "The next part of the solution is to create an Automation account.\n",
      "Step 5: Creating an Automation account\n",
      "Just like before, we will be using Azure PowerShell to create a new Azure Automation \n",
      "account within a resource group. Before creating a resource group and an Automation\n",
      "---------------\n",
      "account, a connection to Azure should be established. However, this time, use the \n",
      "credentials for the service principal to connect to Azure. The steps are as follows:\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 363\n",
      "1. The command to connect to Azure using the service application is as follows. The \n",
      "value is taken from the variables that we initialized in the previous steps:\n",
      "Login-AzAccount -ServicePrincipal -CertificateThumbprint $newCert.\n",
      "Thumbprint -ApplicationId $ServicePrincipal.ApplicationId -Tenant \"xxxx-\n",
      "xxxxxx-xxxxx-xxxxx\" \n",
      "2. Make sure that you have access by checking Get-AzContext as shown here. Make a \n",
      "note of the subscription ID as it will be needed in subsequent commands:\n",
      "Get-AzContext\n",
      "3. After connecting to Azure, a new resource containing the resources for the \n",
      "solution and a new Azure Automation account should be created. You are naming \n",
      "the resource group VaultMonitoring, and creating it in the West Europe region. You \n",
      "will be creating the remainder of the resources in this resource group as well:\n",
      "$IntegrationResourceGroup = \"VaultMonitoring\" \n",
      "$rgLocation = \"West Europe\"\n",
      "---------------\n",
      "$automationAccountName = \"MonitoringKeyVault\" \n",
      "New-AzResourceGroup -name $IntegrationResourceGroup -Location $rgLocation \n",
      "New-AzAutomationAccount -Name $automationAccountName -ResourceGroupName \n",
      "$IntegrationResourceGroup -Location $rgLocation -Plan Free\n",
      "4. Next, create three automation variables. The values for these, that is, the \n",
      "subscription ID, tenant ID, and application ID, should already be available using the \n",
      "previous steps:\n",
      "New-AzAutomationVariable -Name \"azuresubscriptionid\" \n",
      "-AutomationAccountName $automationAccountName -ResourceGroupName \n",
      "$IntegrationResourceGroup -Value \" xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx \" \n",
      "-Encrypted $true \n",
      " \n",
      "New-AzAutomationVariable -Name \"azuretenantid\" -AutomationAccountName \n",
      "$automationAccountName -ResourceGroupName $IntegrationResourceGroup -Value \n",
      "\" xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx \" -Encrypted $true \n",
      " \n",
      "New-AzutomationVariable -Name \"azureappid\" -AutomationAccountName\n",
      "---------------\n",
      "$automationAccountName -ResourceGroupName $IntegrationResourceGroup -Value \n",
      "\" xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx \" -Encrypted $true\n",
      "---------------\n",
      "364 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "5. Now it's time to upload a certificate, which will be used to connect to Azure from \n",
      "Azure Automation:\n",
      "$securepfxpwd = ConvertTo-SecureString -String 'password' -AsPlainText \n",
      "-Force # Password for the private key PFX certificate \n",
      "New-AzAutomationCertificate -Name \"AutomationCertifcate\" -Path \"C:\\book\\\n",
      "azureautomation.pfx\" -Password $securepfxpwd -AutomationAccountName \n",
      "$automationAccountName -ResourceGroupName $IntegrationResourceGroup\n",
      "6. The next step is to install PowerShell modules related to Key Vault and Event Grid \n",
      "in the Azure Automation account, as these modules are not installed by default.\n",
      "7. From the Azure portal, navigate to the already-created VaultMonitoring resource \n",
      "group by clicking on Resource Groups in the left-hand menu.\n",
      "8. Click on the already-provisioned Azure Automation account, MonitoringKeyVault, \n",
      "and then click on Modules in the left-hand menu. The Event Grid module is\n",
      "---------------\n",
      "dependent on the Az.profile module, and so we have to install it before the Event \n",
      "Grid module.\n",
      "9. Click on Browse Gallery in the top menu and type Az.profile in the search box, as \n",
      "shown in Figure 11.16:\n",
      " Figure 11.16: The Az.Profile module in the module gallery\n",
      "10. From the search results, select Az.Profile and click on the Import button in \n",
      "the top menu. Finally, click on the OK button. This step takes a few seconds to \n",
      "complete. After a few seconds, the module should be installed.\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 365\n",
      "11. The status of the installation can be checked from the Module menu item. \n",
      "Figure 11.17 demonstrates how we can import a module:\n",
      "Figure 11.17: Az.Profile module status\n",
      "12. Perform steps 9, 10, and 11 again in order to import and install the Az.EventGrid \n",
      "module. If you are warned to install any dependencies before proceeding, go \n",
      "ahead and install the dependencies first.\n",
      "13. Perform steps 9, 10, and 11 again in order to import and install the Az.KeyVault \n",
      "module. If you are warned to install any dependencies before proceeding, go \n",
      "ahead and install the dependency first.\n",
      "Since we have imported the necessary modules, let's go ahead and create the Event \n",
      "Grid topic.\n",
      "---------------\n",
      "366 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "Step 6: Creating an Event Grid topic\n",
      "If you recall the architecture that we used, we need an Event Grid topic. Let's create \n",
      "one.\n",
      "The command that's used to create an Event Grid topic using PowerShell is as follows:\n",
      "New-AzEventGridTopic -ResourceGroupName VaultMonitoring -Name \n",
      "azureforarchitects-topic -Location \"West Europe\" \n",
      "The process of creating an Event Grid topic using the Azure portal is as follows:\n",
      "1. From the Azure portal, navigate to the already-created Vaultmonitoring resource \n",
      "group by clicking on Resource Groups in the left-hand menu.\n",
      "2. Next, click on the +Add button and search for Event Grid Topic in the search box. \n",
      "Select it and then click on the Create button.\n",
      "3. Fill in the appropriate values in the resultant form by providing a name, selecting \n",
      "a subscription, and selecting the newly created resource group, the location, and \n",
      "the event schema.\n",
      "---------------\n",
      "As we already discussed, the Event Grid topic provides an endpoint where the source \n",
      "will send the data. Since we have our topic ready, let's prepare the source Automation \n",
      "account.\n",
      "Step 7: Setting up the runbook\n",
      "This step will focus on creating an Azure Automation account and PowerShell runbooks \n",
      "that will contain the core logic of reading Azure key vaults and retrieving secrets stored \n",
      "within them. The steps required for configuring Azure Automation are as follows:\n",
      "1. Create the Azure Automation runbook: From the Azure portal, navigate to the \n",
      "already-created Vaultmonitoring resource group by clicking on Resource Groups \n",
      "in the left-hand menu.\n",
      "2. Click on the already-provisioned Azure Automation account, MonitoringKeyVault. \n",
      "Then, click on Runbooks in the left-hand menu, and click on +Add a Runbook \n",
      "from the top menu.\n",
      "3. Click on Create a new Runbook and provide a name. Let's call this runbook \n",
      "CheckExpiredAssets, and then set Runbook type to PowerShell:\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 367\n",
      "Figure 11.18: Creating a runbook\n",
      "4. Code the runbook: Declare a few variables to hold the subscription ID, tenant \n",
      "ID, application ID, and certificate thumbprint information. These values \n",
      "should be stored in Azure Automation variables, and the certificate should be \n",
      "uploaded to Automation certificates. The key used for the uploaded certificate is \n",
      "AutomationCertifcate. The values are retrieved from these stores and are assigned \n",
      "to the variables, as shown next:\n",
      "$subscriptionID = get-AutomationVariable \"azuresubscriptionid\" \n",
      "$tenantID = get-AutomationVariable \"azuretenantid\" \n",
      "$applicationId = get-AutomationVariable \"azureappid\" \n",
      "$cert = get-AutomationCertificate \"AutomationCertifcate\" \n",
      "$certThumbprint = ($cert.Thumbprint).ToString()\n",
      "5. The next code within the runbook helps to log into Azure using the service \n",
      "principal with values from the variables declared previously. Also, the code selects\n",
      "---------------\n",
      "an appropriate subscription. The code is shown next:\n",
      "Login-AzAccount -ServicePrincipal -CertificateThumbprint $certThumbprint \n",
      "-ApplicationId $applicationId -Tenant $tenantID \n",
      "Set-AzContext -SubscriptionId $subscriptionID\n",
      "Since Azure Event Grid was provisioned in step 6 of this section, its endpoint and \n",
      "keys are retrieved using the Get-AzEventGridTopic and Get-AzEventGridTopicKey \n",
      "cmdlets.\n",
      "---------------\n",
      "368 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "Azure Event Grid generates two keys—a primary and a secondary. The first key \n",
      "reference is taken as follows:\n",
      "$eventGridName = \"ExpiredAssetsKeyVaultEvents\" \n",
      "$eventGridResourceGroup = \"VaultMonitoring\" \n",
      "$topicEndpoint = (Get-AzEventGridTopic -ResourceGroupName \n",
      "$eventGridResourceGroup -Name $eventGridName).Endpoint \n",
      "$keys = (Get-AzEventGridTopicKey -ResourceGroupName \n",
      "$eventGridResourceGroup -Name $eventGridName ).Key1\n",
      "6. Next, all key vaults that were provisioned within the subscription are \n",
      "retrieved using iteration. While looping, all secrets are retrieved using the \n",
      "Get-AzKeyVaultSecret cmdlet.\n",
      "The expiry date of each secret is compared to the current date, and if the \n",
      "difference is less than a month, it generates an Event Grid event and publishes it \n",
      "using the invoke-webrequest command.\n",
      "The same steps are executed for certificates stored within the key vault. The\n",
      "---------------\n",
      "cmdlet used to retrieve all the certificates is Get-AzKeyVaultCertificate.\n",
      "The event that is published to Event Grid should be in the JSON array. The \n",
      "generated message is converted to JSON using the ConvertTo-Json cmdlet and \n",
      "then converted to an array by adding [ and ] as a prefix and suffix.\n",
      "In order to connect to Azure Event Grid and publish the event, the sender should \n",
      "supply the key in its header. The request will fail if this data is missing in the \n",
      "request payload:\n",
      "$keyvaults = Get-AzureRmKeyVault \n",
      "foreach($vault in $keyvaults) { \n",
      "$secrets = Get-AzureKeyVaultSecret -VaultName $vault.VaultName \n",
      "foreach($secret in $secrets) { \n",
      "if( ![string]::IsNullOrEmpty($secret.Expires) ) { \n",
      "if($secret.Expires.AddMonths(-1) -lt [datetime]::Now) \n",
      "{ \n",
      "$secretDataMessage = @{ \n",
      "id = [System.guid]::NewGuid() \n",
      "subject = \"Secret Expiry happening soon !!\" \n",
      "eventType = \"Secret Expiry\" \n",
      "eventTime = [System.DateTime]::UtcNow \n",
      "data = @{ \n",
      "\"ExpiryDate\" = $secret.Expires\n",
      "---------------\n",
      "\"SecretName\" = $secret.Name.ToString() \n",
      "\"VaultName\" = $secret.VaultName.ToString()\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 369\n",
      "\"SecretCreationDate\" = $secret.Created.ToString() \n",
      "\"IsSecretEnabled\" = $secret.Enabled.ToString() \n",
      "\"SecretId\" = $secret.Id.ToString() \n",
      "} \n",
      "} \n",
      "... \n",
      "Invoke-WebRequest -Uri $topicEndpoint -Body $finalBody -Headers $header \n",
      "-Method Post -UseBasicParsing \n",
      "} \n",
      "} \n",
      "Start-Sleep -Seconds 5 \n",
      "} \n",
      "}\n",
      "7. Publish the runbook by clicking on the Publish button, as shown in Figure 11.19:\n",
      "Figure 11.19: Publishing the runbook\n",
      "8. Scheduler: Create an Azure Automation scheduler asset to execute this runbook \n",
      "once every day at 12.00 AM. Click on Schedules from the left-hand menu of Azure \n",
      "Automation and click on +Add a schedule in the top menu. \n",
      "9. Provide scheduling information in the resulting form.\n",
      "This should conclude the configuration of the Azure Automation account.\n",
      "---------------\n",
      "370 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "Step 8: Working with SendGrid \n",
      "In this step, we will be creating a new SendGrid resource. The SendGrid resource \n",
      "is used to send emails from the application without needing to install a Simple \n",
      "Mail Transfer Protocol (SMTP) server. It provides a REST API and a C# Software \n",
      "Development Kit (SDK), by means of which it is quite easy to send bulk emails. In the \n",
      "current solution, Azure Functions will be used to invoke the SendGrid APIs to send \n",
      "emails, and so this resource needs to be provisioned. This resource has separate costing \n",
      "and is not covered as part of the Azure cost—there is a free tier available that can be \n",
      "used for sending emails:\n",
      "1. A SendGrid resource is created just like any other Azure resource. Search for \n",
      "sendgrid, and we will get SendGrid Email Delivery in the results.\n",
      "2. Select the resource and click on the Create button to open its configuration form.\n",
      "3. Select an appropriate pricing tier.\n",
      "---------------\n",
      "4. Provide the appropriate contact details. \n",
      "5. Tick the Terms of use check box.\n",
      "6. Complete the form and then click on the Create button.\n",
      "7. After the resource is provisioned, click on the Manage button in the top menu—\n",
      "this will open the SendGrid website. The website may request email configuration. \n",
      "Then, select API Keys from the Settings section and click on the Create API Key \n",
      "button:\n",
      "Figure 11.20: Creating API keys for SendGrid\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 371\n",
      "8. From the resulting window, select Full Access and click on the Create & View \n",
      "button. This will create the key for the SendGrid resource; keep a note of this key, \n",
      "as it will be used with the Azure Functions configuration for SendGrid:\n",
      "Figure 11.21: Setting up the access level in the SendGrid portal\n",
      "Now that we have configured access levels for SendGrid, let's configure another third-\n",
      "party service, which is called Twilio.\n",
      "Step 9: Getting started with Twilio\n",
      "In this step, we will be creating a new Twilio account. Twilio is used for sending bulk \n",
      "SMS messages. To create an account with Twilio, navigate to twilio.com and create a \n",
      "new account. After successfully creating an account, a mobile number is generated that \n",
      "can be used to send SMS messages to receivers:\n",
      "---------------\n",
      "372 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "Figure 11.22: Choosing a Twilio number\n",
      "The Twilio account provides both production and test keys. Copy the test key and token \n",
      "to a temporary location, such as Notepad, as they will be required later within Azure \n",
      "Functions:\n",
      "Figure 11.23: Setting up Twilio\n",
      "We have SendGrid and Twilio in place for the notification service; however, we need \n",
      "something that can take the event and notify the users. Here comes the role of a \n",
      "function app. In the next section, we will be creating a function app that will help with \n",
      "sending SMS and emails.\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 373\n",
      "Step 10: Setting up a function app\n",
      "In this step, we will be creating a new function app responsible for sending emails \n",
      "and SMS notifications. The purpose of the function app within the solution is to send \n",
      "notification messages to users regarding the expiry of secrets in the key vault. A single \n",
      "function will be responsible for sending both emails and SMS messages—note that this \n",
      "could have been divided into two separate functions. The first step is to create a new \n",
      "function app and host a function within it:\n",
      "1. As we have done before, navigate to your resource group, click on the +Add button \n",
      "in the top menu, and search for the function app resource. Then, click on the \n",
      "Create button to get the Function App form.\n",
      "2. Fill in the Function App form and click on the Create button. The name of the \n",
      "function app must be unique across Azure.\n",
      "3. Once the function app is provisioned, create a new function called\n",
      "---------------\n",
      "SMSandEMailFunction by clicking on the + button next to the Functions item in the \n",
      "left-hand menu. Then, select In-portal from the central dashboard.\n",
      "4. Select HTTP trigger and name it SMSandEMailFunction. Then, click on the Create \n",
      "button—the Authorization level option can be any value.\n",
      "5. Remove the default code, replace it with the code shown in the following listing, \n",
      "and then click on the Save button in the top menu:\n",
      "#r \"SendGrid\" \n",
      "#r \"Newtonsoft.Json\" \n",
      "#r \"Twilio.Api\" \n",
      "using System.Net; \n",
      "using System; \n",
      "using SendGrid.Helpers.Mail; \n",
      "using Microsoft.Azure.WebJobs.Host; \n",
      "using Newtonsoft.Json; \n",
      "using Twilio; \n",
      "using System.Configuration; \n",
      "public static HttpResponseMessage Run(HttpRequestMessage req, TraceWriter \n",
      "log, out Mail message,out SMSMessage sms) \n",
      "{ \n",
      "log.Info(\"C# HTTP trigger function processed a request.\"); \n",
      "string alldata = req.Content.ReadAsStringAsync().GetAwaiter().GetResult(); \n",
      "message = new Mail(); \n",
      "var personalization = new Personalization();\n",
      "---------------\n",
      "personalization.AddBcc(new Email(ConfigurationManager.\n",
      "AppSettings[\"bccStakeholdersEmail\"]));\n",
      "---------------\n",
      "374 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "personalization.AddTo(new Email(ConfigurationManager.\n",
      "AppSettings[\"toStakeholdersEmail\"])); \n",
      "var messageContent = new Content(\"text/html\", alldata); \n",
      "message.AddContent(messageContent); \n",
      "message.AddPersonalization(personalization); \n",
      "message.Subject = \"Key Vault assets Expiring soon..\"; \n",
      "message.From = new Email(ConfigurationManager.AppSettings[\"serviceEmail\"]); \n",
      "string msg = alldata; \n",
      "sms = new SMSMessage(); \n",
      "sms.Body = msg; \n",
      "sms.To = ConfigurationManager.AppSettings[\"adminPhone\"]; \n",
      "sms.From = ConfigurationManager.AppSettings[\"servicePhone\"]; \n",
      "return req.CreateResponse(HttpStatusCode.OK, \"Hello \"); \n",
      "}\n",
      "6. Click on the function app name in the left-hand menu and click again on the \n",
      "Application settings link in the main window:\n",
      "Figure 11.24: Navigating to Application settings\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 375\n",
      "7. Navigate to the Application settings section, as shown in Figure 11.24, and add a \n",
      "few entries by clicking on + Add new setting for each entry.\n",
      "Note that the entries are in the form of key-value pairs, and the values should \n",
      "be actual real-time values. Both adminPhone and servicePhone should already be \n",
      "configured on the Twilio website. servicePhone is the phone number generated by \n",
      "Twilio that is used for sending SMS messages, and adminPhone is the phone number \n",
      "of the administrator to whom the SMS should be sent.\n",
      "Also note that Twilio expects the destination phone number to be in a particular \n",
      "format depending on the country (for India, the format is +91 xxxxx xxxxx). Note \n",
      "the spaces and country code in the number.\n",
      "We also need to add the keys for both SendGrid and Twilio within the application \n",
      "settings. These settings are mentioned in the following list. You may already have\n",
      "---------------\n",
      "these values handy because of activities performed in earlier steps:\n",
      "• The value of SendGridAPIKeyAsAppSetting is the key for SendGrid.\n",
      "• TwilioAccountSid is the system identifier for the Twilio account. This value \n",
      "was already copied and stored in a temporary location in Step 9: Getting \n",
      "started with Twilio.\n",
      "• TwilioAuthToken is the token for the Twilio account. This value was already \n",
      "copied and stored in a temporary place in an earlier step.\n",
      "---------------\n",
      "376 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "8. Save the settings by clicking on the Save button in the top menu:\n",
      "Figure 11.25: Configuring application settings\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 377\n",
      "9. Click on the Integrate link in the left-hand menu just below the name of the \n",
      "function, and click on + New Output. This is to add an output for the SendGrid \n",
      "service:\n",
      "Figure 11.26: Adding an output to the function app\n",
      "10. Next, select SendGrid; it might ask you to install the SendGrid extension. Install \n",
      "the extension, which will take a couple of minutes:\n",
      "Figure 11.27: Configuring a function app\n",
      "---------------\n",
      "378 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "11. After installing the extension, the output configuration form appears. The \n",
      "important configuration items in this form are Message parameter name and \n",
      "SendGrid API Key App Setting. Leave the default value for Message parameter \n",
      "name and click on the drop-down list to select SendGridAPIKeyAsAppSetting \n",
      "as the API app setting key. This was already configured in a previous step within \n",
      "the app settings configuration. The form should be configured as shown in Figure \n",
      "11.28, and then you need to click on the Save button:\n",
      "Figure 11.28: Setting up SendGrid\n",
      "12. Click on + New Output again; this is to add an output for the Twilio service.\n",
      "13. Then, select Twilio SMS. It might ask you to install the Twilio SMS extension. \n",
      "Install the extension, which will take a couple of minutes.\n",
      "14. After installing the extension, the output configuration form appears. The\n",
      "---------------\n",
      "important configuration items in this form are Message parameter name, Account \n",
      "SID setting, and Auth Token setting. Change the default value for Message \n",
      "parameter name to sms. This is done because the message parameter is already \n",
      "used for the SendGrid service parameter. Ensure that the value of Account \n",
      "SID setting is TwilioAccountSid and that the value of Auth Token setting is \n",
      "TwilioAuthToken. These values were already configured in a previous step of the \n",
      "app settings configuration. The form should be configured as shown in Figure \n",
      "11.29, and then you should click on Save:\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 379\n",
      "Figure 11.29: Setting up Twilio SMS output\n",
      "Our SendGrid and Twilio accounts are ready. Now it's time to use the connectors and \n",
      "add them to the logic app. In the next part, we will create the logic app and will use \n",
      "connectors to work with the resources we have created so far.\n",
      "Step 11: Creating a logic app\n",
      "In this step, we will be creating a new logic app workflow. We have authored an Azure \n",
      "Automation runbook that queries all the secrets in all key vaults and publishes an \n",
      "event if it finds any of them expiring within a month. The logic app's workflow acts as a \n",
      "subscriber to these events:\n",
      "1. The first step within the Logic App menu is to create a logic app workflow.\n",
      "2. Fill in the resultant form after clicking on the Create button. We are provisioning \n",
      "the logic app in the same resource group as the other resources for this solution.\n",
      "3. After the logic app is provisioned, it opens the designer window. Select Blank\n",
      "---------------\n",
      "Logic App from the Templates section.\n",
      "4. In the resultant window, add a trigger that can subscribe to Event Grid. Logic Apps \n",
      "provides a trigger for Event Grid, and you can search for this to see whether it's \n",
      "available.\n",
      "---------------\n",
      "380 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "5. Next, select the When a resource event occurs (preview) trigger:\n",
      "Figure 11.30: Selecting a trigger from Event Grid\n",
      "6. In the resultant window, select Connect with Service Principal.\n",
      "Provide the service principal details, including the application ID (Client ID), \n",
      "tenant ID, and password. This trigger does not accept a service principal that \n",
      "authenticates with the certificate—it accepts a service principal only with \n",
      "a password. Create a new service principal at this stage that authenticates \n",
      "with a password (the steps for creating a service principal based on password \n",
      "authentication were covered earlier, in step 2) and use the details of the newly \n",
      "created service principal for Azure Event Grid configuration, as shown in \n",
      "Figure 11.31:\n",
      "Figure 11.31: Providing the service principal details for connection\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 381\n",
      "7. Select the subscription. Based on the scope of the service principal, this will get \n",
      "auto-filled. Select Microsoft.EventGrid.Topics as the Resource Type value and set \n",
      "the name of the custom topic as ExpiredAssetsKeyVaultEvents:\n",
      "Figure 11.32: Providing Event Grid trigger details\n",
      "8. The previous step will create a connector, and the connection information can be \n",
      "changed by clicking on Change connection.\n",
      "9. The final configuration of the Event Grid trigger should be similar to Figure 11.33:\n",
      "Figure 11.33: Event Grid trigger overview\n",
      "---------------\n",
      "382 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "10. Add a new Parse JSON activity after the Event Grid trigger—this activity needs \n",
      "the JSON schema. Generally, the schema is not available, but this activity helps \n",
      "generate the schema if valid JSON is provided to it:\n",
      "Figure 11.34: Parse JSON activity\n",
      "11. Click on Use sample payload to generate schema and provide the following data:\n",
      "{ \n",
      "\"ExpiryDate\": \"\", \n",
      "\"SecretName\": \"\", \n",
      "\"VaultName\": \"\", \n",
      "\"SecretCreationDate\": \"\", \n",
      "\"IsSecretEnabled\": \"\", \n",
      "\"SecretId\": \"\" \n",
      "}\n",
      "A question might arise here regarding the sample payload. At this stage, how \n",
      "do you calculate the payload that's generated by the Event Grid publisher? The \n",
      "answer to this lies in the fact that this sample payload is exactly the same as is \n",
      "used in the data element in the Azure Automation runbook. You can take a look at \n",
      "that code snippet again:\n",
      "data = @{ \n",
      "\"ExpiryDate\" = $certificate.Expires \n",
      "\"CertificateName\" = $certificate.Name.ToString()\n",
      "---------------\n",
      "\"VaultName\" = $certificate.VaultName.ToString() \n",
      "\"CertificateCreationDate\" = $certificate.Created.ToString() \n",
      "\"IsCertificateEnabled\" = $certificate.Enabled.ToString() \n",
      "\"CertificateId\" = $certificate.Id.ToString() \n",
      "}\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 383\n",
      "12. The Content box should contain dynamic content coming out from the previous \n",
      "trigger, as demonstrated in Figure 11.35:\n",
      " Figure 11.35: Providing dynamic content to the Parse JSON activity \n",
      "13. Add another Azure Functions action after Parse JSON, and then select \n",
      "Choose an Azure function. Select the Azure function apps called \n",
      "NotificationFunctionAppBook and SMSAndEmailFunction, which were created \n",
      "earlier:\n",
      "---------------\n",
      "384 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "Figure 11.36: Adding an Azure Functions action\n",
      "14. Click on the Request Body text area and fill it with the following code. This is done \n",
      "to convert the data into JSON before sending it to the Azure function:\n",
      "{ \n",
      "\"alldata\" : \n",
      "}\n",
      "15. Place the cursor after the \":\" in the preceding code and click on Add dynamic \n",
      "content | Body from the previous activity:\n",
      "Figure 11.37: Converting data to JSON before sending it to an Azure function\n",
      "---------------\n",
      "Creating an end-to-end solution using serverless technologies | 385\n",
      "16. Save the entire logic app; it should look as follows:\n",
      "Figure 11.38: Logic app workflow\n",
      "Once you save the logic app, your solution is ready to be tested. If you don't have any \n",
      "keys or secrets, try adding them with an expiry date so that you can confirm whether \n",
      "your solution is working.\n",
      "Testing\n",
      "Upload some secrets and certificates that have expiry dates to Azure Key Vault and \n",
      "execute the Azure Automation runbook. The runbook is scheduled to run on a schedule. \n",
      "Additionally, the runbook will publish events to Event Grid. The logic app should be \n",
      "enabled, and it will pick the event and finally invoke the Azure function to send email \n",
      "and SMS notifications.\n",
      "The email should look as follows:\n",
      "Figure 11.39: Email received regarding the expiring keys\n",
      "---------------\n",
      "386 | Azure solutions using Azure Logic Apps, Event Grid, and Functions\n",
      "In this exercise, we had a problem, we architected a solution, and we implemented it. \n",
      "This is exactly what happens in the role of an architect. Customers will have specific \n",
      "requirements and, based on those, you must develop a solution. On that note, we are \n",
      "concluding this chapter. Let's do a quick recap of what we have discussed.\n",
      "Summary\n",
      "This chapter introduced Logic Apps and demonstrated a complete end-to-end solution \n",
      "using multiple Azure services. The chapter focused heavily on creating an architecture \n",
      "that integrated multiple Azure services to create an end-to-end solution. The services \n",
      "used in the solution were Azure Automation, Azure Logic Apps, Azure Event Grid, Azure \n",
      "Functions, SendGrid, and Twilio. These services were implemented through the Azure \n",
      "portal and PowerShell using service principals as service accounts. The chapter also\n",
      "---------------\n",
      "showed a number of ways of creating service principals with password and certificate \n",
      "authentication.\n",
      "A solution to a problem can be found in multiple ways. You could use an Outlook trigger \n",
      "in a logic app instead of SendGrid. There will be many solutions to a problem—the one \n",
      "to go with depends on what approach you are taking. The more familiar you are with \n",
      "the services, the greater the number of options you will have. In the next chapter, \n",
      "you will learn about the importance of events in both Azure and Azure application \n",
      "architecture.\n",
      "---------------\n",
      "Events are everywhere! Any activity or task that changes the state of a work item \n",
      "generates an event. Due to a lack of infrastructure and the non-availability of cheap \n",
      "devices, there previously was not much traction for the Internet of Things (IoT). \n",
      "Historically, organizations used hosted environments from internet service \n",
      "providers (ISPs) that just had monitoring systems on top of them. These monitoring \n",
      "systems raised events that were few and far between.\n",
      "However, with the advent of the cloud, things are changing rapidly. With increased \n",
      "deployments on the cloud, especially of Platform as a Service (PaaS) services, \n",
      "organizations no longer need much control over the hardware and the platform, and \n",
      "now every time there is a change in an environment, an event is raised. With the \n",
      "emergence of cloud events, IoT has gained a lot of prominence and events have started \n",
      "to take center stage.\n",
      "Another recent phenomenon has been the rapid burst of growth in the availability\n",
      "---------------\n",
      "of data. The velocity, variety, and volume of data has spiked, and so has the need \n",
      "for solutions for storing and processing data. Multiple solutions and platforms have \n",
      "emerged, such as Hadoop, data lakes for storage, data lakes for analytics, and machine \n",
      "learning services.\n",
      "Azure Big Data \n",
      "eventing solutions\n",
      "12\n",
      "---------------\n",
      "390 | Azure Big Data eventing solutions\n",
      "Apart from storage and analytics, there is also a need for services that are capable of \n",
      "ingesting millions upon millions of events and messages from various sources. There \n",
      "is also a need for services that can work on temporal data, rather than working on an \n",
      "entire snapshot of data. For example, event data/IoT data is used in applications that \n",
      "make decisions based on real-time or near real-time data, such as traffic management \n",
      "systems or systems that monitor temperature.\n",
      "Azure provides a plethora of services that help in capturing and analyzing real-time \n",
      "data from sensors. In this chapter, we will go through a couple of eventing services in \n",
      "Azure, as listed here:\n",
      "• Azure Event Hubs\n",
      "• Azure Stream Analytics\n",
      "There are other eventing services, such as Azure Event Grid, that are not covered in this \n",
      "chapter; however, they are extensively covered in Chapter 10, Azure Integration Services\n",
      "---------------\n",
      "with Azure functions (Durable Functions and Proxy functions).\n",
      "Introducing events\n",
      "Events are important constructs in both Azure and Azure application architecture. \n",
      "Events are everywhere within the software ecosystem. Generally, any action that is \n",
      "taken results in an event that can be trapped, and then further action can be taken. To \n",
      "take this discussion forward, it is important to first understand the basics of events.\n",
      "Events help in capturing the new state of a target resource. A message is a lightweight \n",
      "notification of a condition or a state change. Events are different than messages. \n",
      "Messages are related to business functionality, such as sending order details to \n",
      "another system. They contain raw data and can be large in size. In comparison, events \n",
      "are different; for instance, a virtual machine being stopped is an event. Figure 12.1 \n",
      "demonstrates this transition from the current state to the target state:\n",
      "Figure 12.1: Transition of a state due to an event\n",
      "---------------\n",
      "Introducing events | 391\n",
      "Events can be stored in durable storage as historical data and events can also be used to \n",
      "find patterns that are emerging on an ongoing basis. Events can be thought of as data \n",
      "being streamed constantly. To capture, ingest, and perform analysis on a stream of data, \n",
      "special infrastructure components that can read a small window of data and provide \n",
      "insights are needed, and that is where the Stream Analytics service comes into the \n",
      "picture.\n",
      "Event streaming\n",
      "Processing events as they are ingested and streamed over a time window provides real-\n",
      "time insights about data. The time window could 15 minutes or an hour—the window \n",
      "is defined by the user and depends on the insights that are to be extracted from data. \n",
      "Take credit card swipes, for instance—millions of credit card swipes happen every \n",
      "minute, and fraud detection can be done over streamed events for a time window of \n",
      "one or two minutes.\n",
      "---------------\n",
      "Event streaming refers to services that can accept data as and when it arises, rather \n",
      "than accepting it periodically. For example, event streams should be capable of \n",
      "accepting temperature information from devices as and when they send it, rather than \n",
      "making the data wait in a queue or a staging environment.\n",
      "Event streaming also has the capability of querying data while in transit. This is \n",
      "temporal data that is stored for a while, and the queries occur on the moving data; \n",
      "therefore, the data is not stationary. This capability is not available on other data \n",
      "platforms, which can only query stored data and not temporal data that has just been \n",
      "ingested.\n",
      "Event streaming services should be able to scale easily to accept millions or even \n",
      "billions of events. They should be highly available such that sources can send events and \n",
      "data to them at any time. Real-time data ingestion and being able to work on that data,\n",
      "---------------\n",
      "rather than data that's stored in a different location, is the key to event streaming.\n",
      "---------------\n",
      "392 | Azure Big Data eventing solutions\n",
      "But when we already have so many data platforms with advanced query execution \n",
      "capabilities, why do we need event steaming? One of the main advantages of event \n",
      "streaming is that it provides real-time insights and information whose usefulness is \n",
      "time-dependent. The same information found after a few minutes or hours might not \n",
      "be that useful. Let's consider some scenarios in which working on incoming data is \n",
      "quite important. These scenarios can't be effectively and efficiently solved by existing \n",
      "data platforms:\n",
      "• Credit card fraud detection: This should happen as and when a fraudulent \n",
      "transaction happens.\n",
      "• Telemetry information from sensors: In the case of IoT devices sending vital \n",
      "information about their environments, the user should be notified as and when an \n",
      "anomaly is detected.\n",
      "• Live dashboards: Event streaming is needed to create dashboards that show live \n",
      "information.\n",
      "---------------\n",
      "• Datacenter environment telemetry: This will let the user know about any \n",
      "intrusions, security breaches, failures of components, and more.\n",
      "There are many possibilities for applying event streaming within an enterprise, and its \n",
      "importance cannot be stressed enough.\n",
      "Event Hubs\n",
      "Azure Event Hubs is a streaming platform that provides functionality related to the \n",
      "ingestion and storage of streaming-related events.\n",
      "It can ingest data from a variety of sources; these sources could be IoT sensors or \n",
      "any applications using the Event Hubs Software Development Kit (SDK). It supports \n",
      "multiple protocols for ingesting and storing data. These protocols are industry \n",
      "standard, and they include the following:\n",
      "• HTTP: This is a stateless option and does not require an active session.\n",
      "• Advanced Messaging Queuing Protocol (AMQP): This requires an active session \n",
      "(that is, an established connection using sockets) and works with Transport Layer \n",
      "Security (TLS) and Secure Socket Layer (SSL).\n",
      "---------------\n",
      "• Apache Kafka: This is a distributed streaming platform similar to Stream Analytics.   \n",
      "However, Stream Analytics is designed to run real-time analytics on multiple \n",
      "streams of data from various sources, such as IoT sensors and websites.\n",
      "Event Hubs is an event ingestion service. It can't query a request and output query \n",
      "results to another location. That is the responsibility of Stream Analytics, which is \n",
      "covered in the next section.\n",
      "---------------\n",
      "Introducing events | 393\n",
      "To create an Event Hubs instance from the portal, search for Event Hubs in Marketplace \n",
      "and click on Create. Select a subscription and an existing resource group (or create a \n",
      "new one). Provide a name for the Event Hubs namespace, the preferred Azure region \n",
      "to host it in, the pricing tier (Basic or Standard, explained later), and the number of \n",
      "throughput units (explained later):\n",
      "Figure 12.2: Creating an Event Hubs namespace\n",
      "Event Hubs, being a PaaS service, is highly distributed, highly available, and highly \n",
      "scalable.\n",
      "Event Hubs comes with the following two SKUs or pricing tiers:\n",
      "• Basic: This comes with one consumer group and can retain messages for 1 day. It \n",
      "can have a maximum of 100 brokered connections.\n",
      "• Standard: This comes with a maximum of 20 consumer groups and can retain \n",
      "messages for 1 day with additional storage for 7 days. It can have a maximum of \n",
      "1,000 brokered connections. It is also possible to define policies in this SKU.\n",
      "---------------\n",
      "394 | Azure Big Data eventing solutions\n",
      "Figure 12.3 shows the different SKUs available while creating a new Event Hubs \n",
      "namespace. It provides an option to choose an appropriate pricing tier, along with other \n",
      "important details:\n",
      "Figure 12.3: Event Hubs SKUs\n",
      "Throughput can also be configured at the namespace level. Namespaces are containers \n",
      "that consist of multiple event hubs in the same subscription and region. The throughput \n",
      "is calculated as throughput units (TUs). Each TU provides:\n",
      "• Up to 1 MB per second of ingress or a maximum of 1,000 ingress events and \n",
      "management operations per second.\n",
      "• Up to 2 MB per second of egress or a maximum of 4,096 events and management \n",
      "operations per second.\n",
      "• Up to 84 GB of storage.\n",
      "The TUs can range from 1 to 20 and they are billed on an hourly basis.\n",
      "It is important to note that the SKU cannot be changed after provisioning an Event \n",
      "Hubs namespace. Due consideration and planning should be undertaken before\n",
      "---------------\n",
      "selecting an SKU. The planning process should include planning the number of \n",
      "consumer groups required and the number of applications interested in reading events \n",
      "from the event hub.\n",
      "---------------\n",
      "Event Hubs architecture | 395\n",
      "Also, the Standard SKU is not available in every region. It should be checked for \n",
      "availability at the time of the design and implementation of the event hub. The URL \n",
      "for checking region availability is https:/ /azure.microsoft.com/global-infrastructure/\n",
      "services/?products=event-hubs.\n",
      "Event Hubs architecture\n",
      "There are three main components of the Event Hubs architecture: The Event \n",
      "Producers, the Event Hub, and the Event Consumer, as shown in the following diagram:\n",
      "Figure 12.4: Event Hubs architecture\n",
      "Event Producers generate events and send them to the Event Hub. The Event \n",
      "Hub stores the ingested events and provides that data to the Event Consumer. \n",
      "The Event Consumer is whatever is interested in those events, and it connects to \n",
      "the Event Hub to fetch the data.\n",
      "Event hubs cannot be created without an Event Hubs namespace. The Event Hubs \n",
      "namespace acts as a container and can host multiple event hubs. Each Event Hubs\n",
      "---------------\n",
      "namespace provides a unique REST-based endpoint that is consumed by clients to send \n",
      "data to Event Hubs. This namespace is the same namespace that is needed for Service \n",
      "Bus artifacts, such as topics and queues.\n",
      "The connection string of an Event Hubs namespace is composed of its URL, policy \n",
      "name, and key. A sample connection string is shown in the following code block:\n",
      "Endpoint=sb://demoeventhubnsbook.servicebus.windows.\n",
      "net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=M/\n",
      "E4eeBsr7DAlXcvw6ziFqlSDNbFX6E49Jfti8CRkbA=\n",
      "This connection string can be found in the Shared Access Signature (SAS) menu item \n",
      "of the namespace. There can be multiple policies defined for a namespace, each having \n",
      "different levels of access to the namespace. The three levels of access are as follows:\n",
      "• Manage: This can manage the event hub from an administrative perspective. It \n",
      "also has rights for sending and listening to events.\n",
      "• Send: This can write events to Event Hubs.\n",
      "---------------\n",
      "• Listen: This can read events from Event Hubs.\n",
      "---------------\n",
      "396 | Azure Big Data eventing solutions\n",
      "By default, the RootManageSharedAccessKey policy is created when creating an event hub, \n",
      "as shown in Figure 12.5. Policies help in creating granular access control on Event Hubs. \n",
      "The key associated with each policy is used by consumers to determine their identity; \n",
      "additional policies can also be created with any combination of the three previously \n",
      "mentioned access levels:\n",
      "Figure 12.5: Shared access policies in Event Hubs\n",
      "Event hubs can be created from the Event Hubs namespace service by performing the \n",
      "following actions:\n",
      "---------------\n",
      "Event Hubs architecture | 397\n",
      "1. Click on Event Hubs in the left-hand menu and click on + Event Hub in the \n",
      "resultant screen:\n",
      "Figure 12.6: Creating an event hub from the Azure portal\n",
      "---------------\n",
      "398 | Azure Big Data eventing solutions\n",
      "2. Next, provide values for the Partition Count and Message Retention fields, \n",
      "along with the name of your choice. Then, select Off for Capture, as demonstrated \n",
      "in Figure 12.7:\n",
      "Figure 12.7: Creating a new event hub\n",
      "After the event hub is created, you will see it in the list of event hubs, as shown in \n",
      "Figure 12.8:\n",
      "Figure 12.8: List of created event hubs\n",
      "---------------\n",
      "Event Hubs architecture | 399\n",
      "Event Hubs also allows the storage of events to a storage account or data lake directly \n",
      "using a feature known as Capture.\n",
      "Capture helps in the automatic storage of ingested data to either an Azure Storage \n",
      "account or an Azure Data Lake Storage account. This feature ensures that the ingestion \n",
      "and storage of events happens in a single step, rather than transferring data into \n",
      "storage being a separate activity:\n",
      "Figure 12.9: Capture feature options\n",
      "Separate policies can be assigned to each event hub by adding a new policy at the event \n",
      "hub level.\n",
      "After creating the policy, the connection string is available from the Secure Access \n",
      "Signature left-menu item in the Azure portal.\n",
      "---------------\n",
      "400 | Azure Big Data eventing solutions\n",
      "Since a namespace can consist of multiple event hubs, the connection string for an \n",
      "individual event hub will be similar to the following code block. The difference here is in \n",
      "the key value and the addition of EntityPath with the name of the event hub:\n",
      "Endpoint=sb://azuretwittereventdata.servicebus.windows\n",
      "=rxEu5K4Y2qsi5wEeOKuOvRnhtgW8xW35UBex4VlIKqg=;EntityPath=myeventhub\n",
      "We had to keep the Capture option set to Off while creating the event hub, and it can \n",
      "be switched back on after creating the event hub. It helps to save events to Azure Blob \n",
      "storage or an Azure Data Lake Storage account automatically. The configuration for the \n",
      "size and time interval is shown in Figure 12.10:\n",
      "Figure 12.10: Selecting the size and time interval for capturing events\n",
      "We did not cover the concepts of partitions and message retention options while \n",
      "creating event hubs.\n",
      "Partitioning is an important concept related to the scalability of any data store. Events\n",
      "---------------\n",
      "are retained within event hubs for a specific period of time. If all events are stored \n",
      "within the same data store, then it becomes extremely difficult to scale that data store. \n",
      "Every event producer will connect to the same data store and send their events to it. \n",
      "Compare this with a data store that can partition the same data into multiple smaller \n",
      "data stores, each being uniquely identified with a value.\n",
      "---------------\n",
      "Event Hubs architecture | 401\n",
      "The smaller data store is called a partition, and the value that defines the partition is \n",
      "known as the partition key. This partition key is part of the event data.\n",
      "Now the event producers can connect to the event hub, and based on the value of the \n",
      "partition key, the event hub will store the data in an appropriate partition. This will \n",
      "allow the event hub to ingest multiple events at the same time in parallel.\n",
      "Deciding on the number of partitions is a crucial aspect of the scalability of an event \n",
      "hub. Figure 12.11 shows that ingested data is stored in the appropriate partition \n",
      "internally by Event Hubs using the partition key:\n",
      "Figure 12.11: Partitioning in an event hub\n",
      "It is important to understand that one partition might have multiple keys. The user \n",
      "decides how many partitions are required, and the event hub internally decides the \n",
      "best way to allocate the partition keys between them. Each partition stores data in an\n",
      "---------------\n",
      "orderly way using a timestamp, and newer events are appended toward the end of the \n",
      "partition.\n",
      "It is important to note that it is not possible to change the number of partitions once \n",
      "the event hub is created.\n",
      "It is also important to remember that partitions also help in bringing parallelism and \n",
      "concurrency for applications reading the events. For example, if there are 10 partitions, \n",
      "10 parallel readers can read the events without any degradation in performance.\n",
      "Message retention refers to the time period for which events should be stored. After \n",
      "the expiry of the retention period, the events are discarded.\n",
      "---------------\n",
      "402 | Azure Big Data eventing solutions\n",
      "Consumer groups\n",
      "Consumers are applications that read events from an event hub. Consumer groups \n",
      "are created for consumers to connect to in order to read the events. There can \n",
      "be multiple consumer groups for an event hub, and each consumer group has access \n",
      "to all the partitions within an event hub. Each consumer group forms a query on the \n",
      "events in events hubs. Applications can use consumer groups and each application \n",
      "will get a different view of the event hub events. A default $default consumer group \n",
      "is created when creating an event hub. It is good practice for one consumer to be \n",
      "associated with one consumer group for optimal performance. However, it is possible to \n",
      "have five readers on each partition in a consumer group:\n",
      "Figure 12.12: Event receivers in a consumer group\n",
      "Now that you understand consumer groups, it is time to go deeper into the concept of \n",
      "Event Hubs throughput.\n",
      "---------------\n",
      "A primer on Stream Analytics | 403\n",
      "Throughput\n",
      "Partitions help with scalability, while throughput helps with capacity per second. So, \n",
      "what is capacity in terms of Event Hubs? It is the amount of data that can be handled \n",
      "per second.\n",
      "In Event Hubs, a single TU allows the following:\n",
      "• 1 MB of ingestion data per second or 1,000 events per second (whichever happens \n",
      "first)\n",
      "• 2 MB of egress data per second or 4,096 events per second (whichever happens \n",
      "first)\n",
      "The auto-inflate option helps in increasing the throughput automatically if the number \n",
      "of incoming/outgoing events or the incoming/outgoing total size crosses a threshold. \n",
      "Instead of throttling, the throughput will scale up and down. The configuration of \n",
      "throughput at the time of the creation of the namespace is shown in Figure 12.13. Again, \n",
      "careful thought should go into deciding the TUs:\n",
      "Figure 12.13: Selecting the TUs along with auto-inflate\n",
      "A primer on Stream Analytics\n",
      "---------------\n",
      "Event Hubs is a highly scalable data streaming platform, so we need another service \n",
      "that can process these events as a stream rather than just as stored data. Stream \n",
      "Analytics helps in processing and examining a stream of big data, and Stream Analytics \n",
      "jobs help to execute the processing of events.\n",
      "Stream Analytics can process millions of events per second and it is quite easy to get \n",
      "started with it. Azure Stream Analytics is a PaaS that is completely managed by Azure. \n",
      "Customers of Stream Analytics do not have to manage the underlying hardware and \n",
      "platform.\n",
      "---------------\n",
      "404 | Azure Big Data eventing solutions\n",
      "Each job comprises multiple inputs, outputs, and a query, which transforms the \n",
      "incoming data into new output. The whole architecture of Stream Analytics is shown in \n",
      "Figure 12.14:\n",
      "Figure 12.14: Azure Stream Analytics architecture\n",
      "In Figure 12.14, the event sources are displayed on the extreme left. These are the \n",
      "sources that produce the events. They could be IoT devices, custom applications \n",
      "written in any programming language, or events coming from other Azure platforms, \n",
      "such as Log Analytics or Application Insights.\n",
      "These events must first be ingested into the system, and there are numerous Azure \n",
      "services that can help to ingest this data. We've already looked at Event Hubs and how \n",
      "they help in ingesting data. There are other services, such as IoT Hub, that also help in \n",
      "ingesting device-specific and sensor-specific data. IoT Hub and ingestion are covered\n",
      "---------------\n",
      "in detail in Chapter 11, Designing IoT Solutions. This ingested data undergoes processing \n",
      "as it arrives in a stream, and this processing is done using Stream Analytics. The output \n",
      "from Stream Analytics could be fed to a presentation platform, such as Power BI, to \n",
      "show real-time data to stakeholders, or a storage platform such as Cosmos DB, Data \n",
      "Lake Storage, or Azure Storage, from which the data can be read and actioned later by \n",
      "Azure Functions and Service Bus queues.\n",
      "Stream Analytics helps in gathering insights from real-time ingested data within a time \n",
      "window frame and helps in identifying patterns.\n",
      "---------------\n",
      "A primer on Stream Analytics | 405\n",
      "It does so through three different tasks:\n",
      "• Input: The data should be ingested within the analytics process. The data can \n",
      "originate from Event Hubs, IoT Hub, or Azure Blob storage. Multiple separate \n",
      "reference inputs using a storage account and SQL Database can be used for lookup \n",
      "data within queries.\n",
      "• Query: This is where Stream Analytics does the core job of analyzing the ingested \n",
      "data and extracting meaningful insights and patterns. It does so with the help \n",
      "of JavaScript user-defined functions, JavaScript user-defined aggregates, Azure \n",
      "Machine Learning, and Azure Machine Learning studio.\n",
      "• Output: The result of the queries can be sent to multiple different types of \n",
      "destinations, and prominent among them are Cosmos DB, Power BI, Synapse \n",
      "Analytics, Data Lake Storage, and Functions:\n",
      "Figure 12.15: Stream Analytics process\n",
      "Stream Analytics is capable of ingesting millions of events per second and can execute \n",
      "queries on top of them.\n",
      "---------------\n",
      "Input data is supported in any of the three following formats:\n",
      "• JavaScript Object Notation (JSON): This is a lightweight, plaintext-based format \n",
      "that is human readable. It consists of name-value pairs; an example of a JSON \n",
      "event follows:\n",
      "{ \n",
      " \"SensorId\" : 2, \n",
      " \"humidity\" : 60, \n",
      " \"temperature\" : 26C \n",
      "}\n",
      "• Event Hubs\n",
      "• IoT Hub\n",
      "• Blob Storage\n",
      "• Query Language\n",
      "• JavaScript UDF\n",
      "• JavaScript UDA\n",
      "• Azure Machine Learning Studio\n",
      "• Azure Machine Learning\n",
      "• Event Hubs\n",
      "• IoT Hub\n",
      "• Blob/Table Storage\n",
      "• Service Bus Topics/Queues\n",
      "• Azure Functions\n",
      "• Power BI\n",
      "• Cosmos DB\n",
      "• SQL Database\n",
      "• Data Lake Storage\n",
      "• Azure Synapse Analytics\n",
      "Input OutputQuery\n",
      "---------------\n",
      "406 | Azure Big Data eventing solutions\n",
      "• Comma-Separated Values (CSV): These are also plaintext values, which are \n",
      "separated by commas. An example of CSV is shown in Figure 12.16. The first row is \n",
      "the header, containing three fields, followed by two rows of data:\n",
      "Figure 12.16: Plaintext values\n",
      "• Avro: This format is similar to JSON; however, it is stored in a binary format rather \n",
      "than a text format:\n",
      "{\n",
      "\t \"firstname\":\t\"Ritesh\",\n",
      "   \"lastname\": \"Modi\",\n",
      " \"email\": \"ritesh.modi@outlook.com\"\n",
      "}\n",
      "However, this does not mean that Stream Analytics can only ingest data using these \n",
      "three formats. It can also create custom .NET-based deserializers, using which any \n",
      "format of data can be ingested, depending upon the deserializers' implementation. \n",
      "The steps you can follow to write a custom deserializer are available at https:/ /docs.\n",
      "microsoft.com/azure/stream-analytics/custom-deserializer-examples.\n",
      "Not only can Stream Analytics receive events, but it also provides advanced query\n",
      "---------------\n",
      "capability for the data that it receives. The queries can extract important insights from \n",
      "the temporal data streams and output them.\n",
      "As shown in Figure 12.17, there is an input dataset and an output dataset; the query \n",
      "moves the events from the input to the output. The INTO clause refers to the output \n",
      "location, and the FROM clause refers to the input location. The queries are very similar to \n",
      "SQL queries, so the learning curve is not too steep for SQL programmers:\n",
      "Figure 12.17: Stream Analytics query for receiving Twitter data\n",
      "---------------\n",
      "A primer on Stream Analytics | 407\n",
      "Event Hubs provides mechanisms for sending outputs from queries to target \n",
      "destinations. At the time of writing, Stream Analytics supports multiple destinations for \n",
      "events and query outputs, as shown before. \n",
      "It is also possible to define custom functions that can be reused within queries. There \n",
      "are four options provided to define custom functions.\n",
      "• Azure Machine Learning\n",
      "• JavaScript user-defined functions\n",
      "• JavaScript user-defined aggregates \n",
      "• Azure Machine Learning studio\n",
      "The hosting environment\n",
      "Stream Analytics jobs can run on hosts that are running on the cloud, or they can run \n",
      "on IoT edge devices. IoT edge devices are devices that are near to IoT sensors, rather \n",
      "than on the cloud. Figure 12.18 shows the New Stream Analytics job pane:\n",
      "Figure 12.18: Creating a new Stream Analytics job\n",
      "Let's check out streaming units in detail.\n",
      "---------------\n",
      "408 | Azure Big Data eventing solutions\n",
      "Streaming units\n",
      "From Figure 12.18, you can see that the only configuration that is unique to Stream \n",
      "Analytics is streaming units. Streaming units refers to the resources (that is, CPU \n",
      "and memory) that are assigned for running a Stream Analytics job. The minimum and \n",
      "maximum streaming units are 1 and 120, respectively.\n",
      "Streaming units must be pre-allocated according to the amount of data and the number \n",
      "of queries executed on that data; otherwise, the job will fail.\n",
      "It is possible to scale streaming units up and down from the Azure portal.\n",
      "A sample application using Event Hubs and Stream Analytics\n",
      "In this section, we will be creating a sample application comprising multiple \n",
      "Azure services, including Azure Logic Apps, Azure Event Hubs, Azure Storage, and \n",
      "Azure Stream Analytics.\n",
      "In this sample application, we will be reading all tweets containing the word \"Azure\" and \n",
      "storing them in an Azure storage account.\n",
      "---------------\n",
      "To create this solution, we first need to provision all the necessary resources.\n",
      "Provisioning a new resource group\n",
      "Navigate to the Azure portal, log in using valid credentials, click on + Create a resource, \n",
      "and search for Resource group. Select Resource group from the search results and \n",
      "create a new resource group. Then, provide a name and choose an appropriate location. \n",
      "Note that all resources should be hosted in the same resource group and location so \n",
      "that it is easy to delete them:\n",
      "Figure 12.19: Provisioning a new resource group in the Azure portal\n",
      "---------------\n",
      "Provisioning a new resource group | 409\n",
      "Next, we will create an Event Hubs namespace.\n",
      "Creating an Event Hubs namespace\n",
      "Click on + Create a resource and search for Event Hubs. Select Event Hubs from \n",
      "the search results and create a new event hub. Then, provide a name and location, \n",
      "and select a subscription based on the resource group that was created earlier. \n",
      "Select Standard as the pricing tier and also select Enable Auto-inflate, as shown in \n",
      "Figure 12.20:\n",
      "Figure 12.20: Creating an Event Hubs namespace\n",
      "---------------\n",
      "410 | Azure Big Data eventing solutions\n",
      "By now, an Event Hubs namespace should have been created. It is a pre-requisite to \n",
      "have a namespace before an event hub can be created. The next step is to provision an \n",
      "event hub.\n",
      "Creating an event hub\n",
      "From the Event Hubs namespace service, click on Events Hubs in the left-\n",
      "hand menu, and then click on + Event hubs to create a new event hub. Name \n",
      "it azuretwitterdata and provide an optimal number of partitions and a Message \n",
      "Retention value:\n",
      "Figure 12.21: Creating the azuretwitterdata event hub\n",
      "After this step, you will have an event hub that can be used to send event data, which \n",
      "is stored in durable storage such as a Data Lake Storage account or an Azure Storage \n",
      "account, to be used by downstream services.\n",
      "---------------\n",
      "Provisioning a new resource group | 411\n",
      "Provisioning a logic app\n",
      "After the resource group is provisioned, click on + Create a resource and search \n",
      "for Logic Apps. Select Logic Apps from the search results and create a new logic app. \n",
      "Then, provide a name and location, and select a subscription based on the resource \n",
      "group created earlier. It is good practice to enable Log Analytics. Logic Apps is covered \n",
      "in more detail in Chapter 11, Azure Solutions using Azure Logic Apps, Event Grid, and \n",
      "Functions. The logic app is responsible for connecting to Twitter using an account and \n",
      "fetching all the tweets with Azure in them:\n",
      "Figure 12.22: Creating a logic app\n",
      "---------------\n",
      "412 | Azure Big Data eventing solutions\n",
      "After the logic app is created, select the When a new tweet is posted trigger on the \n",
      "design surface, sign in, and then configure it as shown in Figure 12.23. You will need a \n",
      "valid Twitter account before configuring this trigger:\n",
      "Figure 12.23: Configuring the frequency of incoming tweets\n",
      "Next, drop a Send event action on the designer surface; this action is responsible for \n",
      "sending tweets to the event hub:\n",
      "Figure 12.24: Adding an action to send tweets to the event hub\n",
      "---------------\n",
      "Provisioning a new resource group | 413\n",
      "Select the name of the event hub that was created in an earlier step.\n",
      "The value specified in the content textbox is an expression that has been dynamically \n",
      "composed using Logic Apps–provided functions and Twitter data. Clicking on Add \n",
      "dynamic content provides a dialog through which the expression can be composed:\n",
      "Figure 12.25: Configuring Logic Apps activity using dynamic expressions\n",
      "The value of the expression is as follows:\n",
      "json(concat('{','tweetdata:' ,'\"',triggerBody()?['TweetText'],'\"', '}'))\n",
      "In the next section, we will provision the storage account.\n",
      "Provisioning the storage account\n",
      "Click on + Create a resource and search for Storage Account. Select Storage \n",
      "Account from the search results and create a new storage account. Then, provide \n",
      "a name and location, and select a subscription based on the resource group that was \n",
      "created earlier. Finally, select StorageV2 for Account Kind, Standard for Performance,\n",
      "---------------\n",
      "and Locally-redundant storage (LRS) for the Replication field.\n",
      "Next, we will create a Blob storage container to store the data coming out of Stream \n",
      "Analytics.\n",
      "Creating a storage container\n",
      "Stream Analytics will output the data as files, which will be stored within a Blob storage \n",
      "container. A container named twitter will be created within Blob storage, as shown in \n",
      "Figure 12.26:\n",
      "---------------\n",
      "414 | Azure Big Data eventing solutions\n",
      "Figure 12.26: Creating a storage container\n",
      "Let's create a new Stream Analytics job with a hosting environment on the cloud and set \n",
      "the streaming units to the default settings.\n",
      "Creating Stream Analytics jobs\n",
      "The input for this Stream Analytics job comes from the event hub, and so we need to \n",
      "configure this from the Inputs menu:\n",
      "Figure 12.27: Creating an input Stream Analytics job\n",
      "---------------\n",
      "Provisioning a new resource group | 415\n",
      "The output for the Stream Analytics job is a Blob storage account, so you need to \n",
      "configure the output accordingly. Provide a path pattern that is suitable for this \n",
      "exercise; for example, {datetime:ss} is the path pattern that we are using for this \n",
      "exercise:\n",
      "Figure 12.28: Creating a Blob storage account as output\n",
      "The query is quite simple; you are just copying the data from the input to the output:\n",
      "Figure 12.29: Query for copying Twitter feeds\n",
      "---------------\n",
      "416 | Azure Big Data eventing solutions\n",
      "While this example just involves copying data, there can be more complex queries for \n",
      "performing transformation before loading data into a destination.\n",
      "This concludes all the steps for the application; now you should be able to run it.\n",
      "Running the application\n",
      "The logic app should be enabled and Stream Analytics should be running. Now, run the \n",
      "logic app; it will create a job to run all the activities within it, as shown in Figure 12.30:\n",
      "Figure 12.30: Overview of the GetAzureTwitterData application\n",
      "---------------\n",
      "Provisioning a new resource group | 417\n",
      "The Storage Account container should get data, as shown in Figure 12.31:\n",
      "Figure 12.31: Checking the Storage Account container data\n",
      "As an exercise, you can extend this sample solution and evaluate the sentiment of the \n",
      "tweets every three minutes. The Logic Apps workflow for such an exercise would be as \n",
      "follows:\n",
      "Figure 12.32: Flowchart for analyzing tweet sentiment\n",
      "---------------\n",
      "418 | Azure Big Data eventing solutions\n",
      "To detect sentiment, you'll need to use the Text Analytics API, which should be \n",
      "configured before being used in Logic Apps.\n",
      "Summary\n",
      "This chapter focused on topics related to the streaming and storage of events. Events \n",
      "have become an important consideration in overall solution architecture. We covered \n",
      "important resources, such as Event Hubs and Stream Analytics, and foundational \n",
      "concepts, such as consumer groups and throughputs, as well as creating an end-to-\n",
      "end solution using them along with Logic Apps. You learned that events are raised from \n",
      "multiple sources, and in order to get insights in real time about activities and their \n",
      "related events, services such as Event Hubs and Stream Analytics play a significant \n",
      "role. In the next chapter, we will learn about integrating Azure DevOps and Jenkins and \n",
      "implementing some of the industry's best practices while developing solutions.\n",
      "---------------\n",
      "In the previous chapter, you learned about big data eventing and its relationship with \n",
      "Azure's Event Hubs and Stream Analytics services. Software development is a complex \n",
      "undertaking comprising multiple processes and tools, and involving people from \n",
      "different departments. They all need to come together and work in a cohesive manner. \n",
      "With so many variables, the risks are high when you are delivering to end customers. \n",
      "One small omission or misconfiguration might lead to the application coming crashing \n",
      "down. This chapter is about adopting and implementing practices that reduce this risk \n",
      "considerably and ensure that high-quality software can be delivered to the customer \n",
      "over and over again.\n",
      "Integrating Azure \n",
      "DevOps\n",
      "13\n",
      "---------------\n",
      "422 | Integrating Azure DevOps\n",
      "Before getting into the details of DevOps, here is a list of the problems faced by \n",
      "software companies that DevOps addresses:\n",
      "• Rigid organizations that don't welcome change\n",
      "• Time-consuming processes\n",
      "• Isolated teams working in silos\n",
      "• Monolithic design and big bang deployments\n",
      "• Manual execution\n",
      "• A lack of innovation\n",
      "In this chapter, we will cover the following topics:\n",
      "• DevOps\n",
      "• DevOps practices\n",
      "• Azure DevOps\n",
      "• DevOps preparation\n",
      "• DevOps for PaaS solutions\n",
      "• DevOps for virtual machine-based (IaaS) solutions\n",
      "• DevOps for container-based (IaaS) solutions\n",
      "• Azure DevOps and Jenkins\n",
      "• Azure Automation\n",
      "• Azure tools for DevOps\n",
      "DevOps\n",
      "There's currently no industry-wide consensus regarding the definition of \n",
      "DevOps. Organizations have formulated their own definition of DevOps and tried \n",
      "to implement it. They have their own perspective and think they've implemented \n",
      "DevOps once they implement automation and configuration management, and use Agile\n",
      "---------------\n",
      "processes.\n",
      "---------------\n",
      "DevOps | 423\n",
      "Based on my experience working on DevOps projects in industry, I have defined DevOps \n",
      "as the following: DevOps is about the delivery mechanism of software systems. It's \n",
      "about bringing people together, making them collaborate and communicate, working \n",
      "together toward a common goal and vision. It's about taking joint responsibility, \n",
      "accountability, and ownership. It's about implementing processes that foster \n",
      "collaboration and a service mindset. It enables delivery mechanisms that bring agility \n",
      "and flexibility to the organization. Contrary to popular belief, DevOps isn't about tools, \n",
      "technology, and automation. These are enablers that help with collaboration, the \n",
      "implementation of Agile processes, and faster and better delivery to the customer.\n",
      "There are multiple definitions available on the internet for DevOps, and they aren't \n",
      "wrong. DevOps doesn't provide a framework or methodology. It's a set of principles and\n",
      "---------------\n",
      "practices that, when employed within an organization, engagement, or project, achieve \n",
      "the goal and vision of both DevOps and the organization. These principles and practices \n",
      "don't mandate any specific processes, tools and technologies, or environments. DevOps \n",
      "provides guidance that can be implemented through any tool, technology, or process, \n",
      "although some of the technology and processes might be more applicable than others \n",
      "to achieve the vision of DevOps' principles and practices.\n",
      "Although DevOps practices can be implemented in any organization that provides \n",
      "services and products to customers, going forward in this book, we'll look at DevOps \n",
      "from the perspective of software development and the operations department of any \n",
      "organization.\n",
      "So, what is DevOps? DevOps is defined as a set of principles and practices bringing all \n",
      "teams, including developers and operations, together from the start of the project for\n",
      "---------------\n",
      "faster, quicker, and more efficient end-to-end delivery of value to the end customer \n",
      "again and again, in a consistent and predictable manner, reducing time to market, \n",
      "thereby gaining a competitive advantage.\n",
      "The preceding definition of DevOps doesn't indicate or refer to any specific processes, \n",
      "tools, or technology. It doesn't prescribe any methodology or environment.\n",
      "The goal of implementing DevOps principles and practices in any organization is to \n",
      "ensure that the demands of stakeholders (including customers) and expectations are \n",
      "met efficiently and effectively.\n",
      "---------------\n",
      "424 | Integrating Azure DevOps\n",
      "Customer demands and expectations are met when the following happens:\n",
      "• The customer gets the features they want\n",
      "• The customer gets the features they want when they want them\n",
      "• The customer gets faster updates on features\n",
      "• The quality of delivery is high\n",
      "When an organization can meet these expectations, customers are happy and remain \n",
      "loyal. This, in turn, increases the market competitiveness of the organization, which \n",
      "results in a bigger brand and market valuation. It has a direct impact on the top and \n",
      "bottom lines of the organization. The organization can invest further in innovation and \n",
      "customer feedback, bringing about continuous changes to its systems and services in \n",
      "order to stay relevant.\n",
      "The implementation of DevOps principles and practices in any organization is guided by \n",
      "its surrounding ecosystem. This ecosystem is made up of the industry and domains the \n",
      "organization belongs to.\n",
      "---------------\n",
      "DevOps is based on a set of principles and practices. We'll look into the details of these \n",
      "principles and practices later in this chapter. The core principles of DevOps are: \n",
      "• Agility: Being Agile increases the overall flexibility to changes and ensures that \n",
      "adaptability increases to every changing environment and being productive. Agile \n",
      "processes have a shorter work duration and it's easy to find issues earlier in the \n",
      "development life cycle rather than much later, thereby reducing the technical \n",
      "debt.\n",
      "• Automation: The adoption of tools and automation increases the overall efficiency \n",
      "and predictability of the process and end product. It helps in doing things faster \n",
      "and in an easier and cheaper manner.\n",
      "• Collaboration: Collaboration refers to a common repository, the rotation of \n",
      "work responsibilities, the sharing of data and information, and other aspects that \n",
      "improve the productivity of each member of the team, thereby supporting the\n",
      "---------------\n",
      "overall effective delivery of the product.\n",
      "• Feedback: This refers to quick and early feedback loops between multiple teams \n",
      "about things that work and things that don't work. It helps teams to prioritize \n",
      "issues and fix them in subsequent releases.\n",
      "---------------\n",
      "The essence of DevOps | 425\n",
      "The core DevOps practices are:\n",
      "• Continuous integration: This refers to the process of validating and verifying the \n",
      "quality and correctness of the code pushed within the repository by developers. It \n",
      "can be scheduled, manual, or continuous. Continuous means that the process will \n",
      "check for various quality attributes each time a developer pushes the code, while \n",
      "scheduled means on a given time schedule, the checks will be conducted. Manual \n",
      "refers to manual execution by an administrator or developer.\n",
      "• Configuration management: This is an important facet of DevOps and provides \n",
      "guidance for configuring infrastructure and applications either by pulling \n",
      "configurations from configuration management servers or by pushing these \n",
      "configurations on a schedule. Configuration management should bring back the \n",
      "environment to the expected desired state every time it gets executed.\n",
      "• Continuous delivery: Continuous delivery refers to the state of readiness\n",
      "---------------\n",
      "of an application to be able to be deployed in any existing, as well as a new, \n",
      "environment. It is generally executed by means of a release definition in lower \n",
      "environments like development and testing.\n",
      "• Continuous deployment: Continuous deployment refers to the ability to deploy \n",
      "the environment and application in production automatically. It is generally \n",
      "executed by means of a release definition in the production environment.\n",
      "• Continuous learning: This refers to the process of understanding the issues \n",
      "faced by operations and customers and ensuring that they get communicated to \n",
      "development and testing teams such that they can fix those issues in subsequent \n",
      "releases to improve the overall health and usability of the application.\n",
      "The essence of DevOps\n",
      "DevOps is not a new paradigm; however, it's gaining a lot of popularity and traction. \n",
      "Its adoption is at its highest level, and more and more companies are undertaking this\n",
      "---------------\n",
      "journey. I purposely mentioned DevOps as a journey because there are different levels \n",
      "of maturity within DevOps. While successfully implementing continuous deployment \n",
      "and delivery are considered the highest level of maturity in this journey, adopting \n",
      "source code control and Agile software development are considered the first step in the \n",
      "DevOps journey.\n",
      "---------------\n",
      "426 | Integrating Azure DevOps\n",
      "One of the first things DevOps talks about is breaking down the barriers between the \n",
      "development and the operations teams. It brings about close collaboration between \n",
      "multiple teams. It's about breaking the mindset that the developer is responsible for \n",
      "writing the code only and passing it on to operations for deployment once it's tested. \n",
      "It's also about breaking the mindset that operations have no role to play in development \n",
      "activities. Operations should influence the planning of the product and should be aware \n",
      "of the features coming up as releases. They should also continually provide feedback to \n",
      "the developers on operational issues such that they can be fixed in subsequent releases. \n",
      "They should influence the design of the system to improve the operational working of \n",
      "the system. Similarly, developers should help the operations team to deploy the system \n",
      "and solve incidents when they arise.\n",
      "---------------\n",
      "The definition of DevOps talks about faster and more efficient end-to-end delivery \n",
      "of systems to stakeholders. It doesn't talk about how fast or efficient the delivery \n",
      "should be. It should be fast enough for the organization's domain, industry, customer \n",
      "segmentation, and needs. For some organizations, quarterly releases are good enough, \n",
      "while for others it could be weekly. Both are valid from a DevOps point of view, and \n",
      "these organizations can deploy relevant processes and technologies to achieve \n",
      "their target release deadlines. DevOps doesn't mandate any specific time frame for \n",
      "continuous integration/continuous deployment (CI/CD). Organizations should \n",
      "identify the best implementation of DevOps principles and practices based on their \n",
      "overall project, engagement, and organizational vision.\n",
      "The definition also talks about end-to-end delivery. This means that everything from \n",
      "the planning and delivery of the system through to the services and operations should\n",
      "---------------\n",
      "be part of DevOps adoption. Processes should allow greater flexibility, modularity, and \n",
      "agility in the application development life cycle. While organizations are free to use the \n",
      "best fitting process—Waterfall, Agile, Scrum, or another—typically, organizations tend to \n",
      "favor Agile processes with iteration-based delivery. This allows faster delivery in smaller \n",
      "units, which are far more testable and manageable compared to a large delivery.\n",
      "DevOps repeatedly talks about end customers in a consistent and predictable manner. \n",
      "This means that organizations should continually deliver to customers with newer and \n",
      "upgraded features using automation. We can't achieve consistency and predictability \n",
      "without the use of automation. Manual work should be non-existent to ensure a high \n",
      "level of consistency and predictability. Automation should also be end-to-end, to avoid \n",
      "failures. This also indicates that the system design should be modular, allowing faster\n",
      "---------------\n",
      "delivery on systems that are reliable, available, and scalable. Testing plays a big role in \n",
      "consistent and predictable delivery.\n",
      "---------------\n",
      "DevOps practices | 427\n",
      "The end result of implementing these practices and principles is that the organization \n",
      "is able to meet the expectations and demands of customers. The organization is able \n",
      "to grow faster than the competition, and further increase the quality and capability of \n",
      "their product and services through continuous innovation and improvement.\n",
      "Now that you understand the idea behind DevOps, it's time to look into core DevOps \n",
      "practices.\n",
      "DevOps practices\n",
      "DevOps consists of multiple practices, each providing a distinct functionality to \n",
      "the overall process. The following diagram shows the relationship between them. \n",
      "Configuration management, continuous integration, and continuous deployment \n",
      "form the core practices that enable DevOps. When we deliver software services that \n",
      "combine these three services, we achieve continuous delivery. Continuous delivery is \n",
      "the capability and level of maturity of an organization that's dependent on the maturity\n",
      "---------------\n",
      "of configuration management, continuous integration, and continuous deployment. \n",
      "Continuous feedback, at all stages, forms the feedback loop that helps to provide \n",
      "superior services to customers. It runs across all DevOps practices. Let's deep dive into \n",
      "each of these capabilities and DevOps practices:\n",
      "Figure 13.1:  DevOps practices\n",
      "---------------\n",
      "428 | Integrating Azure DevOps\n",
      "Configuration management\n",
      "Business applications and services need an environment in which they can be \n",
      "deployed. Typically, the environment is an infrastructure composed of multiple \n",
      "servers, computers, network, storage, containers, and many more services \n",
      "working together such that business applications can be deployed on top of them. \n",
      "Business applications are decomposed into multiple services running on multiple \n",
      "servers, either on-premises or on the cloud, and each service has its own configuration \n",
      "along with requirements related to the infrastructure's configuration. In short, both \n",
      "the infrastructure and the application are needed to deliver systems to customers, \n",
      "and both of them have their own configuration. If the configuration drifts, the \n",
      "application might not work as expected, leading to downtime and failure. Moreover, \n",
      "as the Application Lifecycle Management (ALM) process dictates the use of multiple\n",
      "---------------\n",
      "stages and environments, an application would be deployed to multiple environments \n",
      "with different configurations. The application would be deployed to the development \n",
      "environment for developers to see the result of their work. It would then be deployed to \n",
      "multiple test environments with different configurations for functional tests, load and \n",
      "stress tests, performance tests, integration tests, and more; it would also be deployed to \n",
      "the preproduction environment to conduct user-acceptance tests, and finally into the \n",
      "production environment. It's important that an application can be deployed to multiple \n",
      "environments without undertaking any manual changes to its configuration.\n",
      "Configuration management provides a set of processes and tools and they help to \n",
      "ensure that each environment and application gets its own configuration. Configuration \n",
      "management tracks configuration items, and anything that changes from environment\n",
      "---------------\n",
      "to environment should be treated as a configuration item. Configuration management \n",
      "also defines the relationships between configuration items and how changes in one \n",
      "configuration item will impact other configuration items.\n",
      "Usage of configuration management\n",
      "Configuration management helps in the following places:\n",
      "• Infrastructure as Code: When the process of provisioning infrastructure and \n",
      "its configuration is represented through code, and the same code goes through \n",
      "the application life cycle process, it's known as Infrastructure as Code (IaC). IaC \n",
      "helps to automate the provisioning and configuration of infrastructure. It also \n",
      "represents the entire infrastructure in code that can be stored in a repository \n",
      "and version-controlled. This allows users to employ the previous environment's \n",
      "configurations when needed. It also enables the provisioning of an environment \n",
      "multiple times in a consistent and predictable manner. All environments\n",
      "---------------\n",
      "provisioned in this way are consistent and equal in all ALM stages. There are many \n",
      "tools that help in achieving IaC, including ARM templates, Ansible, and Terraform.\n",
      "---------------\n",
      "DevOps practices | 429\n",
      "• Deploying and configuring the application: The deployment of an application \n",
      "and its configuration is the next step after provisioning the infrastructure. \n",
      "Examples include deploying a webdeploy package on a server, deploying a SQL \n",
      "server schema and data (bacpac) on another server, and changing the SQL \n",
      "connection string on the web server to represent the appropriate SQL server. \n",
      "Configuration management stores values for the application's configuration for \n",
      "each environment on which it is deployed.\n",
      "The configuration applied should also be monitored. The expected and desired \n",
      "configuration should be consistently maintained. Any drift from this expected and \n",
      "desired configuration would render the application unavailable. Configuration \n",
      "management is also capable of finding the drift and re-configuring the application and \n",
      "environment to its desired state.\n",
      "With automated configuration management in place, nobody on the team has to deploy\n",
      "---------------\n",
      "and configure environments and applications in production. The operations team isn't \n",
      "reliant on the development team or long deployment documentation.\n",
      "Another aspect of configuration management is source code control. Business \n",
      "applications and services comprise code and other artifacts. Multiple team members \n",
      "work on the same files. The source code should always be up to date and should \n",
      "be accessible by only authenticated team members. The code and other artifacts \n",
      "by themselves are configuration items. Source control helps in collaboration and \n",
      "communication within the team since everybody is aware of what everyone else is \n",
      "doing and conflicts are resolved at an early stage.\n",
      "Configuration management can be broadly divided into two categories:\n",
      "• Inside the virtual machine\n",
      "• Outside the virtual machine\n",
      "Configuration management tools\n",
      "The tools available for configuration management inside the virtual machine are \n",
      "discussed next.\n",
      "Desired State Configuration\n",
      "---------------\n",
      "Desired State Configuration (DSC) is a configuration-management platform from \n",
      "Microsoft, built as an extension to PowerShell. DSC was originally launched as part \n",
      "of Windows Management Framework (WMF) 4.0. It's available as part of WMF 4.0 and \n",
      "5.0 for all Windows Server operating systems before Windows 2008 R2. WMF 5.1 is \n",
      "available out of the box on Windows Server 2016/2019 and Windows 10.\n",
      "---------------\n",
      "430 | Integrating Azure DevOps\n",
      "Chef, Puppet, and Ansible\n",
      "Apart from DSC, there's a host of configuration-management tools, such \n",
      "as Chef, Puppet, and Ansible, supported by Azure. Details about these tools \n",
      "aren't covered in this book. Read more about them here: https:/ /docs.microsoft.com/\n",
      "azure/virtual-machines/windows/infrastructure-automation.\n",
      "The tools available for configuration management outside of a virtual machine are \n",
      "mentioned next.\n",
      "ARM templates\n",
      "ARM templates are the primary means of provisioning resources in ARM. ARM \n",
      "templates provide a declarative model through which resources and their configuration, \n",
      "scripts, and extensions are specified. ARM templates are based on JavaScript Object \n",
      "Notation (JSON) format. It uses JSON syntax and conventions to declare and configure \n",
      "resources. JSON files are text-based, user friendly, and easily readable. They can be \n",
      "stored in a source code repository and have version control on them. They are also a\n",
      "---------------\n",
      "means to represent infrastructure as code that can be used to provision resources in \n",
      "Azure resource groups over and over again, predictably, consistently, and uniformly. \n",
      "Templates provide the flexibility to be generic and modular in their design and \n",
      "implementation. Templates give us the ability to accept parameters from users, declare \n",
      "internal variables, help define dependencies between resources, link resources within \n",
      "the same or different resource groups, and execute other templates. They also provide \n",
      "scripting language-type expressions and functions that make them dynamic and \n",
      "customizable at runtime. There are two chapters dedicated to ARM templates in this \n",
      "book: Chapters 15, Cross Subscription Deployments Using ARM Templates, and Chapter \n",
      "16, ARM Templates Modular Design and Implementation.\n",
      "Now, it's time to focus on the next important DevOps principle: continuous integration.\n",
      "Continuous integration\n",
      "---------------\n",
      "Multiple developers write code that's eventually stored in a common repository. The \n",
      "code is normally checked in or pushed to the repository when the developers have \n",
      "finished developing their features. This can happen in a day or might take days or weeks. \n",
      "Some of the developers might be working on the same feature, and they might also \n",
      "follow the same practices of pushing/checking in code in days or weeks. This can create \n",
      "issues with the quality of the code. One of the tenets of DevOps is to fail fast. Developers \n",
      "should check in/push their code to the repository often and compile the code to check \n",
      "whether they've introduced bugs and that the code is compatible with the code written \n",
      "by their colleagues. If a developer doesn't follow this practice, the code on their machine \n",
      "will grow too large and will be difficult to integrate with other code. Moreover, if the \n",
      "compile fails, it's difficult and time-consuming to fix the issues that arise.\n",
      "---------------\n",
      "DevOps practices | 431\n",
      "Code integration\n",
      "Continuous integration solves these kinds of challenges. Continuous integration helps \n",
      "in compiling and validating the code pushed/checked in by a developer by taking it \n",
      "through a series of validation steps. Continuous integration creates a process flow \n",
      "that consists of multiple steps. Continuous integration is composed of continuous \n",
      "automated builds and continuous automated tests. Normally, the first step is compiling \n",
      "the code. After the successful compilation, each step is responsible for validating \n",
      "the code from a specific perspective. For example, unit tests can be executed on the \n",
      "compiled code, and then code coverage can be executed to check which code paths \n",
      "are executed by unit tests. These could reveal whether comprehensive unit tests are \n",
      "written or whether there's scope to add further unit tests. The end result of continuous \n",
      "integration is deployment packages that can be used by continuous deployment to\n",
      "---------------\n",
      "deploy them to multiple environments.\n",
      "Frequent code push\n",
      "Developers are encouraged to check in their code multiple times a day, instead of doing \n",
      "so after days or weeks. Continuous integration initiates the execution of the entire \n",
      "pipeline as soon as the code is checked in or pushed. If compilation succeeds, code \n",
      "tests, and other activities that are part of the pipeline, are executed without error; the \n",
      "code is deployed to a test environment and integration tests are executed on it.\n",
      "Increased productivity\n",
      "Continuous integration increases developer productivity. They don't have to manually \n",
      "compile their code, run multiple types of tests one after another, and then create \n",
      "packages out of it. It also reduces the risk of getting bugs introduced in the code and \n",
      "the code doesn't get stale. It provides early feedback to the developers about the quality \n",
      "of their code. Overall, the quality of deliverables is high and they are delivered faster by\n",
      "---------------\n",
      "adopting continuous integration practices. A sample continuous integration pipeline is \n",
      "shown here:\n",
      "Figure 13.2: Continuous integration pipeline\n",
      "---------------\n",
      "432 | Integrating Azure DevOps\n",
      "Build automation\n",
      "Build automation consists of multiple tasks executing in sequence. Generally, the \n",
      "first task is responsible for fetching the latest source code from the repository. The \n",
      "source code might comprise multiple projects and files. They are compiled to generate \n",
      "artifacts, such as executables, dynamic link libraries, and assemblies. Successful build \n",
      "automation reflects that there are no compile-time errors in the code.\n",
      "There could be more steps to build automation, depending on the nature and type of \n",
      "the project.\n",
      "Test automation\n",
      "Test automation consists of tasks that are responsible for validating different aspects \n",
      "of code. These tasks are related to testing code from a different perspective and are \n",
      "executed in sequence. Generally, the first step is to run a series of unit tests on the \n",
      "code. Unit testing refers to the process of testing the smallest denomination of a\n",
      "---------------\n",
      "feature by validating its behavior in isolation from other features. It can be automated \n",
      "or manual; however, the preference is toward automated unit testing.\n",
      "Code coverage is another type of automated testing that can be executed on code \n",
      "to find out how much of the code is executed when running unit tests. It's generally \n",
      "represented as a percentage and refers to how much code is testable through unit \n",
      "testing. If the code coverage isn't close to 100%, it's either because the developer hasn't \n",
      "written unit tests for that behavior or the uncovered code isn't required at all.\n",
      "The successful execution of test automation, resulting in no significant code failure, \n",
      "should start executing the packaging tasks. There could be more steps to test \n",
      "automation depending on the nature and type of the project.\n",
      "Packaging\n",
      "Packaging refers to the process of generating deployable artifacts, such as MSI, NuGet, \n",
      "and webdeploy packages, and database packages; versioning them; and then storing\n",
      "---------------\n",
      "them in a location such that they can be consumed by other pipelines and processes.\n",
      "Once the process of continuous integration completes, the process of continuous \n",
      "deployment starts, and that will be the focus of the next section.\n",
      "---------------\n",
      "DevOps practices | 433\n",
      "Continuous deployment\n",
      "By the time the process reaches continuous deployment, continuous integration \n",
      "has ensured that we have fully working bits of an application that can now be taken \n",
      "through different continuous deployment activities. Continuous deployment refers to \n",
      "the capability of deploying business applications and services to preproduction and \n",
      "production environments through automation. For example, continuous deployment \n",
      "could provision and configure the preproduction environment, deploy applications to it, \n",
      "and configure the applications. After conducting multiple validations, such as functional \n",
      "tests and performance tests on the preproduction environment, the production \n",
      "environment is provisioned, configured, and the application is deployed through \n",
      "automation. There are no manual steps in the deployment process. Every deployment \n",
      "task is automated. Continuous deployment can provision the environment and deploy\n",
      "---------------\n",
      "the application from scratch, while it can just deploy delta changes to an existing \n",
      "environment if the environment already exists.\n",
      "All environments are provisioned through automation using IaC. This ensures that \n",
      "all environments, whether development, test, preproduction, or production, are the \n",
      "same. Similarly, the application is deployed through automation, ensuring that it's \n",
      "also deployed uniformly across all environments. The configuration across these \n",
      "environments could be different for the application.\n",
      "Continuous deployment is generally integrated with continuous integration. When \n",
      "continuous integration has done its work, by generating the final deployable packages, \n",
      "continuous deployment kicks in and starts its own pipeline. This pipeline is called \n",
      "the release pipeline. The release pipeline consists of multiple environments, with \n",
      "each environment consisting of tasks responsible for provisioning the environment,\n",
      "---------------\n",
      "configuring the environment, deploying applications, configuring applications, \n",
      "executing operational validation on environments, and testing the application on \n",
      "multiple environments.\n",
      "Employing continuous deployment provides immense benefits. There is a high level of \n",
      "confidence in the overall deployment process, which helps with faster and risk-free \n",
      "releases on production. The chances of anything going wrong decrease drastically. The \n",
      "team will be less stressed, and rollback to the previous working environment is possible \n",
      "if there are issues with the current release:\n",
      "Figure 13.3: Continuous deployment pipeline\n",
      "Staging \n",
      "environment\n",
      "Test \n",
      "automation\n",
      "Production \n",
      "environment\n",
      "Performance \n",
      "tests environment application\n",
      "Provision \n",
      "environment\n",
      "Deploy \n",
      "packages\n",
      "environment application\n",
      "Functional \n",
      "tests\n",
      "Acceptance \n",
      "tests\n",
      "Provision \n",
      "environment\n",
      "Deploy \n",
      "packages\n",
      "---------------\n",
      "434 | Integrating Azure DevOps\n",
      "Although every system demands its own configuration of the release pipeline, an \n",
      "example of continuous deployment is shown in the preceding diagram. It's important \n",
      "to note that, generally, provisioning and configuring multiple environments is part \n",
      "of the release pipeline, and approvals should be sought before moving to the next \n",
      "environment. The approval process might be manual or automated, depending on the \n",
      "maturity of the organization.\n",
      "Next, we will look into aspects related to the test environment.\n",
      "Test environment deployment\n",
      "The release pipeline starts once the drop is available from continuous integration \n",
      "and the first step it should take is to get all the artifacts from the drop. After this, the \n",
      "release pipeline might create a completely new bare-metal test environment or reuse \n",
      "an existing one. This is again dependent on the type of project and the nature of the\n",
      "---------------\n",
      "testing planned to be executed in this environment. The environment is provisioned \n",
      "and configured. The application artifacts are deployed and configured.\n",
      "Test automation\n",
      "After deploying an application, a series of tests can be performed on the environment. \n",
      "One of the tests executed here is a functional test. Functional tests are primarily aimed \n",
      "at validating the feature completeness and functionality of the application. These tests \n",
      "are written from requirements gathered from the customer. Another set of tests that \n",
      "can be executed is related to the scalability and availability of the application. This \n",
      "typically includes load tests, stress tests, and performance tests. It should also include \n",
      "an operational validation of the infrastructure environment.\n",
      "Staging environment deployment\n",
      "This is very similar to the test environment deployment, the only difference being that \n",
      "the configuration values for the environment and application would be different.\n",
      "Acceptance tests\n",
      "---------------\n",
      "Acceptance tests are generally conducted by application stakeholders, and these can be \n",
      "manual or automated. This step is a validation from the customer's point of view about \n",
      "the correctness and completeness of the application's functionality.\n",
      "Deployment to production\n",
      "Once the customer gives their approval, the same steps as that of the test and staging \n",
      "environment deployment are executed, the only difference being that the configuration \n",
      "values for the environment and application are specific to the production environment. \n",
      "A validation is conducted after deployment to ensure that the application is running \n",
      "according to expectations.\n",
      "---------------\n",
      "DevOps practices | 435\n",
      "Continuous delivery is an important DevOps principle and closely resembles continuous \n",
      "deployment; however, there are a few differences. In the next section, we will look into \n",
      "continuous delivery.\n",
      "Continuous delivery\n",
      "Continuous delivery and continuous deployment might sound similar to you; however, \n",
      "they aren't the same. While continuous deployment talks about deployment to \n",
      "multiple environments and finally to the production environment through automation, \n",
      "continuous delivery is the ability to generate application packages that are readily \n",
      "deployable in any environment. To generate artifacts that are readily deployable, \n",
      "continuous integration should be used to generate the application artifacts; a new or \n",
      "existing environment should be used to deploy these artifacts and conduct functional \n",
      "tests, performance tests, and user-acceptance tests through automation. Once these \n",
      "activities are successfully executed without any errors, the application package is\n",
      "---------------\n",
      "considered readily deployable. Continuous delivery includes continuous integration \n",
      "and deployment to an environment for final validations. It helps get feedback more \n",
      "quickly from both the operations and the end user. This feedback can then be used to \n",
      "implement subsequent iterations.\n",
      "In the next section, we will look into continuous learning.\n",
      "Continuous learning\n",
      "With all the previously mentioned DevOps practices, it's possible to create great \n",
      "business applications and deploy them automatically to the production environment; \n",
      "however, the benefits of DevOps won't last for long if continuous improvement and \n",
      "feedback principles are not in place. It's of the utmost importance that real-time \n",
      "feedback about the application behavior is passed on as feedback to the development \n",
      "team from both end users and the operations team.\n",
      "Feedback should be passed to the teams, providing relevant information about what's \n",
      "going well and what isn't.\n",
      "---------------\n",
      "An application's architecture and design should be built with monitoring, auditing, \n",
      "and telemetry in mind. The operations team should collect telemetry information \n",
      "from the production environment, capturing any bugs and issues, and pass it on to the \n",
      "development team so that it can be fixed for subsequent releases.\n",
      "---------------\n",
      "436 | Integrating Azure DevOps\n",
      "Continuous learning helps to make the application robust and resilient to failure. It \n",
      "helps in making sure that the application is meeting consumer requirements. Figure 13.4  \n",
      "shows the feedback loop that should be implemented between different teams:\n",
      "Figure 13.4: Feedback loop\n",
      "After going through the important practices related to DevOps, now it's time to get into \n",
      "tools and services that make these possible. \n",
      "Azure DevOps\n",
      "Let's look at another top-of-the-line online service that enables continuous integration, \n",
      "continuous deployment, and continuous delivery seamlessly: Azure DevOps. In fact, it \n",
      "would be more appropriate to call it a suite of services available under a single name. \n",
      "Azure DevOps is a PaaS provided by Microsoft and hosted on the cloud. The same \n",
      "service is available as Team Foundation Services (TFS) on-premises. All examples \n",
      "shown in this book use Azure DevOps.\n",
      "---------------\n",
      "According to Microsoft, Azure DevOps is a cloud-based collaboration platform that \n",
      "helps teams to share code, track work, and ship software. Azure DevOps is a new \n",
      "name; earlier, it was known as Visual Studio Team Services (VSTS). Azure DevOps is an \n",
      "enterprise software-development tool and service that enables organizations to provide \n",
      "automation facilities to their end-to-end application life cycle management process, \n",
      "from planning to deploying applications, and getting real-time feedback from software \n",
      "systems. This increases the maturity and capability of an organization to deliver high-\n",
      "quality software systems to their customers.\n",
      "Successful software delivery involves efficiently bringing numerous processes and \n",
      "activities together. These include executing and implementing various Agile processes, \n",
      "increasing collaboration among teams, the seamless and automatic transition of \n",
      "artifacts from one phase of the ALM to another phase, and deployments to multiple\n",
      "---------------\n",
      "environments. It's important to track and report on these activities to measure and \n",
      "improve delivery processes. Azure DevOps makes this simple and easy. It provides a \n",
      "whole suite of services that enables the following:\n",
      "Monitoring Issues Feedback\n",
      "Telemetry Bugs Communication\n",
      "---------------\n",
      "Azure DevOps | 437\n",
      "• Collaboration among every team member by providing a single interface for the \n",
      "entire application life cycle management\n",
      "• Collaboration among development teams using source-code-management \n",
      "services\n",
      "• Collaboration among test teams using test-management services\n",
      "• Automatic validation of code and packaging through continuous integration using \n",
      "build-management services\n",
      "• Automatic validation of application functionality, deployment, and configuration \n",
      "of multiple environments through continuous deployment and delivery using \n",
      "release-management services\n",
      "• Tracking and work-item management using work-management services\n",
      "The following table shows all the services available to a typical project from the Azure \n",
      "DevOps left navigation bar:\n",
      "Table 13.1: A list of Azure DevOps services\n",
      "Service Description\n",
      "Boards\n",
      "Boards helps in the planning of the project by displaying the current progress of\n",
      "---------------\n",
      "tasks, backlogs, and user stories alongside sprint information. It also provides a \n",
      "Kanban process and helps depict the current tasks in progress and completed.\n",
      "Repos\n",
      "Repos helps in managing repositories. It provides support with creating \n",
      "managing permissions. There can be multiple repositories within a project.\n",
      "Pipelines\n",
      "Both release and build pipelines are created and managed from Pipelines. It \n",
      "helps in automating the build and release process. There can be multiple build \n",
      "and release pipelines within a project.\n",
      "Test Plans All testing-related artifacts along with their management are available from  \n",
      "Test Plans. \n",
      "Artifacts NuGet packages and other artifacts are stored and managed here.\n",
      "---------------\n",
      "438 | Integrating Azure DevOps\n",
      "An organization in Azure DevOps serves as a security boundary and logical container \n",
      "that provides all the services that are needed to implement a DevOps strategy. Azure \n",
      "DevOps allows the creation of multiple projects within a single organization. By default, \n",
      "a repository is created with the creation of a project; however, Azure DevOps allows the \n",
      "creation of additional repositories within a single project. The relationship between an \n",
      "Azure DevOps Organization, Projects, and a Repository is shown in Figure 13.5:\n",
      "Figure 13.5: Relationship between Azure DevOps Organization, Projects, and Repository\n",
      "Azure DevOps provides two types of repositories:\n",
      "• Git\n",
      "• Team Foundation Version Control (TFVC)\n",
      "It also provides the flexibility to choose between the Git or TFVC source-control \n",
      "repository. There can be a combination of TFS and TFVC repositories available within a \n",
      "single project.\n",
      "---------------\n",
      "Azure DevOps | 439\n",
      "TFVC\n",
      "TFVC is the traditional and centralized way of implementing version control, where \n",
      "there's a central repository and developers work on it directly in connected mode to \n",
      "check in their changes. If the central repository is offline or unavailable, developers \n",
      "can't check in their code and have to wait for it to be online and available. Other \n",
      "developers can see only the checked-in code. Developers can group multiple changes \n",
      "into a single changeset for checking in code changes that are logically grouped to \n",
      "form a single change. TFVC locks the code files that are undergoing edits. Other \n",
      "developers can read a locked file, but they can't edit it. They must wait for the prior \n",
      "edit to complete and release the lock before they can edit. The history of check-ins and \n",
      "changes is maintained on the central repository, while the developers have the working \n",
      "copy of the files but not the history.\n",
      "---------------\n",
      "TFVC works very well with large teams that are working on the same projects. This \n",
      "enables control over the source code at a central location. It also works best for long-\n",
      "duration projects since the history can be managed at a central location. TFVC has no \n",
      "issues working with large and binary files.\n",
      "Git\n",
      "Git, on the other hand, is a modern, distributed way of implementing version control, \n",
      "where developers can work on their own local copies of code and history in offline \n",
      "mode. Developers can work offline on their local clone of code. Each developer has a \n",
      "local copy of code and its entire history, and they work on their changes with this local \n",
      "repository. They can commit their code to the local repository. They can connect to \n",
      "the central repository for the synchronization of their local repository on a per-need \n",
      "basis. This allows every developer to work on any file since they would be working on\n",
      "---------------\n",
      "their local copy. Branching in Git doesn't create another copy of the original code and is \n",
      "extremely fast to create.\n",
      "Git works well with both small and large teams. Branching and merging is a breeze with \n",
      "the advanced options that Git has.\n",
      "Git is the recommended way of using source control because of the rich functionality \n",
      "it provides. We'll use Git as the repository for our sample application in this book. In \n",
      "the next section, we will have a detailed overview of implementing automation through \n",
      "DevOps.\n",
      "---------------\n",
      "440 | Integrating Azure DevOps\n",
      "Preparing for DevOps\n",
      "Going forward, our focus will be on process and deployment automation using different \n",
      "patterns in Azure. These include the following:\n",
      "• DevOps for IaaS solutions\n",
      "• DevOps for PaaS solutions\n",
      "• DevOps for container-based solutions\n",
      "Generally, there are shared services that aren't unique to any one application. These \n",
      "services are consumed by multiple applications from different environments, such as \n",
      "development, testing, and production. The life cycle of these shared services is different \n",
      "for each application. Therefore, they have different version-control repositories, a \n",
      "different code base, and build and release management. They have their own cycle of \n",
      "plan, design, build, test, and release.\n",
      "The resources that are part of this group are provisioned using ARM templates, \n",
      "PowerShell, and DSC configurations.\n",
      "The overall flow for building these common components is shown here:\n",
      "Figure 13.6: Overall flow for building common components\n",
      "---------------\n",
      "Push code \n",
      "to VSTS Git \n",
      "branch or \n",
      "raise Pull \n",
      "request\n",
      "Code quality \n",
      "checks\n",
      "Run \n",
      "Integration \n",
      "tests\n",
      "Push code \n",
      "to VSTS Git \n",
      "branch or \n",
      "raise Pull \n",
      "request\n",
      "If build fails, \n",
      "developer \n",
      "reworks on \n",
      "code\n",
      "VSTS \n",
      "provides \n",
      "feedback to \n",
      "developer\n",
      "Generate \n",
      "build \n",
      "artifacts and \n",
      "drop them\n",
      "Compile  \n",
      "the code\n",
      "Execute unit \n",
      "tests\n",
      "Generate \n",
      "build label\n",
      "Push \n",
      "changes to \n",
      "shared repo\n",
      "VSTS CI \n",
      "kicks in and \n",
      "starts build \n",
      "pipeline\n",
      "---------------\n",
      "Preparing for DevOps | 441\n",
      "The release process is shown in Figure 13.7:\n",
      "Figure 13.7: Release process\n",
      "On the DevOps journey, it's important to understand and provision the common \n",
      "components and services before starting any software engagement, product, or service.\n",
      "The first step in getting started with Azure DevOps is to provision an organization.\n",
      "Azure DevOps organizations\n",
      "A version-control system is needed to collaborate at the code level. Azure DevOps \n",
      "provides both centralized and decentralized versions of control systems. Azure DevOps \n",
      "also provides orchestration services for building and executing build and release \n",
      "pipelines. It's a mature platform that organizes all DevOps-related version control and \n",
      "builds and releases work-item-related artifacts. After an organization is provisioned in \n",
      "Azure DevOps, an Azure DevOps project should be created to hold all project-related \n",
      "artifacts.\n",
      "An Azure DevOps organization can be provisioned by visiting https:/ /dev.azure.com.\n",
      "Build\n",
      "---------------\n",
      "pipeline ran \n",
      "successfully\n",
      "Deploy \n",
      "to test \n",
      "environment\n",
      "Perform \n",
      "test activities\n",
      "Success or \n",
      "rollback\n",
      "Perform \n",
      "operational \n",
      "validation on \n",
      "production\n",
      "Deploy to \n",
      "production\n",
      "Create/\n",
      "update next \n",
      "environment\n",
      "Start \n",
      "deploying \n",
      "environment\n",
      "Create test \n",
      "environment \n",
      "ARM \n",
      "templates\n",
      "Approval for \n",
      "deploying \n",
      "to next/\n",
      "production\n",
      "Build \n",
      "pipeline \n",
      "generated \n",
      "artifacts\n",
      "Execute \n",
      "release \n",
      "pipeline\n",
      "---------------\n",
      "442 | Integrating Azure DevOps\n",
      "An Azure DevOps organization is the top-level administrative and management \n",
      "boundary that provides security, access, and collaboration between team members \n",
      "belonging to an organization. There can be multiple projects within an organization and \n",
      "each project comprises multiple teams.\n",
      "Provisioning Azure Key Vault\n",
      "It isn't advisable to store secrets, certificates, credentials, or other sensitive \n",
      "information in code configuration files, databases, or any other general storage system. \n",
      "It's advised to store this important data in a vault that's specifically designed for storing \n",
      "secrets and credentials. Azure Key Vault provides such a service. Azure Key Vault is \n",
      "available as a resource and service from Azure. Now, let's move on to exploring the \n",
      "storage options for configurations.\n",
      "Provisioning a configuration-management server/service\n",
      "A configuration-management server/service that provides storage for configurations\n",
      "---------------\n",
      "and applies those configurations to different environments is always a good strategy \n",
      "for automating deployments. DSC on custom virtual machines and DSC from Azure \n",
      "Automation, Chef, Puppet, and Ansible are some options and can be used on Azure \n",
      "seamlessly for both Windows as well as Linux environments. This book uses DSC as a \n",
      "configuration-management tool for all purposes, and it provides a pull server that holds \n",
      "all configuration documents (MOF files) for the sample application. It also maintains \n",
      "the database of all virtual machines and containers that are configured and registered \n",
      "with the pull server to pull configuration documents from it. The local configuration \n",
      "manager on these targets virtual machines, and containers periodically check the \n",
      "availability of new configurations as well as drifts in the current configuration and \n",
      "report back to the pull server. It also has built-in reporting capabilities that provide\n",
      "---------------\n",
      "information about nodes that are compliant, as well as those that are non-compliant, \n",
      "within a virtual machine. A pull server is a general web application that hosts the DSC \n",
      "pull server endpoint. In the next topic, we will discuss a technique to monitor processes \n",
      "in real time with Log Analytics.\n",
      "---------------\n",
      "Preparing for DevOps | 443\n",
      "Log Analytics\n",
      "Log Analytics is an audit and monitoring service provided by Azure to get real-time \n",
      "information about all changes, drifts, and events occurring within virtual machines and \n",
      "containers. It provides a centralized workspace and dashboard for IT administrators \n",
      "for viewing, searching, and conducting drill-down searches on all changes, drifts, and \n",
      "events that occur on these virtual machines. It also provides agents that are deployed \n",
      "on target virtual machines and containers. Once deployed, these agents start sending \n",
      "all changes, events, and drifts to the centralized workspace. Let's check out the storage \n",
      "options for deploying multiple applications. \n",
      "Azure Storage accounts\n",
      "Azure Storage is a service provided by Azure to store files as blobs. All scripts and code \n",
      "for automating the provisioning, deployment, and configuration of the infrastructure \n",
      "and sample application are stored in the Azure DevOps Git repository and are packaged\n",
      "---------------\n",
      "and deployed in an Azure Storage account. Azure provides PowerShell script-extension \n",
      "resources that can automatically download DSC and PowerShell scripts and execute \n",
      "them on virtual machines during the execution of ARM templates. This storage acts \n",
      "as common storage across all deployments for multiple applications. Storing scripts \n",
      "and templates in a Storage account ensures that they can be used across projects \n",
      "irrespective of projects in Azure DevOps. Let's move on to exploring the importance of \n",
      "images in the next section.\n",
      "Docker and OS images\n",
      "Both virtual machine and container images should be built as part of the common \n",
      "services build-and-release pipeline. Tools such as Packer and Docker Build can be used \n",
      "to generate these images.\n",
      "Management tools\n",
      "All management tools, such as Kubernetes, DC/OS, Docker Swarm, and ITIL tools, \n",
      "should be provisioned before building and deploying the solution.\n",
      "We'll conclude this section on DevOps preparation with management tools. There are\n",
      "---------------\n",
      "multiple choices for each activity within a DevOps ecosystem and we should enable \n",
      "them as part of the DevOps journey—it should not be an afterthought, but rather part of \n",
      "DevOps planning.\n",
      "---------------\n",
      "444 | Integrating Azure DevOps\n",
      "DevOps for PaaS solutions\n",
      "The typical architecture for Azure PaaS app services is based on Figure 13.8:\n",
      "Figure 13.8: A typical Azure PaaS app service architecture\n",
      "The architecture shows some of the important components—such as Azure SQL, \n",
      "Storage accounts, and the version control system—that participate in the Azure App \n",
      "Service-based cloud solution architecture. These artifacts should be created using \n",
      "ARM templates. These ARM templates should be part of the overall configuration \n",
      "management strategy. It can have its own build and release management pipelines, \n",
      "similar to the one shown in Figure 13.9:\n",
      "Source Control\n",
      "Resource\n",
      "Group\n",
      "Azure Active\n",
      "Directory\n",
      "deploy\n",
      "Validate\n",
      "deployment\n",
      "IP address\n",
      "authenticate\n",
      "access token\n",
      "App Service app\n",
      "App Service Plan\n",
      "Instances\n",
      "Last-known good\n",
      "Production\n",
      "Staging\n",
      "Deployment Slots\n",
      "Storage Account\n",
      "Blob Container\n",
      "App Logs Web Server\n",
      "Logs\n",
      "Azure SQL Database\n",
      "Logical Server\n",
      "Database Database\n",
      "---------------\n",
      "DevOps for PaaS solutions | 445\n",
      "Figure 13.9: Choosing deployment options for the app service\n",
      "Now that we have explored the various deployment source options, let's go ahead and \n",
      "dive into understanding how to deploy cloud solutions on Azure.\n",
      "Azure App Service\n",
      "Azure App Service provides managed hosting services for cloud solutions. It's a \n",
      "fully-managed platform that provisions and deploys cloud solutions. Azure App \n",
      "Service takes away the burden of creating and managing infrastructure and provides \n",
      "minimum service-level agreements (SLAs) for hosting your cloud solutions.\n",
      "Deployment slots\n",
      "Azure App Service provides deployment slots that make deployment to them seamless \n",
      "and easy. There are multiple slots, and swapping between slots is done at a DNS level. \n",
      "It means anything in the production slot can be swapped with a staging slot by just \n",
      "swapping the DNS entries. This helps in deploying the custom cloud solution to staging\n",
      "---------------\n",
      "and, after all checks and tests, they can be swapped to production if found satisfactory. \n",
      "However, in the event of any issue in production after swapping, the previous good \n",
      "values from the production environment can be reinstated by swapping again. Let's \n",
      "move on to understanding Azure's database offering and some of its key features.\n",
      "---------------\n",
      "446 | Integrating Azure DevOps\n",
      "Azure SQL\n",
      "Azure SQL is a SQL PaaS service provided by Azure to host databases. Azure provides a \n",
      "secure platform to host databases and takes complete ownership to manage the \n",
      "availability, reliability, and scalability of the service. With Azure SQL, there's no need to \n",
      "provision custom virtual machines, deploy a SQL server, and configure it. Instead, the \n",
      "Azure team does this behind the scenes and manages it on your behalf. It also provides \n",
      "a firewall service that enables security; only an IP address allowed by the firewall can \n",
      "connect the server and access the database. The virtual machines provisioned to host \n",
      "web applications have distinct public IP addresses assigned to them and they're added \n",
      "to Azure SQL firewall rules dynamically. Azure SQL Server and its database is created \n",
      "upon executing the ARM template. Next, we will cover build and release pipelines.\n",
      "The build and release pipelines\n",
      "---------------\n",
      "In this section, a new build pipeline is created that compiles and validates an ASP.\n",
      "NET MVC application, and then generates packages for deployment. After package \n",
      "generation, a release definition ensures that deployment to the first environment \n",
      "happens in an App Service and Azure SQL as part of continuous deployment.\n",
      "There are two ways to author build and release pipelines:\n",
      "1. Using the classic editor\n",
      "2. Using YAML files\n",
      "YAML files provide more flexibility for authoring build and release pipelines.\n",
      "The project structure of the sample application is shown in Figure 13.10:\n",
      "---------------\n",
      "DevOps for PaaS solutions | 447\n",
      "Figure 13.10: Project structure of a sample application\n",
      "---------------\n",
      "448 | Integrating Azure DevOps\n",
      "In this project, there's an ASP.NET MVC application—the main application—and it \n",
      "consists of application pages. Web Deploy packages will be generated out of this project \n",
      "from build pipelines and they will eventually be on Web Apps. There are other projects \n",
      "that are also part of the solution, as mentioned next:\n",
      "• Unit test project: Code for unit-testing the ASP.NET MVC application. Assemblies \n",
      "from this project will be generated and executed in the build execution.\n",
      "• SQL Database project: Code related to the SQL database schema, structure, \n",
      "and master data. DacPac files will be generated out of this project using the build \n",
      "definition.\n",
      "• Azure resource group project: ARM templates and parameter code to provision \n",
      "the entire Azure environment on which the ASP.NET MVC application and the SQL \n",
      "tables are created.\n",
      "The build pipeline is shown in Figure 13.11:\n",
      "Figure 13.11: Build pipeline of the ASP.NET MVC application\n",
      "---------------\n",
      "DevOps for PaaS solutions | 449\n",
      "The configuration of each task is shown in Table 13.2:\n",
      "Task name\n",
      "Use NuGet 4.4.1\n",
      "NuGet restore\n",
      "Build solution\n",
      "---------------\n",
      "450 | Integrating Azure DevOps\n",
      "Task name\n",
      "Test \n",
      "Assemblies\n",
      "Publish \n",
      "symbols path\n",
      "Publish \n",
      "Artifact - MVC \n",
      "application\n",
      "---------------\n",
      "DevOps for PaaS solutions | 451\n",
      "Task name\n",
      "Publish \n",
      "Artifact - \n",
      "IaaS code (ARM \n",
      "templates)\n",
      "Build solution \n",
      "Database1/\n",
      "Database1.\n",
      "sqlproj\n",
      "---------------\n",
      "452 | Integrating Azure DevOps\n",
      "Table 13.2: Configuration of the build pipeline tasks\n",
      "The build pipeline is configured to execute automatically as part of continuous \n",
      "integration, as shown in Figure 13.12:\n",
      "Figure 13.12: Enabling continuous integration in the build pipeline\n",
      "The release definition consists of multiple environments, such as development, testing, \n",
      "System Integration Testing (SIT), User Acceptance Testing (UAT), preproduction, \n",
      "and production. The tasks are pretty similar in each environment, with the addition of \n",
      "tasks specific to that environment. For example, a test environment has additional tasks \n",
      "related to the UI, and functional and integration testing, compared to a development \n",
      "environment.\n",
      "Task name\n",
      "Copy Files to: \n",
      "$(build. \n",
      "artifactst \n",
      "aging \n",
      "directory)\n",
      "---------------\n",
      "DevOps for PaaS solutions | 453\n",
      "The release definition for such an application is shown in Figure 13.13:\n",
      "Figure 13.13: Release definition\n",
      "The release tasks for a single environment are shown in Figure 13.14:\n",
      "Figure 13.14: Release tasks for a single environment\n",
      "---------------\n",
      "454 | Integrating Azure DevOps\n",
      "The configuration for each of the tasks is listed here:\n",
      "Task name\n",
      "Replace \n",
      "tokens in \n",
      "*.SetParameters.\n",
      "xml\n",
      "(This is a task \n",
      "installed from \n",
      "Marketplace.)\n",
      "---------------\n",
      "DevOps for PaaS solutions | 455\n",
      "Task name\n",
      "Azure \n",
      "Deployment: \n",
      "Create Or Update \n",
      "Resource Group \n",
      "action on devRG\n",
      "---------------\n",
      "456 | Integrating Azure DevOps\n",
      "Task name\n",
      "Deploy Azure App \n",
      "Service\n",
      "---------------\n",
      "DevOps for PaaS solutions | 457\n",
      "Table 13.3: Configuration of the release pipeline tasks\n",
      "In this section, you saw ways to configure build and release pipelines in Azure DevOps. \n",
      "In the next section onward, the focus will be on different architectures, such as IaaS, \n",
      "containers, and different scenarios.\n",
      "Task name\n",
      "Azure SQL \n",
      "Publish\n",
      "---------------\n",
      "458 | Integrating Azure DevOps\n",
      "DevOps for IaaS\n",
      "IaaS involves the management and administration of base infrastructure and \n",
      "applications together and there are multiple resources and components that need to \n",
      "be provisioned, configured, and deployed on multiple environments. It is important to \n",
      "understand the architecture before going ahead.\n",
      "The typical architecture for an IaaS virtual machine-based solution is shown here:\n",
      "Figure 13.15: Architecture for an IaaS virtual machine-based solution\n",
      "Each of the components listed in the architecture is discussed from the next section \n",
      "onward.\n",
      "Azure virtual machines\n",
      "Azure virtual machines that host web applications, application servers, databases, \n",
      "and other services are provisioned using ARM templates. They're attached to a virtual \n",
      "network and have a private IP address from the same network. The public IP for virtual \n",
      "machines is optional since they're attached to a public load balancer. Operational\n",
      "---------------\n",
      "Insights agents are installed on virtual machines to monitor the virtual machines. \n",
      "PowerShell scripts are also executed on these virtual machines, downloaded from a \n",
      "Storage account available in another resource group to open relevant firewall ports, \n",
      "download appropriate packages, and install local certificates to secure access through \n",
      "PowerShell. The web application is configured to run on the provided port on these \n",
      "virtual machines. The port number for the web application and all its configuration is \n",
      "pulled from the DSC pull server and dynamically assigned.\n",
      "---------------\n",
      "DevOps for IaaS | 459\n",
      "Azure public load balancers\n",
      "A public load balancer is attached to some of the virtual machines for sending \n",
      "requests to them in a round-robin fashion. This is generally needed for front-end web \n",
      "applications and APIs. A public IP address and DNS name can be assigned to a load \n",
      "balancer such that it can serve internet requests. It accepts HTTP web requests on \n",
      "a different port and routes them to the virtual machines. It also probes certain \n",
      "ports on HTTP protocols with some provided application paths. Network Address \n",
      "Translation (NAT) rules can also be applied such that they can be used to log into the \n",
      "virtual machines using remote desktops.\n",
      "An alternative resource to the Azure public Load Balancer is the Azure Application \n",
      "Gateway. Application gateways are layer-7 load balancers and provide features such as \n",
      "SSL termination, session affinity, and URL-based routing. Let's discuss the build pipeline \n",
      "in the next section.\n",
      "The build pipeline\n",
      "---------------\n",
      "A typical build pipeline for an IaaS virtual machine-based solution is shown next. A \n",
      "release pipeline starts when a developer pushes their code to the repository. The build \n",
      "pipeline starts automatically as part of continuous integration. It compiles and builds \n",
      "the code, executes unit tests on it, checks code quality, and generates documentation \n",
      "from code comments. It deploys the new binaries into the development environment \n",
      "(note that the development environment is not newly created), changes configuration, \n",
      "executes integration tests, and generates build labels for easy identification. It then \n",
      "drops the generated artifacts into a location accessible by the release pipeline. If there \n",
      "are issues during the execution of any step in this pipeline, this is communicated to the \n",
      "developer as part of the build pipeline feedback so that they can rework and resubmit \n",
      "their changes. The build pipeline should fail or pass based on the severity of issues\n",
      "---------------\n",
      "found, and that varies from organization to organization. A typical build pipeline is \n",
      "shown in Figure 13.16:\n",
      "---------------\n",
      "460 | Integrating Azure DevOps\n",
      "Figure 13.16: A typical IaaS build pipeline\n",
      "Similar to the build pipeline, let's learn about the implementation of a release pipeline.\n",
      "The release pipeline\n",
      "A typical release pipeline for an IaaS virtual machine-based deployment is shown next. \n",
      "A release pipeline starts after the completion of the build pipeline. The first step in \n",
      "the release pipeline is to gather the artifacts generated by the build pipeline. They are \n",
      "generally deployable assemblies, binaries, and configuration documents. The release \n",
      "pipeline executes and creates or updates the first environment, which generally is a \n",
      "test environment. It uses ARM templates to provision all IaaS and PaaS services and \n",
      "resources on Azure and configures them as well. They also help in executing scripts \n",
      "and DSC configuration after virtual machines are created as post-creation steps. This \n",
      "helps to configure the environment within the virtual machine and the operating\n",
      "---------------\n",
      "system. At this stage, application binaries from the build pipeline are deployed and \n",
      "configured. Different automated tests are performed to check the solution and, \n",
      "if found satisfactory, the pipeline moves the deployment to the next environment \n",
      "after obtaining the necessary approvals. The same steps are executed in the next \n",
      "environment, including the production environment. Finally, the operational validation \n",
      "tests are executed in production to ensure that the application is working as expected \n",
      "and there are no deviations.\n",
      "Check in \n",
      "code in local \n",
      "branch\n",
      "Update Dev \n",
      "environment \n",
      "with new \n",
      "binaries\n",
      "Generate build \n",
      "artifacts and \n",
      "drop them\n",
      "Apply \n",
      "VSTS \n",
      "provides \n",
      "feedback to \n",
      "developer\n",
      "If build fails, \n",
      "developer \n",
      "reworks on \n",
      "code\n",
      "Execute \n",
      "interation \n",
      "tests\n",
      "Check in \n",
      "code in local \n",
      "branch\n",
      "Generate \n",
      "documentation\n",
      "Generate \n",
      "build label\n",
      "Execution of \n",
      "unit tests\n",
      "Execution of \n",
      "code quality \n",
      "checks\n",
      "Push \n",
      "changes to \n",
      "shared repo\n",
      "VSTS CI \n",
      "kicks in and \n",
      "starts build \n",
      "pipeline\n",
      "Build\n",
      "---------------\n",
      "pipeline \n",
      "compiles the \n",
      "code\n",
      "---------------\n",
      "DevOps for IaaS | 461\n",
      "At this stage, if there are any issues or bugs, they should be rectified and the entire \n",
      "cycle should be repeated; however, if this doesn't happen within a stipulated time \n",
      "frame, the last-known snapshot should be restored in the production environment to \n",
      "minimize downtime. A typical release pipeline is shown in Figure 13.17:\n",
      "Figure 13.17: A typical IaaS release pipeline\n",
      "This section concludes the DevOps process for IaaS solutions and the next chapter will \n",
      "focus on containers on virtual machines. Please note that containers can also run on \n",
      "PaaS like App Service and Azure Functions. \n",
      "Build \n",
      "pipeline ran \n",
      "successfully\n",
      "Deploy and \n",
      "application\n",
      "environment \n",
      "using DSC \n",
      "Perform \n",
      "test activities\n",
      "Deploy and \n",
      "application \n",
      "(DSC)\n",
      "Perform \n",
      "tests and get \n",
      "approval\n",
      "Get approval \n",
      "to deploy to \n",
      "next stage\n",
      "Deploy and \n",
      "production\n",
      "environment \n",
      "using DSC \n",
      "Execute \n",
      "operational \n",
      "validation \n",
      "test\n",
      "Create/Update next \n",
      "stage environment\n",
      "- ARM templates\n",
      "- Virtual machines\n",
      "---------------\n",
      "- Other laas resources\n",
      "Start \n",
      "deploying \n",
      "environment\n",
      "Create test \n",
      "environment\n",
      "- ARM templates\n",
      "- Virtual machines\n",
      "- Other laas resources\n",
      "Build \n",
      "pipeline \n",
      "generated \n",
      "artifacts\n",
      "Execute \n",
      "release \n",
      "pipeline\n",
      "Prepare for \n",
      "deployment\n",
      "---------------\n",
      "462 | Integrating Azure DevOps\n",
      "DevOps with containers\n",
      "In a typical architecture, container runtimes are deployed on virtual machines and \n",
      "containers are run within them. The typical architecture for IaaS container-based \n",
      "solutions is shown here:\n",
      "Figure 13.18: Architecture for IaaS container-based solutions\n",
      "These containers are managed by container orchestrators such as Kubernetes. \n",
      "Monitoring services are provided by Log Analytics and all secrets and keys are stored \n",
      "in Azure Key Vault. There is also a pull server, which could be on a virtual machine or \n",
      "Azure Automation, providing configuration information to the virtual machines.\n",
      "Containers\n",
      "Containers are a virtualization technology; however, they don't virtualize physical \n",
      "servers. Instead, containers are an operating system-level virtualization. This means \n",
      "that containers share the operating system kernel provided by their host among \n",
      "themselves and with the host. Running multiple containers on a host (physical or\n",
      "---------------\n",
      "virtual) shares the host operating system kernel. There's a single operating system \n",
      "kernel provided by the host and used by all containers running on top of it.\n",
      "Containers are also completely isolated from their host and other containers, much \n",
      "like a virtual machine. Containers use operating system namespaces, control groups \n",
      "on Linux, to provide the perception of a new operating system environment, and use \n",
      "specific operating system virtualization techniques on Windows. Each container gets its \n",
      "own copy of the operating system resources.\n",
      "---------------\n",
      "DevOps with containers | 463\n",
      "Docker\n",
      "Docker provides management features to containers. It comprises two executables:\n",
      "• The Docker daemon\n",
      "• The Docker client\n",
      "The Docker daemon is the workhorse for managing containers. It's a management \n",
      "service that's responsible for managing all activities on the host related to containers. \n",
      "The Docker client interacts with the Docker daemon and is responsible for capturing \n",
      "inputs and sending them to the Docker daemon. The Docker daemon provides \n",
      "the runtime; libraries; graph drivers; the engines to create, manage, and monitor \n",
      "containers; and images on the host server. It can also create custom images that are \n",
      "used for building and shipping applications to multiple environments.\n",
      "The Dockerfile\n",
      "The Dockerfile is the primary building block for creating container images. It's a \n",
      "simple text-based human-readable file without an extension and is even named \n",
      "Dockerfile. Although there's a mechanism to name it differently, generally it is named\n",
      "---------------\n",
      "Dockerfile. The Dockerfile contains instructions to create a custom image using a \n",
      "base image. These instructions are executed sequentially from top to bottom by the \n",
      "Docker daemon. The instructions refer to the command and its parameters, such \n",
      "as COPY, ADD, RUN, and ENTRYPOINT. The Dockerfile enables IaC practices by converting \n",
      "the application deployment and configuration into instructions that can be versioned \n",
      "and stored in a source code repository. Let's check out the build steps in the following \n",
      "section.\n",
      "The build pipeline\n",
      "There's no difference, from the build perspective, between the container and a virtual-\n",
      "machine-based solution. The build step remains the same. A typical release pipeline for \n",
      "an IaaS container-based deployment is shown next. \n",
      "The release pipeline\n",
      "The only difference between a typical release pipeline for an IaaS container-based \n",
      "deployment and the release pipeline is the container-image management and the\n",
      "---------------\n",
      "creation of containers using Dockerfile and Docker Compose. Advanced container-\n",
      "management utilities, such as Docker Swarm, DC/OS, and Kubernetes, can also be \n",
      "deployed and configured as part of release management. However, note that these \n",
      "container management tools should be part of the shared services release pipeline, as \n",
      "discussed earlier. Figure 13.19 shows a typical release pipeline for a container-based \n",
      "solution:\n",
      "---------------\n",
      "464 | Integrating Azure DevOps\n",
      "Figure 13.19: Container-based release pipeline\n",
      "The focus of the next section is integration with other toolsets, such as Jenkins.\n",
      "Azure DevOps and Jenkins\n",
      "Azure DevOps is an open platform orchestrator that integrates with other orchestrator \n",
      "tools seamlessly. It provides all the necessary infrastructure and features that integrate \n",
      "well with Jenkins, as well. Organizations with well-established CI/CD pipelines built \n",
      "on Jenkins can reuse them with the advanced but simple features of Azure DevOps to \n",
      "orchestrate them.\n",
      "Jenkins can be used as a repository and can execute CI/CD pipelines in Azure DevOps, \n",
      "while it's also possible to have a repository in Azure DevOps and execute CI/CD \n",
      "pipelines in Jenkins.\n",
      "Build \n",
      "pipeline ran \n",
      "successfully\n",
      "Create and run \n",
      "containers\n",
      "- DSC \n",
      "Deploy and \n",
      "application\n",
      "Deploy and \n",
      "production\n",
      "Download \n",
      "container \n",
      "images from \n",
      "repository\n",
      "Perform \n",
      "test activities\n",
      "Execute \n",
      "operational \n",
      "validation \n",
      "test\n",
      "Perform \n",
      "test and get \n",
      "approval\n",
      "---------------\n",
      "Deploy and \n",
      "application \n",
      "(DSC)\n",
      "environment \n",
      "using DSC \n",
      "Create/Update next \n",
      "stage environment\n",
      "- ARM templates\n",
      "- Virtual machines\n",
      "- laas + containers\n",
      "Start \n",
      "deploying \n",
      "environment\n",
      "Create test \n",
      "environment\n",
      "- ARM templates\n",
      "- Virtual machines\n",
      "- Other laas resources\n",
      "Get approval \n",
      "to deploy to \n",
      "next stage\n",
      "Build \n",
      "pipeline \n",
      "generated \n",
      "artifacts\n",
      "Execute \n",
      "release \n",
      "pipeline\n",
      "Prepare for \n",
      "deployment\n",
      "Build/Update \n",
      "container \n",
      "image\n",
      "---------------\n",
      "Azure DevOps and Jenkins | 465\n",
      "The Jenkins configuration can be added in Azure DevOps as service hooks, and \n",
      "whenever any code change is committed to the Azure DevOps repository, it can trigger \n",
      "pipelines in Jenkins. Figure 13.20 shows the configuration of Jenkins from the Azure \n",
      "DevOps service hook configuration section:\n",
      "Figure 13.20: Configuration of Jenkins\n",
      "There are multiple triggers that execute the pipelines in Jenkins; one of them is Code \n",
      "pushed, as shown in Figure 13.21:\n",
      "Figure 13.21: Code pushed trigger executed\n",
      "---------------\n",
      "466 | Integrating Azure DevOps\n",
      "It's also possible to deploy to Azure VM and execute Azure DevOps release pipelines, as \n",
      "explained here: https:/ /docs.microsoft.com/azure/virtual-machines/linux/tutorial-\n",
      "build-deploy-jenkins.\n",
      "Jenkins should already be deployed before using it in any scenario. The deployment \n",
      "process on Linux can be found at https:/ /docs.microsoft.com/azure/virtual-machines/\n",
      "linux/tutorial-jenkins-github-docker-cicd.\n",
      "The next section will be more focused on tools and services related to configuration \n",
      "management. Azure automation provides DSC-related services such as the pull server.\n",
      "Azure Automation\n",
      "Azure Automation is Microsoft's platform for all automation implementation \n",
      "with regard to cloud, on-premises, and hybrid deployments. Azure Automation is \n",
      "a mature automation platform that provides rich capabilities in terms of the following:\n",
      "• Defining assets, such as variables, connections, credentials, certificates, and \n",
      "modules\n",
      "---------------\n",
      "• Implementing runbooks using Python, PowerShell scripts, and PowerShell \n",
      "workflows\n",
      "• Providing UIs to create runbooks\n",
      "• Managing the full runbook life cycle, including building, testing, and publishing\n",
      "• Scheduling runbooks\n",
      "• The ability to run runbooks anywhere—on cloud or on-premises\n",
      "• DSC as a configuration-management platform\n",
      "• Managing and configuring environments—Windows and Linux, applications, and \n",
      "deployment\n",
      "• The ability to extend Azure Automation by importing custom modules\n",
      "Azure Automation provides a DSC pull server that helps to create a centralized \n",
      "configuration management server that consists of configurations for nodes/virtual \n",
      "machines and their constituents.\n",
      "It implements the hub and spoke pattern wherein nodes can connect to the DSC pull \n",
      "server and download configurations assigned to them, and reconfigure themselves \n",
      "to reflect their desired state. Any changes or deviations within these nodes are\n",
      "---------------\n",
      "autocorrected by DSC agents the next time they run. This ensures that administrators \n",
      "don't need to actively monitor the environment to find any deviations.\n",
      "---------------\n",
      "Azure Automation | 467\n",
      "DSC provides a declarative language in which you define the intent and configuration, \n",
      "but not how to run and apply those configurations. These configurations are based on \n",
      "the PowerShell language and ease the process of configuration management.\n",
      "In this section, we'll look into a simple implementation of using Azure Automation DSC \n",
      "to configure a virtual machine to install and configure the web server (IIS) and create \n",
      "an index.htm file that informs users that the website is under maintenance.\n",
      "Next, you will learn how to provision an Azure Automation account.\n",
      "Provisioning an Azure Automation account\n",
      "Create a new Azure Automation account from the Azure portal or PowerShell within an \n",
      "existing or new resource group. You may notice in Figure 13.22 that Azure Automation \n",
      "provides menu items for DSC:\n",
      "Figure 13.22: DSC in an Azure Automation account\n",
      "It provides the following:\n",
      "• DSC nodes: These list all the virtual machines and containers that are enlisted\n",
      "---------------\n",
      "with the current Azure Automation DSC pull server. These virtual machines and \n",
      "containers are managed using configurations from the current DSC pull server.\n",
      "• DSC configurations: These list all the raw PowerShell configurations imported and \n",
      "uploaded to the DSC pull server. They are in human-readable format and aren't in \n",
      "a compiled state.\n",
      "• DSC node configurations: These list all compiles of DSC configurations available \n",
      "on the pull server to be assigned to nodes—virtual machines and containers. A DSC \n",
      "configuration produces MOF files after compilations and they're eventually used \n",
      "to configure nodes.\n",
      "---------------\n",
      "468 | Integrating Azure DevOps\n",
      "After provisioning an Azure Automation account, we can create a sample DSC \n",
      "configuration, as shown in the next section.\n",
      "Creating a DSC configuration\n",
      "The next step is to write a DSC configuration using any PowerShell editor to reflect the \n",
      "intent of the configuration. For this sample, a single configuration, ConfigureSiteOnIIS, \n",
      "is created. It imports the base DSC module, PSDesiredStateConfiguration, which \n",
      "consists of resources used within the configuration. It also declares a node web server. \n",
      "When this configuration is uploaded and compiled, it will generate a DSC configuration \n",
      "named ConfigureSiteOnIISwebserver. This configuration can then be applied to nodes.\n",
      "The configuration consists of a few resources. These resources configure the target \n",
      "node. The resources install a web server, ASP.NET, and framework, and create \n",
      "an index.htm file within the inetpub\\wwwroot directory with content to show that the\n",
      "---------------\n",
      "site is under maintenance. For more information about writing DSC configuration, \n",
      "refer to https:/ /docs.microsoft.com/powershell/scripting/dsc/getting-started/\n",
      "wingettingstarted?view=powershell-7.\n",
      "The next code listing shows the entire configuration described in the previous \n",
      "paragraph. This configuration will be uploaded to the Azure Automation account:\n",
      "Configuration ConfigureSiteOnIIS {   \n",
      "    Import-DscResource -ModuleName 'PSDesiredStateConfiguration'   \n",
      "    Node WebServer {   \n",
      "      WindowsFeature IIS  \n",
      "        {  \n",
      "            Name = \"Web-Server\"  \n",
      "            Ensure = \"Present\"  \n",
      "        }         \n",
      "        WindowsFeature AspDotNet  \n",
      "        {  \n",
      "            Name = \"net-framework-45-Core\"  \n",
      "            Ensure = \"Present\"  \n",
      "            DependsOn = \"[WindowsFeature]IIS\"  \n",
      "        }\n",
      "---------------\n",
      "Azure Automation | 469\n",
      "        WindowsFeature AspNet45  \n",
      "        {  \n",
      "            Ensure          = \"Present\"  \n",
      "            Name            = \"Web-Asp-Net45\"  \n",
      "            DependsOn = \"[WindowsFeature]AspDotNet\"  \n",
      "        }   \n",
      "        File IndexFile  \n",
      "        {  \n",
      "            DestinationPath = \"C:\\inetpub\\wwwroot\\index.htm\"  \n",
      "            Ensure = \"Present\"  \n",
      "            Type = \"File\"  \n",
      "            Force = $true  \n",
      "            Contents = \"<HTML><HEAD><Title> Website under construction.</\n",
      "Title></HEAD><BODY> '  \n",
      "             <h1>If you are seeing this page, it means the website is under \n",
      "maintenance and DSC Rocks !!!!!</h1></BODY></HTML>\"  \n",
      "        }  \n",
      "   }  \n",
      "}   \n",
      "After creating a sample DSC configuration, it should be imported within Azure \n",
      "Automation as shown in the next section.\n",
      "Importing the DSC configuration\n",
      "The DSC configuration still isn't known to Azure Automation. It's available on some \n",
      "local machines. It should be uploaded to Azure Automation DSC configurations.\n",
      "---------------\n",
      "Azure Automation provides the Import-AzureRMAutomationDscConfiguration cmdlet to \n",
      "import the configuration to Azure Automation:\n",
      "Import-AzureRmAutomationDscConfiguration -SourcePath \"C:\\DSC\\AA\\DSCfiles\\\n",
      "ConfigureSiteOnIIS.ps1\" -ResourceGroupName \"omsauto\" -AutomationAccountName \n",
      "\"datacenterautomation\" -Published -Verbose     \n",
      "The commands will import the configuration within Azure Automation. After importing, \n",
      "the DSC configuration should be compiled so that it can be assigned to servers for \n",
      "compliance checks and autoremediation.\n",
      "---------------\n",
      "470 | Integrating Azure DevOps\n",
      "Compiling the DSC configuration\n",
      "After the DSC configuration is available in Azure Automation, it can be asked \n",
      "to compile. Azure Automation provides another cmdlet for this. Use the Start-\n",
      "AzureRmAutomationDscCompilationJob cmdlet to compile the imported configuration. The \n",
      "configuration name should match the name of the uploaded configuration. Compilation \n",
      "creates an MOF file named after the configuration and node name together, which in \n",
      "this case is the ConfigureSiteOnIIS web server. The execution of the command is shown \n",
      "here:\n",
      "Start-AzureRmAutomationDscCompilationJob -ConfigurationName ConfigureSiteOnIIS \n",
      "-ResourceGroupName \"omsauto\" -AutomationAccountName \"datacenterautomation\" \n",
      "-Verbose      \n",
      "Now you have accomplished DSC node configuration. In the next section, you will learn \n",
      "to assign configurations to nodes. \n",
      "Assigning configurations to nodes\n",
      "The compiled DSC configurations can be applied to nodes. Use Register-\n",
      "---------------\n",
      "AzureRmAutomationDscNode to assign the configuration to a node. \n",
      "The NodeConfigurationName parameter identifies the configuration name that should be \n",
      "applied to the node. This is a powerful cmdlet that can also configure the DSC agent, \n",
      "which is localconfigurationmanager, on nodes before they can download configurations \n",
      "and apply them. There are multiple localconfigurationmanager parameters that can \n",
      "be configured—details are available at https:/ /devblogs.microsoft.com/powershell/\n",
      "understanding-meta-configuration-in-windows-powershell-desired-state-\n",
      "configuration.\n",
      "Let's heck out the configuration below:\n",
      "Register-AzureRmAutomationDscNode -ResourceGroupName \"omsauto\" \n",
      "-AutomationAccountName \"datacenterautomation\" -AzureVMName testtwo \n",
      "-ConfigurationMode ApplyAndAutocorrect -ActionAfterReboot ContinueConfiguration \n",
      "-AllowModuleOverwrite $true -AzureVMResourceGroup testone -AzureVMLocation \n",
      "\"West Central US\" -NodeConfigurationName \"ConfigureSiteOnIIS.WebServer\" \n",
      "-Verbose\n",
      "---------------\n",
      "Now, we can test whether the configuration has been applied to the servers by \n",
      "browsing the newly deployed website using a browser. After the testing has completed \n",
      "successfully, let's move on to validating the connections.\n",
      "---------------\n",
      "Tools for DevOps | 471\n",
      "Validation\n",
      "If appropriate, network security groups and firewalls are opened and enabled for \n",
      "port 80, and a public IP is assigned to the virtual machine. The default website can be \n",
      "browsed using the IP address. Otherwise, log into the virtual machine that's used to \n",
      "apply the DSC configuration and navigate to http://localhost.\n",
      "It should show the following page:\n",
      "Figure 13.23: Localhost\n",
      "This is the power of configuration management: without writing any significant code, \n",
      "authoring a configuration once can be applied multiple times to the same and multiple \n",
      "servers, and you can be assured that they will run in the desired state without any \n",
      "manual intervention. In the next section, we will check out the various tools available \n",
      "for Azure DevOps. \n",
      "Tools for DevOps\n",
      "As mentioned before, Azure is a rich and mature platform that supports the following:\n",
      "• Multiple choices of languages\n",
      "• Multiple choices of operating systems\n",
      "• Multiple choices of tools and utilities\n",
      "---------------\n",
      "• Multiple patterns for deploying solutions (such as virtual machines, app services, \n",
      "containers, and microservices)\n",
      "With so many options and choices, Azure offers the following:\n",
      "• Open cloud: It is open to open source, Microsoft, and non-Microsoft products, \n",
      "tools, and services.\n",
      "• Flexible cloud: It is easy enough for both end users and developers to use it with \n",
      "their existing skills and knowledge.\n",
      "• Unified management: It provides seamless monitoring and management features.\n",
      "---------------\n",
      "472 | Integrating Azure DevOps\n",
      "All the services and capabilities mentioned here are important for the successful \n",
      "implementation of DevOps. Figure 13.24 shows the open source tools and utilities that \n",
      "can be used for different phases of managing the application life cycle and DevOps in \n",
      "general:\n",
      "Figure 13.24: Open source tools and utilities\n",
      "Figure 13.24 shows the Microsoft tools and utilities that can be used for different \n",
      "phases of managing the application life cycle and DevOps in general. Again, this is just a \n",
      "small representation of all the tools and utilities—there are many more options available, \n",
      "such as the following:\n",
      "• Azure DevOps build orchestration for constructing a build pipeline\n",
      "• Microsoft Test Manager and Pester for testing\n",
      "• DSC, PowerShell, and ARM templates for deployment or configuration \n",
      "management\n",
      "• Log Analytics, Application Insights, and System Center Operations \n",
      "Manager (SCOM) for alerting and monitoring\n",
      "---------------\n",
      "• Azure DevOps and System Center Service Manager for managing processes:\n",
      "---------------\n",
      "Summary | 473\n",
      "Figure 13.25: Microsoft tools and utilities\n",
      "There are many tools available for each of the DevOps practices and in this section, you \n",
      "saw some of the tools and the way to configure them.\n",
      "Summary\n",
      "DevOps is gaining a lot of traction and momentum in the industry. Most organizations \n",
      "have realized its benefits and are looking to implement DevOps. This is happening while \n",
      "most of them are moving to the cloud. Azure, as a cloud platform, provides rich and \n",
      "mature DevOps services, making it easy for organizations to implement DevOps. \n",
      "In this chapter, we discussed DevOps along with its core practices, such as \n",
      "configuration management, continuous integration, continuous delivery, and \n",
      "deployment. We also discussed different cloud solutions based on PaaS, a virtual \n",
      "machine IaaS, and a container IaaS, along with their respective Azure resources, the \n",
      "build and release pipelines. \n",
      "Configuration management was also explained in the chapter, along with DSC\n",
      "---------------\n",
      "services from Azure Automation and using pull servers to configure virtual machines \n",
      "automatically. Finally, we covered Azure's openness and flexibility regarding the choice \n",
      "of languages, tools, and operating systems. \n",
      "In the next chapter, we will go through the details of Kubernetes and its components \n",
      "and interactions, in addition to application design and deployment considerations on \n",
      "Kubernetes.\n",
      "---------------\n",
      "Containers are one of the most talked-about infrastructure components of the \n",
      "last decade. Containers are not a new technology; they have been around for quite \n",
      "some time. They have been prevalent in the Linux world for more than two decades. \n",
      "Containers were not well known in the developer community due to their complexity \n",
      "and the fact that there was not much documentation regarding them. However, around \n",
      "the beginning of this decade, in 2013, a company was launched known as Docker that \n",
      "changed the perception and adoption of containers within the developer world.\n",
      "Docker wrote a robust API wrapper on top of existing Linux LXC containers and made \n",
      "it easy for developers to create, manage, and destroy containers from the command-\n",
      "line interface. When containerizing applications, the number of containers we have \n",
      "can increase drastically over time, and we can reach a point where we need to manage \n",
      "hundreds or even thousands of containers. This is where container orchestrators\n",
      "---------------\n",
      "play a role, and Kubernetes is one of them. Using Kubernetes, we can automate the \n",
      "deployment, scaling, networking, and management of containers.\n",
      "Architecting Azure \n",
      "Kubernetes solutions\n",
      "14\n",
      "---------------\n",
      "476 | Architecting Azure Kubernetes solutions\n",
      "In this chapter, we will look at:\n",
      "• The introductory concepts of containers\n",
      "• The concepts of Kubernetes\n",
      "• The important elements that make Kubernetes work\n",
      "• Architecting solutions using Azure Kubernetes Service\n",
      "Now that you know what Kubernetes is used for, let's start from scratch and discuss \n",
      "what containers are, how they are orchestrated using Kubernetes, and more.\n",
      "Introduction to containers\n",
      "Containers are referred to as operating system–level virtualization systems. They are \n",
      "hosted on an operating system running either on a physical server or a virtual server. \n",
      "The nature of the implementation depends on the host operating system. For example, \n",
      "Linux containers are inspired by cgroups; on the other hand, Windows containers are \n",
      "almost lightweight virtual machines with a small footprint.\n",
      "Containers are truly cross-platform. Containerized applications can run on any\n",
      "---------------\n",
      "platform, such as Linux, Windows, or Mac, uniformly without any changes being \n",
      "needed, which makes them highly portable. This makes them a perfect technology for \n",
      "organizations to adopt as they are platform-agnostic.\n",
      "In addition, containers can run in any cloud environment or on-premises environment \n",
      "without changes being needed. This means that organizations are also not tied to a \n",
      "single cloud provider if they implement containers as their hosting platform on the \n",
      "cloud. They can move their environment from on-premises and lift and shift to the \n",
      "cloud.\n",
      "Containers provide all the benefits that are typically available with virtual machines. \n",
      "They have their own IP addresses, DNS names, identities, networking stacks, \n",
      "filesystems, and other components that give users the impression of using a pristine \n",
      "new operating system environment. Under the hood, the Docker runtime virtualizes \n",
      "multiple operating system kernel–level components to provide that impression.\n",
      "---------------\n",
      "Kubernetes fundamentals | 477\n",
      "All these benefits provide immense benefits for organizations adopting container \n",
      "technology, and Docker is one of the forerunners in this regard. There are other \n",
      "container runtime options available, such as CoreOS Rkt (pronounced as Rocket, out \n",
      "of production), Mesos Containerizer, and LXC containers. Organizations can adopt the \n",
      "technology that they feel comfortable with.\n",
      "Containers were previously not available in the Windows world, only becoming available \n",
      "for Windows 10 and Windows Server 2016. However, containers are now first-class \n",
      "citizens in the Windows world.\n",
      "As mentioned in the introduction, containers should be monitored, governed, and \n",
      "managed well, just like any other infrastructural component within an ecosystem. \n",
      "It's necessary to deploy an orchestrator, such as Kubernetes, that can help you to do \n",
      "so easily. In the next section, you will learn about the fundamentals of Kubernetes, \n",
      "including what its advantages are.\n",
      "---------------\n",
      "Kubernetes fundamentals\n",
      "Many organizations still ask, \"Do we need Kubernetes, or indeed any container \n",
      "orchestrator?\" When we think about container management on a large scale, we need \n",
      "to think about several points, such as scaling, load balancing, life cycle management, \n",
      "continuous delivery, logging and monitoring, and more.\n",
      "You might ask, \"Aren't containers supposed to do all that?\" The answer is that containers \n",
      "are only a low-level piece of the puzzle. The real benefits are gained through the tools \n",
      "that sit on top of the containers. At the end of the day, we need something to help us \n",
      "with orchestration.\n",
      "Kubernetes is a Greek word, κυβερνήτης, which means \"helmsman\" or \"captain of the \n",
      "ship.\" Keeping the maritime theme of Docker containers, Kubernetes is the captain \n",
      "of the ship. Kubernetes is often denoted as K8s, where 8 represents the eight letters \n",
      "between \"K\" and \"s\" in the word \"Kubernetes.\"\n",
      "As mentioned before, containers are more agile than virtual machines. They can be\n",
      "---------------\n",
      "created within seconds and destroyed equally quickly. They have a similar life cycle to \n",
      "virtual machines; however, they need to be monitored, governed, and managed actively \n",
      "within an environment.\n",
      "---------------\n",
      "478 | Architecting Azure Kubernetes solutions\n",
      "It is possible to manage them using your existing toolset; even so, specialized tools, \n",
      "such as Kubernetes, can provide valuable benefits:\n",
      "• Kubernetes is self-healing in nature. When a Pod (read as \"container\" for now) \n",
      "goes down within a Kubernetes environment, Kubernetes will ensure that a new \n",
      "Pod is created elsewhere either on the same node or on another node, to respond \n",
      "to requests on behalf of the application.\n",
      "• Kubernetes also eases the process of upgrading an application. It provides out-of-\n",
      "the-box features to help you perform multiple types of upgrades with the original \n",
      "configuration.\n",
      "• It helps to enable blue-green deployments. In this type of deployment, Kubernetes \n",
      "will deploy the new version of the application alongside the old one, and once it is \n",
      "confirmed that the new application works as expected, a DNS switch will be made \n",
      "to switch to the new version of the application. The old application deployment\n",
      "---------------\n",
      "can continue to exist for rollback purposes.\n",
      "• Kubernetes also helps to implement a rolling-upgrade deployment strategy. Here, \n",
      "Kubernetes will deploy the new version of the application one server at a time, and \n",
      "tear down the old deployment one server at a time. It will carry on this activity \n",
      "until there are no more servers left from the old deployment.\n",
      "• Kubernetes can be deployed on an on-premises data center or on the cloud using \n",
      "the infrastructure as a service (IaaS) paradigm. This means that developers first \n",
      "create a group of virtual machines and deploy Kubernetes on top of it. There is \n",
      "also the alternative approach of using Kubernetes as a platform as a service (PaaS) \n",
      "offering. Azure provides a PaaS service known as Azure Kubernetes Service (AKS), \n",
      "which provides an out-of-the-box Kubernetes environment to developers.\n",
      "When it comes to Deployment, Kubernetes can be deployed in two ways:\n",
      "• Unmanaged clusters: Unmanaged clusters can be created by installing Kubernetes\n",
      "---------------\n",
      "and any other relevant packages on a bare–metal machine or a virtual machine. \n",
      "In an unmanaged cluster, there will be master and worker nodes, formerly known \n",
      "as minions. The master and worker nodes work hand–in–hand to orchestrate the \n",
      "containers. If you are wondering how this is achieved, later in this chapter, we will \n",
      "be exploring the complete architecture of Kubernetes. Right now, just know that \n",
      "there are master and worker nodes.\n",
      "---------------\n",
      "Kubernetes architecture | 479\n",
      "• Managed clusters: Managed clusters are normally provided by the cloud provider; \n",
      "the cloud provider manages the infrastructure for you. In Azure, this service is \n",
      "called AKS. Azure will provide active support regarding patching and managing \n",
      "the infrastructure. With IaaS, organizations have to ensure the availability and \n",
      "scalability of the nodes and the infrastructure on their own. In the case of AKS, \n",
      "the master component will not be visible as it is managed by Azure. However, the \n",
      "worker nodes (minions) will be visible and will be deployed to a separate resource \n",
      "group, so you can access the nodes if needed.\n",
      "Some of the key benefits of using AKS over unmanaged clusters are:\n",
      "• If you are using unmanaged clusters, you need to work to make the solution \n",
      "highly available and scalable. In addition to that, you need to have proper update \n",
      "management in place to install updates and patches. On the other hand, in AKS,\n",
      "---------------\n",
      "Azure manages this completely, enabling developers to save time and be more \n",
      "productive.\n",
      "• Native integration with other services, such as Azure Container Registry to store \n",
      "your container images securely, Azure DevOps to integrate CI/CD pipelines, Azure \n",
      "Monitor for logging, and Azure Active Directory for security.\n",
      "• Scalability and faster startup speed. \n",
      "• Support for virtual machine scale sets.\n",
      "While there is no difference in terms of the basic functionality of these two \n",
      "deployments, the IaaS form of deployment provides the flexibility to add new plugins \n",
      "and configuration immediately that might take some time for the Azure team to make \n",
      "available with AKS. Also, newer versions of Kubernetes are available within AKS quite \n",
      "quickly, without much delay. \n",
      "We have covered the basics of Kubernetes. At this point, you might be wondering how \n",
      "Kubernetes achieves all this. In the next section, we will be looking at the components \n",
      "of Kubernetes and how they work hand–in–hand.\n",
      "---------------\n",
      "Kubernetes architecture\n",
      "The first step in understanding Kubernetes is understanding its architecture. We will \n",
      "go into the details of each component in the next section, but getting a high-level \n",
      "overview of the architecture will help you to understand the interaction between the \n",
      "components.\n",
      "---------------\n",
      "480 | Architecting Azure Kubernetes solutions\n",
      "Kubernetes clusters\n",
      "Kubernetes needs physical or virtual nodes for installing two types of components:\n",
      "• Kubernetes control plane components, or master components\n",
      "• Kubernetes worker nodes (minions), or non-master components\n",
      "Figure 14.1 is a diagram that offers a high-level overview of Kubernetes' architecture. We \n",
      "will get into the components in more detail later on:\n",
      "Figure 14.1: Kubernetes cluster overview\n",
      "The control plane components are responsible for managing and governing the \n",
      "Kubernetes environment and Kubernetes minions.\n",
      "All nodes together—the master as well as the minions—form the cluster. A cluster, in \n",
      "other words, is a collection of nodes. They are virtual or physical, connected to each \n",
      "other, and reachable using the TCP networking stack. The outside world will have no \n",
      "clue about the size or capability of your cluster, or even the names of the worker nodes.\n",
      "---------------\n",
      "The only thing the nodes are aware of is the address of the API server through which \n",
      "they interact with the cluster. For them, the cluster is one large computer that runs \n",
      "their applications.\n",
      "It is Kubernetes that internally decides an appropriate strategy, using controllers, to \n",
      "choose a valid, healthy node that can run the application smoothly.\n",
      "The control plane components can be installed in a high-availability configuration. So \n",
      "far, we have discussed clusters and how they work. In the next section, we will be taking \n",
      "a look at the components of a cluster.\n",
      "---------------\n",
      "Kubernetes architecture | 481\n",
      "Kubernetes components\n",
      "Kubernetes components are divided into two categories: master components and \n",
      "node components. The master components are also known as the control plane of the \n",
      "cluster. The control plane is responsible for managing the worker nodes and the Pods in \n",
      "the cluster. The decision-making authority of a cluster is the control plane, and it also \n",
      "takes care of detection and responses related to cluster events. Figure 14.2 describes the \n",
      "complete architecture of a Kubernetes cluster:\n",
      "Figure 14.2: Kubernetes architecture\n",
      "You need to understand each of these components to administer a cluster correctly. \n",
      "Let's go ahead and discuss what the master components are:\n",
      "• API server: The API server is undoubtedly the brain of Kubernetes. It is the central \n",
      "component that enables all activities within Kubernetes. Every client request, \n",
      "with few exceptions, ends up with the API server, which decides the flow for the\n",
      "---------------\n",
      "request. It is solely responsible for interacting with the etcd server. \n",
      "• etcd: etcd is the data store for Kubernetes. Only the API server is allowed to \n",
      "communicate with etcd, and the API server can perform Create, Read, Update \n",
      "and Delete (CRUD) activities on etcd. When a request ends up with the API server, \n",
      "after validation, the API server can perform any CRUD operations, depending on \n",
      "the etcd request. etcd is a distributed, highly available data store. There can be \n",
      "multiple installations of etcd, each with a copy of the data, and any of them can \n",
      "serve the requests from the API server. In Figure 14.3, you can see that there are \n",
      "multiple instances running in the control plane to provide high availability:\n",
      "---------------\n",
      "482 | Architecting Azure Kubernetes solutions\n",
      "Figure 14.3: Making the control plane highly available\n",
      "• Controller manager: The controller manager is the workhorse of Kubernetes. \n",
      "While the API server receives the requests, the actual work in Kubernetes is done \n",
      "by the controller manager. The controller manager, as the name suggests, is the \n",
      "manager of the controllers. There are multiple controllers in a Kubernetes master \n",
      "node, and each is responsible for managing a single controller.\n",
      "The main responsibility of a controller is managing a single resource in a \n",
      "Kubernetes environment. For example, there is a replication controller manager \n",
      "for managing replication controller resources, and a ReplicaSet controller to \n",
      "manage ReplicaSets in a Kubernetes environment. The controller keeps a watch \n",
      "on the API server, and when it receives a request for a resource managed by it, the \n",
      "controller performs its job.\n",
      "---------------\n",
      "Kubernetes architecture | 483\n",
      "One of the main responsibilities of controllers is to keep running in a loop and \n",
      "ensure that Kubernetes is in the desired state. If there is any deviation from \n",
      "the desired state, the controllers should bring it back to the desired state. A \n",
      "deployment controller watches for any new deployment resources created by \n",
      "the API server. If a new deployment resource is found, the deployment controller \n",
      "creates a new ReplicaSet resource and ensures that the ReplicaSet is always in the \n",
      "desired state. A replication controller keeps running in a loop and checks whether \n",
      "the actual number of Pods in the environment matches the desired number of \n",
      "Pods. If a Pod dies for any reason, the replication controller will find that the \n",
      "actual count has gone down by one and it will schedule a new Pod in the same or \n",
      "another node.\n",
      "• Scheduler: The job of a scheduler is to schedule the Pods on Kubernetes minion\n",
      "---------------\n",
      "nodes. It is not responsible for creating Pods. It is purely responsible for assigning \n",
      "Pods to Kubernetes minion nodes. It does so by taking into account the current \n",
      "state of nodes, how busy they are, their available resources, and also the definition \n",
      "of the Pod. A Pod might have a preference regarding a specific node, and the \n",
      "scheduler will keep these requests in consideration while scheduling Pods to \n",
      "nodes.\n",
      "We will now explore the node components that are deployed in each of the worker \n",
      "nodes in the cluster:\n",
      "• Kubelet: While the API server, scheduler, controllers, and etcd are deployed on \n",
      "master nodes, kubelets are deployed on minion nodes. They act as agents for the \n",
      "Kubernetes master components and are responsible for managing Pods locally on \n",
      "the nodes. There is one kubelet on each node. A kubelet takes commands from the \n",
      "master components and also provides health, monitoring, and update information \n",
      "about nodes and Pods to the master components, such as the API server and\n",
      "---------------\n",
      "the controller manager. They are the conduit for administrative communication \n",
      "between the master and minion nodes.\n",
      "• kube-proxy: kube-proxy, just like kubelets, is deployed on minion nodes. It is \n",
      "responsible for monitoring Pods and Services, as well as updating the local iptables \n",
      "and netfilter firewall rules with any change in the availability of Pods and Services. \n",
      "This ensures that the routing information on nodes is updated as and when new \n",
      "Pods and Services are created or existing Pods and Services are deleted.\n",
      "---------------\n",
      "484 | Architecting Azure Kubernetes solutions\n",
      "• Container runtime: There are many container vendors and providers in the \n",
      "ecosystem today. Docker is the most famous of them all, though others are also \n",
      "gaining popularity. That's why, in our architecture, we denoted the container \n",
      "runtime with the Docker logo. Kubernetes is a generic container orchestrator. \n",
      "It cannot be tightly coupled with any single container vendor, such as Docker. It \n",
      "should be possible to use any container runtime on the minion nodes to manage \n",
      "the life cycle of containers.\n",
      "To run containers in Pods, an industry-based standard known as a container runtime \n",
      "interface (CRI) has been developed and is used by all leading companies. The standard \n",
      "provides rules that should be followed to achieve interoperability with orchestrators \n",
      "such as Kubernetes. Kubelets do not know which container binaries are installed on the \n",
      "nodes. They could be Docker binaries or any other binaries.\n",
      "---------------\n",
      "As these container runtimes are developed with a common industry-based standard, \n",
      "irrespective of which runtime you are using, kubelets will be able to communicate with \n",
      "the container runtime. This decouples container management from Kubernetes cluster \n",
      "management. The responsibilities of the container runtime include the creation of \n",
      "containers, managing the networking stack of the containers, and managing the bridge \n",
      "network. Since the container management is separate from the cluster management, \n",
      "Kubernetes will not interfere in the responsibilities of the container runtime.\n",
      "The components we discussed are applicable to both unmanaged as well as managed \n",
      "AKS clusters. However, the master components are not exposed to the end user, as \n",
      "Azure manages all that in the case of AKS. Later in this chapter, we will cover the \n",
      "architecture of AKS. You will learn about unmanaged clusters and come to understand \n",
      "the differences between these systems more clearly.\n",
      "---------------\n",
      "Next, you will learn about some of the most important Kubernetes resources, also \n",
      "known as the primitives, knowledge that is applicable to both unmanaged and AKS \n",
      "clusters.\n",
      "Kubernetes primitives\n",
      "You have learned that Kubernetes is an orchestration system used to deploy and \n",
      "manage containers. Kubernetes defines a set of building blocks, which are also known \n",
      "as primitives. These primitives together can help us to deploy, maintain, and scale \n",
      "containerized applications. Let's take a look at each of the primitives and understand \n",
      "their roles.\n",
      "---------------\n",
      "Kubernetes primitives | 485\n",
      "Pod\n",
      "Pods are the most basic unit of Deployment in Kubernetes. The immediate question \n",
      "that arises to a curious mind is how is a Pod different to a container? Pods are wrappers \n",
      "on top of containers. In other words, containers are contained within Pods. There can \n",
      "be multiple containers within a Pod; however, best practice is to have a one-Pod-one-\n",
      "container relationship. This does not mean we cannot have more than one container in \n",
      "a Pod. Multiple containers in a Pod is also fine, as long as there is one main container \n",
      "and the rest are ancillary containers. There are also patterns, such as sidecar patterns, \n",
      "that can be implemented with multi-container Pods.\n",
      "Each Pod has its own IP address and networking stack. All containers share the network \n",
      "interface and the stack. All containers within a Pod can be reached locally using the \n",
      "hostname.\n",
      "A simple Pod definition in YAML format is shown in the following lines of code:\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "---------------\n",
      "name: tappdeployment\n",
      "  labels:\n",
      "    appname: tapp\n",
      "    ostype: linux\n",
      "spec:\n",
      "  containers:\n",
      "  - name: mynewcontainer\n",
      "    image: \"tacracr.azurecr.io/tapp:latest\"\n",
      "    ports:\n",
      "    - containerPort: 80\n",
      "      protocol: TCP\n",
      "      name: http\n",
      "The Pod definition shown has a name and defines a few labels, which can be used by the \n",
      "Service resource to expose to other Pods, nodes and external custom resources. It also \n",
      "defines a single container based on a custom image stored in Azure Container Registry \n",
      "and opens port 80 for the container.\n",
      "---------------\n",
      "486 | Architecting Azure Kubernetes solutions\n",
      "Services\n",
      "Kubernetes allows creating Pods with multiple instances. These Pods should be \n",
      "reachable from any Pod or node within a cluster. It is possible to use the IP address of \n",
      "a Pod directly and access the Pod. However, this is far from ideal. Pods are ephemeral \n",
      "and they might get a new IP address if the previous Pod has gone down. In such cases, \n",
      "the application will break easily. Kubernetes provides Services, which decouple Pod \n",
      "instances from their clients. Pods may get created and torn down, but the IP address of \n",
      "a Kubernetes Service remains constant and stable. Clients can connect to the Service \n",
      "IP address, which in turn has one endpoint for each Pod it can send requests to. If there \n",
      "are multiple Pod instances, each of their IP addresses will be available to the Service as \n",
      "an endpoint object. When a Pod goes down, the endpoints are updated to reflect the \n",
      "current Pod instances along with their IP addresses.\n",
      "---------------\n",
      "Services are highly decoupled with Pods. The main intention of Services is to queue \n",
      "for Pods that have labels in their Service selector definitions. A Service defines label \n",
      "selectors, and based on label selectors, Pod IP addresses are added to the Service \n",
      "resource. Pods and Services can be managed independently of each other.\n",
      "A Service provides multiple types of IP address schemes. There are four types of \n",
      "Services: ClusterIP, NodePort, LoadBalancer, and Ingress Controller using Application \n",
      "Gateway.\n",
      "The most fundamental scheme is known as ClusterIP, and it is an internal IP address \n",
      "that can be reached only from within the cluster. The ClusterIP scheme is shown in \n",
      "Figure 14.4:\n",
      "Figure 14.4: The workings of ClusterIP\n",
      "---------------\n",
      "Kubernetes primitives | 487\n",
      "ClusterIP also allows the creation of NodePort, using which it gets a ClusterIP. However, \n",
      "it can also open a port on each of the nodes within a cluster. The Pods can be reached \n",
      "using ClusterIP addresses as well as by using a combination of the node IP and node \n",
      "port:\n",
      "Figure 14.5: The workings of NodePort\n",
      "Services can refer not only to Pods but to external endpoints as well. Finally, Services \n",
      "also allow the creation of a load balancer–based service that is capable of receiving \n",
      "requests externally and redirecting them to a Pod instance using ClusterIP and \n",
      "NodePort internally:\n",
      "Figure 14.6: The workings of Load Balancer\n",
      "There is one final type of service known as Ingress Controller, which provides advanced \n",
      "functionalities such as URL-based routing, as shown in Figure 14.7:\n",
      "Figure 14.7: The workings of Ingress Controller\n",
      "---------------\n",
      "488 | Architecting Azure Kubernetes solutions\n",
      "A service definition in YAML format is shown here:\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: tappservice\n",
      "  labels:\n",
      "    appname: tapp\n",
      "    ostype: linux\n",
      "spec:\n",
      "  type: LoadBalancer\n",
      "  selector:\n",
      "    appname: myappnew\n",
      "  ports:\n",
      "  - name: http\n",
      "    port: 8080\n",
      "    targetPort: 80\n",
      "    protocol: TCP\n",
      "This service definition creates a load balancer–based service using label selectors.\n",
      "Deployments\n",
      "Kubernetes Deployments are higher-level resources in comparison to ReplicaSets and \n",
      "Pods. Deployments provide functionality related to the upgrading and release of an \n",
      "application. Deployment resources create a ReplicaSet, and the ReplicaSet manages the \n",
      "Pod. It is important to understand the need for deployment resources when ReplicaSets \n",
      "already exist.\n",
      "Deployments play a significant role in upgrading applications. If an application is \n",
      "already in production and a new version of the application needs to be deployed, there \n",
      "are a few choices for you:\n",
      "---------------\n",
      "1. Delete existing Pods and create new Pods – in this method, there is downtime for \n",
      "the application, so this method should only be used if downtime is acceptable. \n",
      "There is a risk of increased downtime if the Deployment contains bugs and you \n",
      "have to roll back to a previous version.\n",
      "---------------\n",
      "Kubernetes primitives | 489\n",
      "2. Blue-green deployment – In this method, the existing Pods continue to run and \n",
      "a new set of Pods is created with the new version of the application. The new \n",
      "Pods are not reachable externally. Once the tests have successfully completed, \n",
      "Kubernetes starts pointing to the new set of Pods. The old Pods can stay as-is or \n",
      "can be subsequently deleted.\n",
      "3. Rolling upgrades – In this method, existing Pods are deleted one at a time while \n",
      "new Pods for the new application version are created one at a time. The new Pods \n",
      "are incrementally deployed while the old Pods are incrementally reduced, until \n",
      "they reach a count of zero.\n",
      "All these approaches would have to be carried out manually without a Deployment \n",
      "resource. A Deployment resource automates the entire release and upgrade process. It \n",
      "can also help to automatically roll back to a previous version if there are any issues with \n",
      "the current Deployment.\n",
      "---------------\n",
      "A Deployment definition is shown in the following code listing:\n",
      "---\n",
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: tappdeployment\n",
      "  labels:\n",
      "    appname: tapp\n",
      "    ostype: linux\n",
      "spec:\n",
      "  replicas: 3\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      appname: myappnew\n",
      "  strategy:\n",
      "    type: RollingUpdate\n",
      "    rollingUpdate:\n",
      "       maxSurge: 1\n",
      "       maxUnavailable: 1  \n",
      "  template:\n",
      "---------------\n",
      "490 | Architecting Azure Kubernetes solutions\n",
      "    metadata:\n",
      "      name: mypod\n",
      "      labels:\n",
      "        appname: myappnew\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: mynewcontainer\n",
      "        image: \"tacracr.azurecr.io/tapp:latest\"\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "          protocol: TCP\n",
      "          name: http\n",
      "It is important to note that a Deployment has a strategy property, which determines \n",
      "whether the recreate or RollingUpdate strategy is used. recreate will delete all \n",
      "existing Pods and create new Pods. It also contains configuration details related to \n",
      "RollingUpdate by providing the maximum number of Pods that can be created and \n",
      "destroyed in a single execution.\n",
      "Replication controller and ReplicaSet\n",
      "Kubernetes' replication controller resource ensures that a specified desired number of \n",
      "Pod instances are always running within a cluster. Any deviation from the desired state \n",
      "is watched for by the replication controller, and it creates new Pod instances to meet \n",
      "the desired state.\n",
      "---------------\n",
      "ReplicaSets are the new version of the replication controller. ReplicaSets provide the \n",
      "same functionality as that of replication controllers, with a few advanced functionalities. \n",
      "The main one among these is the rich capability for defining the selectors associated \n",
      "with Pods. With ReplicaSets, it is possible to define the dynamic expressions that were \n",
      "missing with replication controllers.\n",
      "It is recommended to use ReplicaSets rather than replication controllers.\n",
      "The next code listing shows an example of defining a ReplicaSet resource:\n",
      "---\n",
      "apiVersion: apps/v1\n",
      "kind: ReplicaSet\n",
      "metadata:\n",
      "---------------\n",
      "Kubernetes primitives | 491\n",
      "  name: tappdeployment\n",
      "  labels:\n",
      "    appname: tapp\n",
      "    ostype: linux\n",
      "spec:\n",
      "  replicas: 3\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      appname: myappnew\n",
      "  template:\n",
      "    metadata:\n",
      "      name: mypod\n",
      "      labels:\n",
      "        appname: myappnew\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: mynewcontainer\n",
      "        image: \"tacracr.azurecr.io/tapp:latest\"\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "          protocol: TCP\n",
      "          name: http\n",
      "It is important to note that ReplicaSets have a replicas property, which determines \n",
      "the count of Pod instances, a selector property, which defines the Pods that should be \n",
      "managed by ReplicaSet, and finally the template property, which defines the Pod itself.\n",
      "ConfigMaps and Secrets\n",
      "Kubernetes provides two important resources to store configuration data. ConfigMaps \n",
      "are used to store general configuration data that is not security-sensitive. Generic \n",
      "application configuration data, such as folder names, volume names, and DNS names,\n",
      "---------------\n",
      "can be stored in ConfigMaps. On the other hand, sensitive data, such as credentials, \n",
      "certificates, and secrets, should be stored within Secrets resources. This Secrets data is \n",
      "encrypted and stored within the Kubernetes etcd data store.\n",
      "---------------\n",
      "492 | Architecting Azure Kubernetes solutions\n",
      "Both ConfigMaps and Secrets data can be made available as environment variables or \n",
      "volumes within Pods.\n",
      "The definition of the Pod that wants to consume these resources should include a \n",
      "reference to them. We have now covered the Kubernetes primitives and the roles of \n",
      "each of the building blocks. Next, you will be learning about the architecture of AKS.\n",
      "AKS architecture\n",
      "In the previous section, we discussed the architecture of an unmanaged cluster. Now, \n",
      "we will be exploring the architecture of AKS. When you have read this section, you will \n",
      "be able to point out the major differences between the architecture of unmanaged and \n",
      "managed (AKS, in this case) clusters.\n",
      "When an AKS instance is created, the worker nodes only are created. The master \n",
      "components are managed by Azure. The master components are the API server, the \n",
      "scheduler, etcd, and the controller manager, which we discussed earlier. The kubelets\n",
      "---------------\n",
      "and kube-proxy are deployed on the worker nodes. Communication between the \n",
      "nodes and master components happens using kubelets, which act as agents for the \n",
      "Kubernetes clusters for the node:\n",
      "Figure 14.8: AKS architecture\n",
      "When a user requests a Pod instance, the user request lands with the API server. \n",
      "The API server checks and validates the request details and stores in etcd (the data \n",
      "store for the cluster) and also creates the deployment resource (if the Pod request is \n",
      "wrapped around a deployment resource). The deployment controller keeps a watch on \n",
      "the creation of any new deployment resources. If it sees one, it creates a ReplicaSet \n",
      "resource based on the definition provided in the user request.\n",
      "---------------\n",
      "Deploying an AKS cluster | 493\n",
      "The ReplicaSet controller keeps a watch on the creation of any new ReplicaSet \n",
      "resources, and upon seeing a resource being created, it asks the scheduler to schedule \n",
      "the Pods. The scheduler has its own procedure and rules for finding an appropriate \n",
      "node for hosting the Pods. The scheduler informs the kubelet of the node and the \n",
      "kubelet then fetches the definition for the Pod and creates the Pods using the container \n",
      "runtime installed on the nodes. The Pod finally creates the containers within its \n",
      "definition.\n",
      "kube-proxy helps in maintaining the list of IP addresses of Pod and Service information \n",
      "on local nodes, as well as updating the local firewall and routing rules. To do a quick \n",
      "recap of what we have discussed so far, we started off with the Kubernetes architecture \n",
      "and then moved on to primitives, followed by the architecture of AKS. Since you are \n",
      "clear on the concepts, let's go ahead and create an AKS cluster in the next section.\n",
      "---------------\n",
      "Deploying an AKS cluster\n",
      "AKS can be provisioned using the Azure portal, the Azure CLI (command-line \n",
      "interface), Azure PowerShell cmdlets, ARM templates, SDKs (software development \n",
      "kits) for supported languages, and even Azure ARM REST APIs.\n",
      "The Azure portal is the simplest way of creating an AKS instance; however, to enable \n",
      "DevOps, it is better to create an AKS instance using ARM templates, the CLI, or \n",
      "PowerShell.\n",
      "Creating an AKS cluster\n",
      "Let's create a resource group to deploy our AKS cluster. From the Azure CLI, use the az \n",
      "group create command:\n",
      "az group create -n AzureForArchitects -l southeastasia\n",
      "Here, -n denotes the name of the resource group and -l denotes the location. If the \n",
      "request was successful, you will see a similar response to this:\n",
      "Figure 14.9: Resource group creation\n",
      "---------------\n",
      "494 | Architecting Azure Kubernetes solutions\n",
      "Now that we have the resource group ready, we will go ahead and create the AKS \n",
      "cluster using the az aks create command. The following command will create a cluster \n",
      "named AzureForArchitects-AKS in the AzureForArchitects resource group with a node \n",
      "count of 2. The --generate-ssh-keys parameter will allow the creation of RSA (Rivest–\n",
      "Shamir–Adleman) key pairs, a public-key cryptosystem:\n",
      "az aks create --resource-group AzureForArchitects \\\n",
      "--name AzureForArchitects-AKS \\\n",
      "--node-count 2 \\\n",
      "--generate-ssh-keys\n",
      "If the command succeeded, you will be able to see a similar output to this:\n",
      "Figure 14.10: Creating the cluster\n",
      "Going through the cluster, you will see a line item that says \"nodeResourceGroup\": \"MC_\n",
      "AzureForArchitects_AzureForArchitects-AKS_southeastasia\". When creating an AKS \n",
      "cluster, a second resource is automatically created to store the node resources.\n",
      "---------------\n",
      "Deploying an AKS cluster | 495\n",
      "Our cluster is provisioned. Now we need to connect to the cluster and interact with \n",
      "it. To control the Kubernetes cluster manager, we will be using kubectl. In the next \n",
      "section, we will take a quick look at kubectl.\n",
      "Kubectl\n",
      "Kubectl is the main component through which developers and infrastructure \n",
      "consultants can interact with AKS. Kubectl helps in creating a REST request containing \n",
      "the HTTP header and body, and submitting it to the API server. The header contains the \n",
      "authentication details, such as a token or username/password combination. The body \n",
      "contains the actual payload in JSON format.\n",
      "The kubectl command provides rich log details when used along with the verbose \n",
      "switch. The switch takes an integer input that can range from 0 to 9, which can be \n",
      "viewed from the details logs.\n",
      "Connecting to the cluster\n",
      "To connect to the cluster locally, we need to install kubectl. Azure Cloud Shell has\n",
      "---------------\n",
      "kubectl already installed. If you want to connect locally, use az aks install-cli to \n",
      "install kubectl.\n",
      "In order to configure kubectl to connect to our Kubernetes cluster, we need to \n",
      "download the credentials and configure the CLI with them. This can be done using the \n",
      "az aks get-credentials command. Use the command as shown here:\n",
      "az aks get-credentials \\\n",
      "--resource-group AzureForArchitects \\\n",
      "--name AzureForArchitects-AKS\n",
      "Now, we need to verify whether we're connected to the cluster. As mentioned earlier, \n",
      "we'll be using kubectl to communicate with the cluster, and kubectl get nodes will show \n",
      "a list of nodes in the cluster. During creation, we set the node count to 2, so the output \n",
      "should have two nodes. Also, we need to make sure that the status of the node is Ready. \n",
      "The output should be something like Figure 14.11:\n",
      "Figure 14.11: Getting the list of nodes\n",
      "---------------\n",
      "496 | Architecting Azure Kubernetes solutions\n",
      "Since our node is in the Ready state, let's go ahead and create a Pod. There are two ways \n",
      "in which you can create resources in Kubernetes. They are:\n",
      "• Imperative: In this method, we use the kubectl run and kubectl expose commands \n",
      "to create the resources.\n",
      "• Declarative: We describe the state of the resource via JSON or a YAML file. While \n",
      "we were discussing Kubernetes primitives, you saw a lot of YAML files for each of \n",
      "the building blocks. We will pass the file to the kubectl apply command to create \n",
      "the resources, and the resources declared in the file will be created.\n",
      "Let's take the imperative approach first, to create a Pod with the name webserver, \n",
      "running an NGINX container with port 80 exposed:\n",
      "kubectl run webserver --restart=Never --image nginx --port 80\n",
      "Upon successful completion of the command, the CLI will let you know the status:\n",
      "Figure 14.12: Creating a Pod\n",
      "---------------\n",
      "Now that we have tried the imperative method, let's follow the declarative method. \n",
      "You can use the structure of the YAML file we discussed in the Pod subsection of the \n",
      "Kubernetes primitives section and modify it as per your requirements.\n",
      "We will be using the NGINX image, and the Pod will be named webserver-2.\n",
      "You can use any text editor and create the file. The final file will look similar to this:\n",
      "apiVersion: v1\n",
      "kind: Pod\n",
      "metadata:\n",
      "  name: webserver-2\n",
      "  labels:\n",
      "    appname: nginx\n",
      "    ostype: linux\n",
      "spec:\n",
      "---------------\n",
      "Deploying an AKS cluster | 497\n",
      "  containers:\n",
      "  - name: wenserver-2-container\n",
      "    image: nginx\n",
      "    ports:\n",
      "    - containerPort: 80\n",
      "      protocol: TCP\n",
      "      name: http\n",
      "In the kubectl apply command, we will pass the filename to the -f parameter, as shown \n",
      "in Figure 14.13, and you can see that the Pod has been created:\n",
      "Figure 14.13: Creating a Pod using the declarative method.\n",
      "Since we have created the Pods, we can use the kubectl get pods command to list \n",
      "all the Pods. Kubernetes uses the concept of namespaces for the logical isolation of \n",
      "resources. By default, all commands are pointing to the default namespace. If you want \n",
      "to perform an action on a specific namespace, you can pass the namespace name via \n",
      "the -n parameter. In Figure 14.14, you can see that kubectl get pods returns the Pods we \n",
      "created in the previous example, which reside in the default namespace. Also, when we \n",
      "use --all-namespaces, the output returns pods in all namespaces:\n",
      "Figure 14.14: Listing all Pods\n",
      "---------------\n",
      "498 | Architecting Azure Kubernetes solutions\n",
      "Now we will create a simple Deployment that runs NGINX and with a load balancer that \n",
      "exposes it to the internet. The YAML file will look like this:\n",
      "#Creating a deployment that runs six replicas of nginx \n",
      "apiVersion: apps/v1\n",
      "kind: Deployment\n",
      "metadata:\n",
      "  name: nginx-server\n",
      "spec:\n",
      "  replicas: 6\n",
      "  selector:\n",
      "    matchLabels:\n",
      "      app: nginx-server\n",
      "  template:\n",
      "    metadata:\n",
      "      labels:\n",
      "        app: nginx-server\n",
      "    spec:\n",
      "      containers:\n",
      "      - name: nginx-server\n",
      "        image: nginx\n",
      "        ports:\n",
      "        - containerPort: 80\n",
      "          name: http\n",
      "---\n",
      "#Creating Service\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: nginx-service\n",
      "---------------\n",
      "Deploying an AKS cluster | 499\n",
      "spec:\n",
      "  ports:\n",
      "  - port: 80\n",
      "  selector:\n",
      "    app: nginx-server\n",
      "---\n",
      "apiVersion: v1\n",
      "kind: Service\n",
      "metadata:\n",
      "  name: nginx-lb\n",
      "spec:\n",
      "  type: LoadBalancer\n",
      "  ports:\n",
      "  - port: 80\n",
      "  selector:\n",
      "    app: nginx-server\n",
      "We'll be using the kubectl apply command and passing the YAML file to the -f \n",
      "parameter.\n",
      "Upon success, all three Services will be created, and if you execute the kubectl get \n",
      "deployment nginx-server command, you will see six replicas running, as shown in \n",
      "Figure 14.15, to make the Service highly available:\n",
      "Figure 14.15: Checking the deployment\n",
      "---------------\n",
      "500 | Architecting Azure Kubernetes solutions\n",
      "Since our Deployment is provisioned, we need to check what the public IP of the load \n",
      "balancer that we created is. We can use the kubectl get service nginx-lb --watch \n",
      "command. When the load balancer is initializing, EXTERNAL-IP will show as <pending>, \n",
      "the --wait parameter will let the command run in the foreground, and when the public \n",
      "IP is allocated, we will be able to see a new line, as shown here:\n",
      "Figure 14.16: Finding public IP of the load balancer\n",
      "Now that we have the public IP, we can go to the browser and should see the NGINX \n",
      "landing page, as shown in Figure 14.17:\n",
      "Figure 14.17: NGINX landing page\n",
      "Similarly, you can use the YAML files we discussed in the Kubernetes primitives section to \n",
      "create different types of resources.\n",
      "There are a lot of commands, such as logs, describe, exec, and delete, that \n",
      "administrators need to use with the kubectl command. The objective of this section was\n",
      "---------------\n",
      "to enable you to create an AKS cluster, connect to the cluster, and deploy a simple web \n",
      "application.\n",
      "In the next section, we will be discussing AKS networking.\n",
      "AKS networking\n",
      "Networking forms a core component within a Kubernetes cluster. The master \n",
      "components should be able to reach the minion nodes and the Pods running on top of \n",
      "them, while the worker nodes should be able to communicate among themselves as well \n",
      "as with the master components.\n",
      "It might come as a surprise that core Kubernetes does not manage the networking \n",
      "stack at all. It is the job of the container runtime on the nodes.\n",
      "---------------\n",
      "AKS networking | 501\n",
      "Kubernetes has prescribed three important tenets that any container runtime should \n",
      "adhere to. These are as follows:\n",
      "• Pods should be able to communicate with other Pods without any transformation \n",
      "in their source or destination addresses, something that is performed using \n",
      "network address translation (NAT).\n",
      "• Agents such as kubelets should be able to communicate with Pods directly on the \n",
      "nodes.\n",
      "• Pods that are directly hosted on the host network still should be able to \n",
      "communicate with all Pods in the cluster.\n",
      "Every Pod gets a unique IP address within the Kubernetes cluster, along with a \n",
      "complete network stack, similar to virtual machines. They all are connected to the local \n",
      "bridge network created by the Container Networking Interface (CNI) component. The \n",
      "CNI component also creates the networking stack of the Pod. The bridge network then \n",
      "talks to the host network and becomes the conduit for the flow of traffic from Pods to \n",
      "network and vice versa.\n",
      "---------------\n",
      "CNI is a standard managed and maintained by the Cloud Native Computing Foundation \n",
      "(CNCF), and there are many providers that provide their own implementation of the \n",
      "interface. Docker is one of these providers. There are others, such as rkt (read as \n",
      "rocket), weave, calico, and many more. Each has its own capabilities and independently \n",
      "decides the network capabilities, while ensuring that the main tenets of Kubernetes \n",
      "networking are followed completely.\n",
      "AKS provides two distinct networking models:\n",
      "• Kubenet\n",
      "• Azure CNI\n",
      "Kubenet\n",
      "Kubenet is the default networking framework in AKS. Under Kubenet, each node gets an \n",
      "IP address from the subnet of the virtual network they are connected with. The Pods do \n",
      "not get IP addresses from the subnet. Instead, a separate addressing scheme is used to \n",
      "provide IP addresses to Pods and Kubernetes Services. While creating an AKS instance, \n",
      "it is important to set the IP address range for Pods and Services. Since Pods are not on\n",
      "---------------\n",
      "the same network as that of nodes, requests from Pods and to Pods are always NATed/\n",
      "routed to replace the source Pod IP with the node IP address and vice versa.\n",
      "In user-defined routing, Azure can support up to 400 routes, and you also cannot have \n",
      "a cluster larger than 400 nodes. Figure 14.18 shows how the AKS node receives an IP \n",
      "address from the virtual network, but not the Pods created in the node:\n",
      "---------------\n",
      "502 | Architecting Azure Kubernetes solutions\n",
      "Figure 14.18: Networking in AKS\n",
      "By default, this Kubenet is configured with 110 Pods per node. This means there can be a \n",
      "maximum of 110 * 400 Pods in a Kubernetes cluster by default. The maximum number of \n",
      "Pods per node is 250.\n",
      "This scheme should be used when IP address availability and having user-defined \n",
      "routing are not a constraint.\n",
      "In the Azure CLI, you can execute the following command to create an AKS instance \n",
      "using this networking stack:\n",
      "az aks create \\\n",
      "    --resource-group myResourceGroup \\\n",
      "    --name myAKSCluster \\\n",
      "    --node-count 3 \\\n",
      "    --network-plugin kubenet \\\n",
      "    --service-cidr 10.0.0.0/16 \\\n",
      "    --dns-service-ip 10.0.0.10 \\\n",
      "    --pod-cidr 10.244.0.0/16 \\\n",
      "    --docker-bridge-address 172.17.0.1/16 \\\n",
      "    --vnet-subnet-id $SUBNET_ID \\\n",
      "    --service-principal <appId> \\\n",
      "    --client-secret <password>\n",
      "---------------\n",
      "AKS networking | 503\n",
      "Notice how all the IP addresses are explicitly provided for Service resources, Pods, \n",
      "nodes, and Docker bridges. These are non-overlapping IP address ranges. Also notice \n",
      "that Kubenet is used as a network plugin.\n",
      "Azure CNI (advanced networking)\n",
      "With Azure CNI, each node and Pod gets an IP address assigned from the network \n",
      "subnet directly. This means there can be as many Pods as there are unique IP addresses \n",
      "available on a subnet. This makes IP address range planning much more important \n",
      "under this networking strategy.\n",
      "It is important to note that Windows hosting is only possible using the Azure CNI \n",
      "networking stack. Moreover, some of AKS components, such as virtual nodes and \n",
      "virtual kubelets, are also dependent on the Azure CNI stack. There is a need to reserve \n",
      "IP addresses in advance, depending on the number of Pods that will be created. There \n",
      "should always be extra IP addresses available on the subnet, to avoid exhaustion of\n",
      "---------------\n",
      "IP addresses or to avoid the need to rebuild the cluster for a larger subnet due to \n",
      "application demand.\n",
      "By default, this networking stack is configured for 30 Pods per node and it can be \n",
      "configured with 250 Pods as the maximum number of Pods per node.\n",
      "The command to execute to create an AKS instance using this networking stack is \n",
      "shown here:\n",
      "az aks create \\\n",
      "    --resource-group myResourceGroup \\\n",
      "    --name myAKSCluster \\\n",
      "    --network-plugin azure \\\n",
      "    --vnet-subnet-id <subnet-id> \\\n",
      "    --docker-bridge-address 172.17.0.1/16 \\\n",
      "    --dns-service-ip 10.2.0.10 \\\n",
      "    --service-cidr 10.2.0.0/24 \\\n",
      "    --generate-ssh-keys\n",
      "Notice how all the IP addresses are explicitly provided for Service resources, Pods, \n",
      "nodes, and Docker bridges. These are non-overlapping IP address ranges. Also, notice \n",
      "that Azure is used as a network plugin.\n",
      "So far, you have learned how to deploy a solution and manage the networking of an AKS\n",
      "---------------\n",
      "cluster. Security is another important factor that needs to be addressed. In the next \n",
      "section, we will be focusing on access and identity options for AKS.\n",
      "---------------\n",
      "504 | Architecting Azure Kubernetes solutions\n",
      "Access and identity for AKS\n",
      "Kubernetes clusters can be secured in multiple ways.\n",
      "The service account is one of the primary user types in Kubernetes. The Kubernetes API \n",
      "manages the service account. Authorized Pods can communicate with the API server \n",
      "using the credentials of service accounts, which are stored as Kubernetes Secrets. \n",
      "Kubernetes does not have any data store or identity provider of its own. It delegates \n",
      "the responsibility of authentication to external software. It provides an authentication \n",
      "plugin that checks for the given credentials and maps them to available groups. If the \n",
      "authentication is successful, the request passes to another set of authorization plugins \n",
      "to check the permission levels of the user on the cluster, as well as the namespace-\n",
      "scoped resources.\n",
      "For Azure, the best security integration would be to use Azure AD. Using Azure AD, you\n",
      "---------------\n",
      "can also bring your on-premises identities to AKS to provide centralized management \n",
      "of accounts and security. The basic workflow of Azure AD integration is shown in Figure \n",
      "14.19:\n",
      "Figure 14.19: Basic workflow of Azure AD integration\n",
      "Users or groups can be granted access to resources within a namespace or across a \n",
      "cluster. In the previous section, we used the az aks get-credential command to get \n",
      "the credentials and the kubectl configuration context. When the user tries to interact \n",
      "with kubectl, they are prompted to sign in using their Azure AD credentials. Azure AD \n",
      "validates the credentials and a token is issued for the user. Based on the access level \n",
      "they have, they can access the resources in the cluster or the namespace.\n",
      "Additionally, you can leverage Azure Role-Based Access Control (RBAC) to limit access \n",
      "to the resources in the resource group.\n",
      "In the next section, we will be discussing virtual kubelet, which is one of the quickest \n",
      "ways to scale out a cluster.\n",
      "---------------\n",
      "Virtual kubelet | 505\n",
      "Virtual kubelet\n",
      "Virtual kubelet is currently in preview and is managed by the CNCF organization. It is \n",
      "quite an innovative approach that AKS uses for scalability purposes. Virtual kubelet is \n",
      "deployed on the Kubernetes cluster as a Pod. The container running within the Pod \n",
      "uses the Kubernetes SDK to create a new node resource and represents itself to the \n",
      "entire cluster as a node. The cluster components, including the API server, scheduler, \n",
      "and controllers, think of it and treat it as a node and schedule Pods on it.\n",
      "However, when a Pod is scheduled on this node that is masquerading as a node, it \n",
      "communicates to its backend components, known as providers, to create, delete, and \n",
      "update the Pods. One of the main providers on Azure is Azure Container Instances. \n",
      "Azure Batch can also be used as a provider. This means the containers are actually \n",
      "created on Container Instances or Azure Batch rather than on the cluster itself;\n",
      "---------------\n",
      "however, they are managed by the cluster. The architecture of virtual kubelet is shown \n",
      "in Figure 14.20:\n",
      "Figure 14.20: Virtual kubelet architecture\n",
      "---------------\n",
      "506 | Architecting Azure Kubernetes solutions\n",
      "Notice that virtual kubelet is represented as a node within the cluster and can help in \n",
      "hosting and managing Pods, just like a normal kubelet would. However, virtual kubelet \n",
      "has one limitation; this is what we are going to discuss in the next section. \n",
      "Virtual nodes\n",
      "One of the limitations of virtual kubelet is that the Pods deployed on virtual kubelet \n",
      "providers are isolated and do not communicate with other Pods in the cluster. If there \n",
      "is a need for the Pods on these providers to talk to other Pods and nodes in the cluster \n",
      "and vice versa, then virtual nodes should be created. Virtual nodes are created on a \n",
      "different subnet on the same virtual network that is hosting Kubernetes cluster nodes, \n",
      "which can enable communication between Pods. Only the Linux operating system is \n",
      "supported, at the time of writing, for working with virtual nodes.\n",
      "Virtual nodes give a perception of a node; however, the node does not exist. Anything\n",
      "---------------\n",
      "scheduled on such a node actually gets created in Azure Container Instances. Virtual \n",
      "nodes are based on virtual kubelet but have the extra functionality of seamless to-and-\n",
      "fro communication between the cluster and Azure Container Instances.\n",
      "While deploying Pods on virtual nodes, the Pod definition should contain an appropriate \n",
      "node selector to refer to virtual nodes, and also tolerations, as shown in next code \n",
      "snippet:\n",
      "      nodeSelector:\n",
      "        kubernetes.io/role: agent\n",
      "        beta.kubernetes.io/os: linux\n",
      "        type: virtual-kubelet\n",
      "      tolerations:\n",
      "      - key: virtual-kubelet.io/provider\n",
      "        operator: Exists\n",
      "      - key: azure.com/aci\n",
      "        effect: NoSchedule\n",
      "Here, the node selector is using the type property to refer to virtual kubelet and the \n",
      "tolerations property to inform Kubernetes that nodes with taints, virtual-kubelet.io/\n",
      "provider,should allow the Deployment of these Pods on them.\n",
      "---------------\n",
      "Summary | 507\n",
      "Summary\n",
      "Kubernetes is the most widely used container orchestrator and works with different \n",
      "container and network runtimes. In this chapter, you learned about the basics of \n",
      "Kubernetes, its architecture, and some of the important infrastructure components, \n",
      "such as etcd, the API server, controller managers, and the scheduler, along with their \n",
      "purpose. Plus, we looked at important resources that can be deployed to manage \n",
      "applications, such as Pods, replication controllers, ReplicaSets, Deployments, and \n",
      "Services.\n",
      "AKS provides a couple of different networking stacks—Azure CNI and Kubenet. They \n",
      "provide different strategies for assigning IP addresses to Pods. While Azure CNI \n",
      "provides IP addresses to Pods from the underlying subnet, Kubenet uses virtual IP \n",
      "addresses only.\n",
      "We also covered some of the features provided exclusively by Azure, such as virtual \n",
      "nodes, and concepts around virtual kubelet. In the next chapter, we will learn about the\n",
      "---------------\n",
      "provisioning and configuring resources with ARM templates.\n",
      "---------------\n",
      "Azure Resource Manager (ARM) templates are the preferred mechanism for \n",
      "provisioning resources and configuring them on Azure.\n",
      "ARM templates help to implement a relatively new paradigm known as Infrastructure \n",
      "as Code (IaC). ARM templates convert the infrastructure and its configuration into \n",
      "code, which has numerous advantages. IaC brings a high level of consistency and \n",
      "predictability to deployments across environments. It also ensures that environments \n",
      "can be tested before going to production, and, finally, it gives a high level of confidence \n",
      "in the deployment process, maintenance, and governance.\n",
      "Cross-subscription \n",
      "deployments using \n",
      "ARM templates\n",
      "15\n",
      "---------------\n",
      "510 | Cross-subscription deployments using ARM templates\n",
      "The following topics will be covered in this chapter:\n",
      "• ARM templates\n",
      "• Deploying resource groups with ARM templates\n",
      "• Deploying resources across subscriptions and resource groups\n",
      "• Deploying cross-subscription and resource group deployments using linked \n",
      "templates\n",
      "• Creating ARM templates for PaaS, data, and IaaS solutions\n",
      "ARM templates\n",
      "A prominent advantage of IaC is that it can be version controlled. It can also be reused \n",
      "across environments, which provides a high degree of consistency and predictability \n",
      "in deployments, and ensures that the impact and result of deploying an ARM template \n",
      "is the same no matter the number of times the template is deployed. This feature is \n",
      "known as idempotency.\n",
      "ARM templates debuted with the introduction of the ARM specification and have \n",
      "been getting richer in features and growing in maturity since then. It's important to\n",
      "---------------\n",
      "understand that there's generally a feature gap of a few weeks to a couple of months \n",
      "between the actual resource configuration and the availability of the configuration in \n",
      "ARM templates.\n",
      "Each resource has its own configuration. This configuration can be affected in a \n",
      "multitude of ways, including using Azure PowerShell, the Azure CLI, Azure SDKs, REST \n",
      "APIs, and ARM templates.\n",
      "Each of these techniques has its own development and release life cycle, which is \n",
      "different from the actual resource development. Let's try to understand this with the \n",
      "help of an example.\n",
      "The Azure Databricks resource has its own cadence and development life cycle. The \n",
      "consumers of this resource have their own development life cycle, in turn, which is \n",
      "different from the actual resource development. If Databricks gets its first release on \n",
      "December 31, the Azure PowerShell cmdlets for it might not be available on the same\n",
      "---------------\n",
      "date and might even be released on January 31 of the next year; similarly, the availability \n",
      "of these features in the REST API and ARM templates might be around January 15.\n",
      "---------------\n",
      "ARM templates | 511\n",
      "ARM templates are JSON-based documents that, when executed, invoke a REST API on \n",
      "the Azure management plane and submit the entire document to it. The REST API has \n",
      "its own development life cycle, and the JSON schema for the resource has its own life \n",
      "cycle too.\n",
      "This means the development of a feature within a resource needs to happen in at least \n",
      "three different components before they can be consumed from ARM templates. These \n",
      "include:\n",
      "• The resource itself\n",
      "• The REST API for the resource\n",
      "• The ARM template resource schema\n",
      "Each resource in the ARM template has the apiVersion property. This property helps to \n",
      "decide the REST API version that should be used to provision and deploy the resource. \n",
      "Figure 15.1 shows the flow of requests from the ARM template to resource APIs that are \n",
      "responsible for the creation, updating, and deletion of resources:\n",
      "Figure 15.1: Request flow\n",
      "A resource configuration, such as a storage account in an ARM template, looks as \n",
      "follows:\n",
      "{\n",
      "---------------\n",
      "\"type\": \"Microsoft.Storage/storageAccounts\", \n",
      "  \"apiVersion\": \"2019-04-01\", \n",
      "   \"name\": \"[variables('storage2')]\", \n",
      "   \"location\": \"[resourceGroup().location]\", \n",
      "   \"kind\": \"Storage\", \n",
      "   \"sku\": { \n",
      "                \"name\": \"Standard_LRS\" \n",
      "          } \n",
      "}\n",
      "---------------\n",
      "512 | Cross-subscription deployments using ARM templates\n",
      "In the preceding code, the availability of this schema for defining sku is based on the \n",
      "development of the ARM template schema. The availability of the REST API and its \n",
      "version number is determined by apiVersion, which happens to be 2019-04-01. The \n",
      "actual resource is determined by the type property, which has the following two parts:\n",
      "• Resource-provider namespace: Resources in Azure are hosted within namespaces \n",
      "and related resources are hosted within the same namespace.\n",
      "• Resource type: Resources are referenced using their type name.\n",
      "In this case, the resource is identified by its provider name and type, which happens to \n",
      "be Microsoft.Storage/storageaccounts.\n",
      "Previously, ARM templates expected resource groups to be available prior to \n",
      "deployment. They were also limited to deploying to a single resource group within a \n",
      "single subscription.\n",
      "This meant that, until recently, an ARM template could deploy all resources within\n",
      "---------------\n",
      "a single resource group. Azure ARM templates now have added functionality for \n",
      "deploying resources to multiple resource groups within the same subscription or \n",
      "multiple subscriptions simultaneously. It's now possible to create resource groups as \n",
      "part of ARM templates, which means it's now possible to deploy resources in multiple \n",
      "regions into different resource groups.\n",
      "Why would we need to create resource groups from within ARM templates, and \n",
      "why would we need to have cross-subscription and resource group deployments \n",
      "simultaneously?\n",
      "To appreciate the value of creating a resource group and cross-subscription \n",
      "deployments, we need to understand how deployments were carried out prior to these \n",
      "features being available.\n",
      "To deploy an ARM template, a resource group is a prerequisite. Resource groups should \n",
      "be created prior to the deployment of a template. Developers use PowerShell, the Azure \n",
      "CLI, or the REST API to create resource groups and then initiate the deployment of ARM\n",
      "---------------\n",
      "templates. This means that any end-to-end deployment consists of multiple steps. The \n",
      "first step is to provision the resource group and the next step is the deployment of the \n",
      "ARM template to this newly created resource group. These steps could be executed \n",
      "using a single PowerShell script or individual steps from the PowerShell command line. \n",
      "The PowerShell script should be complete with regard to code related to exception \n",
      "handling, taking care of edge cases, and ensuring that there are no bugs in it before it \n",
      "can be said to be enterprise-ready. It is important to note that resource groups can be \n",
      "deleted from Azure, and the next time the script runs, they might be expected to be \n",
      "available. It would fail because it might assume that the resource group exists. In short, \n",
      "the deployment of the ARM template to a resource group should be an atomic step \n",
      "rather than multiple steps.\n",
      "---------------\n",
      "Deploying resource groups with ARM templates | 513\n",
      "Compare this with the ability to create resource groups and their constituent resources \n",
      "together within the same ARM templates. Whenever you deploy the template, it ensures \n",
      "that the resource groups are created if they don't yet exist and continues to deploy \n",
      "resources to them after creation.\n",
      "Let's also look at how these new features can help to remove some of the technical \n",
      "constraints related to disaster recovery sites.\n",
      "Prior to these features, if you had to deploy a solution that was designed with disaster \n",
      "recovery in mind, there were two separate deployments: one deployment for the \n",
      "primary region and another deployment for the secondary region. For example, if \n",
      "you were deploying an ASP.NET MVC application using App Service, you would create \n",
      "an app service and configure it for the primary region, and then you would conduct \n",
      "another deployment with the same template to another region using a different\n",
      "---------------\n",
      "parameters file. When deploying another set of resources in another region, as \n",
      "mentioned before, the parameters used with the template should be different to reflect \n",
      "the differences between the two environments. The parameters would include changes \n",
      "such as a SQL connection string, domain and IP addresses, and other configuration \n",
      "items unique to an environment.\n",
      "With the availability of cross-subscription and resource group deployment, it's possible \n",
      "to create the disaster recovery site at the same time as the primary site. This eliminates \n",
      "two deployments and ensures that the same configuration can be used on multiple \n",
      "sites.\n",
      "Deploying resource groups with ARM templates\n",
      "In this section, an ARM template will be authored and deployed, which will create a \n",
      "couple of resource groups within the same subscription.\n",
      "To use PowerShell to deploy templates that contain resource groups and cross-\n",
      "subscription resources, the latest version of PowerShell should be used. At the time of\n",
      "---------------\n",
      "writing, Azure module version 3.3.0 is being used:\n",
      "Figure 15.2: Verifying the latest Azure module version\n",
      "---------------\n",
      "514 | Cross-subscription deployments using ARM templates\n",
      "If the latest Azure module is not installed, it can be installed using the following \n",
      "command:\n",
      "install-module  -Name az  -Force\n",
      "It's time to create an ARM template that will create multiple resource groups within the \n",
      "same subscription. The code for the ARM template is as follows:\n",
      "{ \n",
      "  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/\n",
      "deploymentTemplate.json#\", \n",
      "  \"contentVersion\": \"1.0.0.0\", \n",
      "  \"parameters\": { \n",
      "    \"resourceGroupInfo\": { \n",
      "      \"type\": \"array\"    }, \n",
      "    \"multiLocation\": { \n",
      "      \"type\": \"array\"      \n",
      "    } \n",
      "  }, \n",
      "  \"resources\": [ \n",
      "    { \n",
      "      \"type\": \"Microsoft.Resources/resourceGroups\", \n",
      "      \"location\": \"[parameters('multiLocation')[copyIndex()]]\", \n",
      "      \"name\": \"[parameters('resourceGroupInfo')[copyIndex()]]\", \n",
      "      \"apiVersion\": \"2019-10-01\", \n",
      "      \"copy\": { \n",
      "        \"name\": \"allResourceGroups\", \n",
      "        \"count\": \"[length(parameters('resourceGroupInfo'))]\" \n",
      "      },\n",
      "---------------\n",
      "\"properties\": {} \n",
      "    } \n",
      "  ], \n",
      "  \"outputs\": {} \n",
      "}\n",
      "The first section of the code is about parameters that the ARM templates expect. These \n",
      "are mandatory parameters, and anybody deploying these templates should provide \n",
      "values for them. Array values must be provided for both the parameters.\n",
      "---------------\n",
      "Deploying resource groups with ARM templates | 515\n",
      "The second major section is the resources JSON array, which can contain multiple \n",
      "resources. In this example, we are creating resource groups, so it is declared within the \n",
      "resources section. Resource groups are getting provisioned in a loop because of the \n",
      "use of the copy element. The copy element ensures that the resource is run a specified \n",
      "number of times and creates a new resource in every iteration. If we send two values \n",
      "for the resourceGroupInfo array parameter, the length of the array would be two, and \n",
      "the copy element will ensure that the resourceGroup resource is executed twice.\n",
      "All resource names within a template should be unique for a resource type. The \n",
      "copyIndex function is used to assign the current iteration number to the overall name \n",
      "of the resource and make it unique. Also, we want the resource groups to be created in \n",
      "different regions using distinct region names sent as parameters. The assignment of a\n",
      "---------------\n",
      "name and location for each resource group is done using the copyIndex function.\n",
      "The code for the parameters file is shown next. This code is pretty straightforward and \n",
      "provides array values to the two parameters expected by the previous template. The \n",
      "values in this file should be changed for all parameters according to your environment:\n",
      "{ \n",
      "  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/\n",
      "deploymentParameters.json#\", \n",
      "  \"contentVersion\": \"1.0.0.0\", \n",
      "  \"parameters\": { \n",
      "    \"resourceGroupInfo\": { \n",
      "      \"value\": [ \"firstResourceGroup\", \"SeocndResourceGroup\" ] \n",
      "    }, \n",
      "    \"multiLocation\": { \n",
      "      \"value\": [ \n",
      "        \"West Europe\", \n",
      "        \"East US\" \n",
      "      ] \n",
      "    } \n",
      "  } \n",
      "}\n",
      "Deploying ARM templates\n",
      "To deploy this template using PowerShell, log in to Azure with valid credentials using \n",
      "the following command:\n",
      "Login-AzAccount\n",
      "---------------\n",
      "516 | Cross-subscription deployments using ARM templates\n",
      "The valid credentials could be a user account or a service principal. Then, use a newly \n",
      "released New-AzDeployment cmdlet to deploy the template. The deployment script is \n",
      "available in the multipleResourceGroups.ps1 file:\n",
      "New-AzDeployment  -Location \"West Europe\"  -TemplateFile \"c:\\users\\rites\\\n",
      "source\\repos\\CrossSubscription\\CrossSubscription\\multipleResourceGroups.\n",
      "json\" -TemplateParameterFile \"c:\\users\\rites\\source\\repos\\CrossSubscription\\\n",
      "CrossSubscription\\multipleResourceGroups.parameters.json\"  -Verbose\n",
      "It's important to understand that the New-AzResourceGroupDeployment cmdlet can't \n",
      "be used here because the scope of the New-AzResourceGroupDeployment cmdlet is a \n",
      "resource group and it expects a resource group to be available as a prerequisite. For \n",
      "deploying resources at the subscription level, Azure had released a new cmdlet that \n",
      "can work above the resource group scope. The new cmdlet, new-AzDeployment, works\n",
      "---------------\n",
      "at the subscription level. It is also possible to have a deployment at the management \n",
      "group level. Management groups are at a higher level than subscriptions using the \n",
      "New-AzManagementGroupDeployment cmdlet.\n",
      "Deployment of templates using Azure CLI\n",
      "The same template can also be deployed using the Azure CLI. Here are the steps to \n",
      "deploy it using the Azure CLI:\n",
      "1. Use the latest version of the Azure CLI to create resource groups using the ARM \n",
      "template. At the time of writing, version 2.0.75 was used for deployment, as shown \n",
      "here: \n",
      "Figure 15.3: Checking the version of the Azure CLI\n",
      "---------------\n",
      "Deploying resources across subscriptions and resource groups | 517\n",
      "2. Log in to Azure using the following command and select the right subscription for \n",
      "use:\n",
      "az login\n",
      "3. If the login has access to multiple subscriptions, select the appropriate \n",
      "subscription using the following command:\n",
      "az account set –subscription xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n",
      "4. Execute the deployment using the following command. The deployment script is \n",
      "available in the multipleResourceGroupsCLI.txt file:\n",
      "C:\\Users\\Ritesh>az deployment create—location westus—template-file \"C:\\\n",
      "users\\rites\\source\\repos\\CrossSubscription\\CrossSubscription\\azuredeploy.\n",
      "json—parameters @\"C:\\users\\rites\\source\\repos\\CrossSubscription\\\n",
      "CrossSubscription\\azuredeploy.parameters.json\"—verbose\n",
      "Once the command is executed, the resources defined within the ARM template should \n",
      "be reflected on the Azure portal.\n",
      "Deploying resources across subscriptions and resource groups\n",
      "---------------\n",
      "In the last section, resource groups were created as part of ARM templates. Another \n",
      "feature in Azure is the provision of resources into multiple subscriptions simultaneously \n",
      "from a single deployment using a single ARM template. In this section, we will provide \n",
      "a new storage account to two different subscriptions and resource groups. The \n",
      "person deploying the ARM template would select one of the subscriptions as the base \n",
      "subscription, using which they would initiate the deployment and provision the storage \n",
      "account into the current and another subscription. The prerequisite for deploying this \n",
      "template is that the person doing the deployment should have access to at least two \n",
      "subscriptions and that they have contributor rights on these subscriptions. The code \n",
      "listing is shown here and is available in the CrossSubscriptionStorageAccount.json file \n",
      "within the accompanying code:\n",
      "---------------\n",
      "518 | Cross-subscription deployments using ARM templates\n",
      "{ \n",
      "    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/\n",
      "deploymentTemplate.json#\", \n",
      "    \"contentVersion\": \"1.0.0.0\", \n",
      "    \"parameters\": { \n",
      "      \"storagePrefix1\": { \n",
      "        \"type\": \"string\", \n",
      "        \"defaultValue\": \"st01\" \n",
      "    ... \n",
      "        \"type\": \"string\", \n",
      "        \"defaultValue\": \"rg01\" \n",
      "      }, \n",
      "      \"remoteSub\": { \n",
      "        \"type\": \"string\", \n",
      "        \"defaultValue\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" \n",
      "      } \n",
      "   ... \n",
      "                            } \n",
      "                        } \n",
      "                    ], \n",
      "                    \"outputs\": {} \n",
      "                } \n",
      "            } \n",
      "        } \n",
      "    ], \n",
      "    \"outputs\": {} \n",
      "}\n",
      "It is important to note that the names of the resource group used within the code \n",
      "should already be available in the respective subscriptions. The code will throw an error \n",
      "if the resource groups are not available. Moreover, the names of the resource group\n",
      "---------------\n",
      "should precisely match those in the ARM template.\n",
      "The code for deploying this template is shown next. In this case, we use \n",
      "New-AzResourceGroupDeployment, because the scope of the deployment is a resource \n",
      "group. The deployment script is available in the CrossSubscriptionStorageAccount.ps1 \n",
      "file within the code bundle:\n",
      "New-AzResourceGroupDeployment  -TemplateFile \"<< path to your \n",
      "CrossSubscriptionStorageAccount.json file >>\" -ResourceGroupName \"<<provide \n",
      "your base subscription resource group name>>\" -storagePrefix1 <<provide prefix \n",
      "for first storage account>> -storagePrefix2 <<provide prefix for first storage \n",
      "account>> -verbose\n",
      "---------------\n",
      "Deploying resources across subscriptions and resource groups | 519\n",
      "Once the command is executed, the resources defined within the ARM template should \n",
      "be reflected in the Azure portal.\n",
      "Another example of cross-subscription and resource group deployments\n",
      "In this section, we create two storage accounts in two different subscriptions, resource \n",
      "groups, and regions from one ARM template and a single deployment. We will use the \n",
      "nested templates approach along with the copy element to provide different names and \n",
      "locations to these resource groups in different subscriptions. \n",
      "However, before we can execute the next set of ARM templates, an Azure Key Vault \n",
      "instance should be provisioned as a prerequisite and a secret should be added to it. This \n",
      "is because the names of the storage accounts are retrieved from Azure Key Vault and \n",
      "passed as parameters to ARM templates to provision the storage account.\n",
      "To provision Azure Key Vault using Azure PowerShell, the next set of commands\n",
      "---------------\n",
      "can be executed. The code for the following commands is available in the \n",
      "CreateKeyVaultandSetSecret.ps1 file:\n",
      "New-AzResourceGroup -Location <<replace with location of your key vault>> \n",
      "-Name <<replace with name of your resource group for key vault>> -verbose \n",
      "New-AzureRmKeyVault -Name <<replace with name of your key vault>> \n",
      "-ResourceGroupName <<replace with name of your resource group for \n",
      "key vault>>  -Location <<replace with location of your key vault>> \n",
      "-EnabledForDeployment -EnabledForTemplateDeployment -EnabledForDiskEncryption \n",
      "-EnableSoftDelete -EnablePurgeProtection -Sku Standard -Verbose \n",
      "You should note that the ResourceID value should be noted from the result of the \n",
      "New-AzKeyVault cmdlet. This value will need to be replaced in the parameters file. See \n",
      "Figure 15.4 for details:\n",
      "Figure 15.4: Creating a Key Vault instance\n",
      "---------------\n",
      "520 | Cross-subscription deployments using ARM templates\n",
      "Execute the following command to add a new secret to the newly created Azure Key \n",
      "Vault instance:\n",
      "Set-AzKeyVaultSecret -VaultName <<replace with name of your key vault>> -Name \n",
      "<<replace with name of yoursecret>> -SecretValue $(ConvertTo-SecureString \n",
      "-String <<replace with value of your secret>> -AsPlainText -Force ) -Verbose\n",
      "The code listing is available in the CrossSubscriptionNestedStorageAccount.json file \n",
      "within the code bundle:\n",
      "{ \n",
      "  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/\n",
      "deploymentTemplate.json#\", \n",
      "  \"contentVersion\": \"1.0.0.0\", \n",
      "  \"parameters\": { \n",
      "    \"hostingPlanNames\": { \n",
      "      \"type\": \"array\", \n",
      "      \"minLength\": 1 \n",
      "    }, \n",
      "  ... \n",
      "      \"type\": \"Microsoft.Resources/deployments\", \n",
      "      \"name\": \"deployment01\", \n",
      "      \"apiVersion\": \"2019-10-01\", \n",
      "      \"subscriptionId\": \"[parameters('subscriptions')[copyIndex()]]\", \n",
      "      \"resourceGroup\": \"[parameters('resourceGroups')[copyIndex()]]\",\n",
      "---------------\n",
      "\"copy\": { \n",
      "        \"count\": \"[length(parameters('hostingPlanNames'))]\", \n",
      "        \"name\": \"mywebsites\",        \"mode\": \"Parallel\" \n",
      "      }, \n",
      "    ... \n",
      "              \"kind\": \"Storage\", \n",
      "              \"properties\": { \n",
      "              } \n",
      "            } \n",
      "          ] \n",
      "...\n",
      "---------------\n",
      "Deploying resources across subscriptions and resource groups | 521\n",
      "Here's the code for the parameters file. It is available in the \n",
      "CrossSubscriptionNestedStorageAccount.parameters.json file:\n",
      "{ \n",
      "  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/\n",
      "deploymentParameters.json#\", \n",
      "  \"contentVersion\": \"1.0.0.0\", \n",
      "  \"parameters\": { \n",
      "    \"hostingPlanNames\": { \n",
      "   ... \n",
      "    \"storageKey\": { \n",
      "      \"reference\": { \n",
      "        \"keyVault\": { \"id\": \"<<replace it with the value of Key vault \n",
      "ResourceId noted before>>\" }, \n",
      "        \"secretName\": \"<<replace with the name of the secret available in \n",
      "Key vault>>\" \n",
      "      } \n",
      "    } \n",
      "  } \n",
      "}\n",
      "Here's the PowerShell code for deploying the previous template. The deployment script \n",
      "is available in the CrossSubscriptionNestedStorageAccount.ps1 file:\n",
      "New-AzResourceGroupDeployment  -TemplateFile \"c:\\users\\rites\\source\\repos\\\n",
      "CrossSubscription\\CrossSubscription\\CrossSubscriptionNestedStorageAccount.\n",
      "json\" -ResourceGroupName rg01 -TemplateParameterFile \"c:\\\n",
      "---------------\n",
      "users\\rites\\source\\repos\\CrossSubscription\\CrossSubscription\\\n",
      "CrossSubscriptionNestedStorageAccount.parameters.json\" -Verbose\n",
      "Once the command gets executed, the resources defined within the ARM template \n",
      "should be reflected in the Azure portal.\n",
      "---------------\n",
      "522 | Cross-subscription deployments using ARM templates\n",
      "Deploying cross-subscription and resource group deployments \n",
      "using linked templates\n",
      "The previous example used nested templates to deploy to multiple subscriptions \n",
      "and resource groups. In the next example, we will deploy multiple App Service plans \n",
      "in separate subscriptions and resource groups using linked templates. The linked \n",
      "templates are stored in Azure Blob storage, which is protected using policies. This \n",
      "means that only the holder of the storage account key or a valid shared access signature \n",
      "can access this template. The access key is stored in Azure Key Vault and is accessed \n",
      "from the parameters file using references under the storageKey element. You should \n",
      "upload the website.json file to a container in Azure Blob storage. The website.json file \n",
      "is a linked template responsible for provisioning an App Service plan and an app service.\n",
      "---------------\n",
      "The file is protected using the Private (no anonymous access) policy, as shown in Figure \n",
      "15.5. A privacy policy ensures that anonymous access is not allowed. For this instance, \n",
      "we have created a container named armtemplates and set it with a private policy:\n",
      "Figure 15.5: Setting a private policy for the container\n",
      "This file can only be accessed using the Shared Access Signature (SAS) keys. The SAS \n",
      "keys can be generated from the Azure portal for a storage account using the Shared \n",
      "access signature item in the left menu shown in Figure 15.6. You should click on the \n",
      "Generate SAS and connection string button to generate the SAS token. It is to be \n",
      "noted that an SAS token is displayed once and not stored within Azure. So, copy it and \n",
      "store it somewhere so that it can be uploaded to Azure Key Vault. Figure 15.6 shows the \n",
      "generation of the SAS token:\n",
      "---------------\n",
      "Deploying cross-subscription and resource group deployments using linked templates | 523\n",
      "Figure 15.6: Generating an SAS token in the Azure portal\n",
      "We will use the same Key Vault instance that was created in the previous section. We \n",
      "just have to ensure that there are two secrets available within the Key Vault instance. \n",
      "The first secret is StorageName and the other one is StorageKey. The commands to create \n",
      "these secrets in the Key Vault instance are as follows:\n",
      "Set-AzKeyVaultSecret -VaultName \"testkeyvaultbook\" -Name \"storageName\" \n",
      "-SecretValue $(ConvertTo-SecureString -String \"uniquename\" -AsPlainText \n",
      "-Force ) -Verbose \n",
      " \n",
      "Set-AzKeyVaultSecret -VaultName \"testkeyvaultbook\" -Name \"storageKey\" \n",
      "-SecretValue $(ConvertTo-SecureString -String \"?sv=2020-03-28&ss=bfqt&srt=sc\n",
      "o&sp=rwdlacup&se=2020-03-30T21:51:03Z&st=2020-03-30T14:51:03Z&spr=https&sig=\n",
      "gTynGhj20er6pDl7Ab%2Bpc29WO3%2BJhvi%2BfF%2F6rHYWp4g%3D\" -AsPlainText -Force \n",
      ") -Verbose\n",
      "---------------\n",
      "524 | Cross-subscription deployments using ARM templates\n",
      "You are advised to change the names of the Key Vault instance and the secret key value \n",
      "based on your storage account.\n",
      "After ensuring that the Key Vault instance has the necessary secrets, the ARM template \n",
      "file code can be used to deploy the nested templates across subscriptions and resource \n",
      "groups.\n",
      "The ARM template code is available in the CrossSubscriptionLinkedStorageAccount.\n",
      "json file and is also shown here. You are advised to change the value of the templateUrl \n",
      "variable within this file. It should be updated with a valid Azure Blob storage file \n",
      "location:\n",
      "{ \n",
      "  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/\n",
      "deploymentTemplate.json#\", \n",
      "  \"contentVersion\": \"1.0.0.0\", \n",
      "  \"parameters\": { \n",
      "    \"hostingPlanNames\": { \n",
      "      \"type\": \"array\", \n",
      "      \"minLength\": 1 \n",
      "  ... \n",
      "      \"type\": \"Microsoft.Resources/deployments\", \n",
      "      \"name\": \"fsdfsdf\", \n",
      "      \"apiVersion\": \"2019-10-01\",\n",
      "---------------\n",
      "\"subscriptionId\": \"[parameters('subscriptions')[copyIndex()]]\", \n",
      "      \"resourceGroup\": \"[parameters('resourceGroups')[copyIndex()]]\", \n",
      "      \"copy\": { \n",
      "        \"count\": \"[length(parameters('hostingPlanNames'))]\", \n",
      "        \"name\": \"mywebsites\", \n",
      "        \"mode\": \"Parallel\" \n",
      "    ... \n",
      "  ] \n",
      "}\n",
      "The code for the parameters file is shown next. You are advised to change the values of \n",
      "the parameters, including the resourceid of the Key Vault instance and the secret name. \n",
      "The names of app services should be unique, or the template will fail to deploy. The \n",
      "code for the parameters file is available in the CrossSubscriptionLinkedStorageAccount.\n",
      "parameters.json code file:\n",
      "---------------\n",
      "Deploying cross-subscription and resource group deployments using linked templates | 525\n",
      "{ \n",
      "  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/\n",
      "deploymentParameters.json#\", \n",
      "  \"contentVersion\": \"1.0.0.0\", \n",
      "  \"parameters\": { \n",
      "    \"hostingPlanNames\": { \n",
      "      \"value\": [ \"firstappservice\", \"secondappservice\" ] \n",
      "  ... \n",
      "    \"storageKey\": { \n",
      "      \"reference\": { \n",
      "        \"keyVault\": { \"id\": \"/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-\n",
      "xxxxxxxxxxxx/resourceGroups/keyvaluedemo/providers/Microsoft.KeyVault/\n",
      "vaults/forsqlvault1\" }, \n",
      "        \"secretName\": \"storageKey\" \n",
      "      } \n",
      "    } \n",
      "  } \n",
      "}\n",
      "Here's the command to deploy the template. The deployment script is available in the \n",
      "CrossSubscriptionLinkedStorageAccount.ps1 file:\n",
      "New-AzureRmResourceGroupDeployment  -TemplateFile \"c:\\users\\\n",
      "rites\\source\\repos\\CrossSubscription\\CrossSubscription\\\n",
      "CrossSubscriptionLinkedStorageAccount.json\" -ResourceGroupName <<replace \n",
      "with the base subscription resource group name >> -TemplateParameterFile\n",
      "---------------\n",
      "\"c:\\users\\rites\\source\\repos\\CrossSubscription\\CrossSubscription\\\n",
      "CrossSubscriptionLinkedStorageAccount.parameters.json\" -Verbose\n",
      "Once the command gets executed, the resources defined within the ARM template \n",
      "should be reflected in the Azure portal.\n",
      "Now that you know how to provision resources across resource groups and \n",
      "subscriptions, we will look at some of the solutions that can be created using ARM \n",
      "templates.\n",
      "---------------\n",
      "526 | Cross-subscription deployments using ARM templates\n",
      "Virtual machine solutions using ARM templates\n",
      "Infrastructure as a service (IaaS) resources and solutions can be deployed and \n",
      "configured using ARM templates. The major resources related to IaaS are virtual \n",
      "machine resources. \n",
      "Creating a virtual machine resource is dependent on multiple other resources in Azure. \n",
      "Some of the resources that are needed to create a virtual machine include: \n",
      "• A storage account or a managed disk for hosting the operating system and \n",
      "data disk\n",
      "• A virtual network along with subnets\n",
      "• A network interface card\n",
      "There are other resources that are optional, including: \n",
      "• Azure Load Balancer\n",
      "• Network security groups\n",
      "• Public IP address\n",
      "• Route tables and more\n",
      "This section will deal with the process of creating virtual machines using ARM \n",
      "templates. As mentioned before in this section, we need to create a few resources, upon\n",
      "---------------\n",
      "which the virtual machine resource will depend, prior to creating the virtual machine \n",
      "resource itself.\n",
      "It is important to note that it is not always necessary to create the dependent \n",
      "resources. They should be created only if they do not exist already. If they already \n",
      "are available within the Azure subscription, the virtual machine resource can be \n",
      "provisioned by referencing those dependent resources.\n",
      "The template is dependent on a few parameters that should be supplied to it at the time \n",
      "of executing the template. These variables relate to the location of the resources and \n",
      "some of their configuration values. These values are taken from parameters because \n",
      "they might change from one deployment to another, so using parameters helps keep \n",
      "the template generic.\n",
      "---------------\n",
      "Virtual machine solutions using ARM templates | 527\n",
      "The first step is to create a storage account, as shown in the following code:\n",
      "{\n",
      "      \"type\": \"Microsoft.Storage/storageAccounts\", \n",
      "      \"name\": \"[variables('storageAccountName')]\", \n",
      "      \"apiVersion\": \"2019-04-01\", \n",
      "      \"location\": \"[parameters('location')]\", \n",
      "      \"sku\": { \n",
      "        \"name\": \"Standard_LRS\" \n",
      "      }, \n",
      "      \"kind\": \"Storage\", \n",
      "      \"properties\": {} \n",
      "      },\n",
      "After creating a storage account, a virtual network should be defined within the ARM \n",
      "template. It is important to note that there is no dependency between a storage \n",
      "account and a virtual network. They can be created in parallel. The virtual network \n",
      "resource has a subnet as its sub-resource. These are both configured with their IP \n",
      "ranges; the subnet typically has a smaller range than the virtual network IP range:\n",
      "    {\n",
      "      \"apiVersion\": \"2019-09-01\",\n",
      "      \"type\": \"Microsoft.Network/virtualNetworks\",\n",
      "      \"name\": \"[variables('virtualNetworkName')]\",\n",
      "---------------\n",
      "\"location\": \"[parameters('location')]\",\n",
      "      \"properties\": {\n",
      "        \"addressSpace\": {\n",
      "          \"addressPrefixes\": [\n",
      "            \"[variables('addressPrefix')]\"\n",
      "          ]\n",
      "        },\n",
      "        \"subnets\": [\n",
      "          {\n",
      "            \"name\": \"[variables('subnetName')]\",\n",
      "            \"properties\": {\n",
      "              \"addressPrefix\": \"[variables('subnetPrefix')]\"\n",
      "            }\n",
      "---------------\n",
      "528 | Cross-subscription deployments using ARM templates\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "If the virtual machine needs to be accessed over the public internet, a public IP \n",
      "address can also be created, as shown in the following code. Again, it is a completely \n",
      "independent resource and can be created in parallel with the storage account and \n",
      "virtual network:\n",
      "    {\n",
      "      \"apiVersion\": \"2019-11-01\",\n",
      "      \"type\": \"Microsoft.Network/publicIPAddresses\",\n",
      "      \"name\": \"[variables('publicIPAddressName')]\",\n",
      "      \"location\": \"[parameters('location')]\",\n",
      "      \"properties\": {\n",
      "        \"publicIPAllocationMethod\": \"Dynamic\",\n",
      "        \"dnsSettings\": {\n",
      "          \"domainNameLabel\": \"[parameters('dnsLabelPrefix')]\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "After creating the virtual network, storage account, and public IP address, a network \n",
      "interface can be created. A network interface is dependent on a virtual network and \n",
      "subnet resource. It can optionally also be associated with a public IP address as well.\n",
      "---------------\n",
      "This is shown in the following code:\n",
      "{\n",
      "      \"apiVersion\": \"2019-11-01\",\n",
      "      \"type\": \"Microsoft.Network/networkInterfaces\",\n",
      "      \"name\": \"[variables('nicName')]\",\n",
      "      \"location\": \"[parameters('location')]\",\n",
      "      \"dependsOn\": [\n",
      "---------------\n",
      "Virtual machine solutions using ARM templates | 529\n",
      "        \"[resourceId('Microsoft.Network/publicIPAddresses/', \n",
      "variables('publicIPAddressName'))]\",\n",
      "        \"[resourceId('Microsoft.Network/virtualNetworks/', \n",
      "variables('virtualNetworkName'))]\"\n",
      "      ],\n",
      "      \"properties\": {\n",
      "        \"ipConfigurations\": [\n",
      "          {\n",
      "            \"name\": \"ipconfig1\",\n",
      "            \"properties\": {\n",
      "              \"privateIPAllocationMethod\": \"Dynamic\",\n",
      "              \"publicIPAddress\": {\n",
      "\"id\": \"[resourceId('Microsoft.Network/\n",
      "publicIPAddresses',variables('publicIPAddressName'))]\"\n",
      "              },\n",
      "\"subnet\": {\n",
      "\"id\": \"[variables('subnetRef')]\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "It is important to note that both the public IP address and the subnet are referred to by \n",
      "their unique Azure identifiers.\n",
      "After the creation of the network interface, we have all the resources that are needed \n",
      "to create a virtual machine. The next code block shows how to create a virtual machine\n",
      "---------------\n",
      "using an ARM template. It has a dependency on the network card and storage account. \n",
      "This indirectly creates dependencies on the virtual network, subnet, and the public IP \n",
      "address.\n",
      "---------------\n",
      "530 | Cross-subscription deployments using ARM templates\n",
      "For the virtual machine, we configure the mandatory resource configuration, lincluding \n",
      "type, apiVersion, location, and name, along with any dependencies, as shown in the \n",
      "following code:\n",
      "{\n",
      "\"apiVersion\": \"2019-07-01\",\n",
      "\"type\": \"Microsoft.Compute/virtualMachines\",\n",
      "\"name\": \"[variables('vmName')]\",\n",
      "\"location\": \"[resourceGroup().location]\",\n",
      "\"tags\": {\n",
      "\"displayName\": \"VirtualMachine\"\n",
      "      },\n",
      "\"dependsOn\": [\n",
      "\"[concat('Microsoft.Storage/storageAccounts/', \n",
      "variables('storageAccountName'))]\",\n",
      "\"[concat('Microsoft.Network/networkInterfaces/', variables('nicName'))]\"\n",
      "      ],\n",
      "\"properties\": {\n",
      "\"hardwareProfile\": { \"vmSize\": \"[variables('vmSize')]\" },\n",
      "\"availabilitySet\": {\n",
      "\"id\": \"[resourceId('Microsoft.Compute/availabilitySets', \n",
      "parameters('adAvailabilitySetName'))]\"\n",
      "        },\n",
      "\"osProfile\": {\n",
      "\"computerName\": \"[variables('vmName')]\",\n",
      "\"adminUsername\": \"[parameters('adminUsername')]\",\n",
      "\"adminPassword\": \"[parameters('adminPassword')]\"\n",
      "        },\n",
      "---------------\n",
      "\"storageProfile\": {\n",
      "\"imageReference\": {\n",
      "---------------\n",
      "Virtual machine solutions using ARM templates | 531\n",
      "\"publisher\": \"[variables('imagePublisher')]\",\n",
      "\"offer\": \"[variables('imageOffer')]\",\n",
      "\"sku\": \"[parameters('windowsOSVersion')]\",\n",
      "\"version\": \"latest\"\n",
      "          },\n",
      "\"osDisk\": { \"createOption\": \"FromImage\" },\n",
      "\"copy\": [\n",
      "            {\n",
      "\"name\": \"dataDisks\",\n",
      "\"count\": 3,\n",
      "\"input\": {\n",
      "\"lun\": \"[copyIndex('dataDisks')]\",\n",
      "\"createOption\": \"Empty\",\n",
      "\"diskSizeGB\": \"1023\",\n",
      "\"name\": \"[concat(variables('vmName'), '-datadisk', copyIndex('dataDisks'))]\"\n",
      "              }\n",
      "            }\n",
      "          ]\n",
      "        },\n",
      "\"networkProfile\": {\n",
      "\"networkInterfaces\": [\n",
      "            {\n",
      "\"id\": \"[resourceId('Microsoft.Network/networkInterfaces', \n",
      "variables('nicName'))]\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "---------------\n",
      "532 | Cross-subscription deployments using ARM templates\n",
      "In the preceding code, the virtual machine is configured with:\n",
      "• A hardware profile—the size of the virtual machine.\n",
      "• An OS profile—the name and credentials for logging in to the virtual machine.\n",
      "• A storage profile—the storage account on which to store the Virtual Hard Disk \n",
      "(VHD) file for the virtual machine, including data disks.\n",
      "• A network profile—the reference to the network interface card.\n",
      "The next section will show an example of using ARM templates to provision a Platform \n",
      "as a Service solution.\n",
      "PaaS solutions using ARM templates\n",
      "Platform as a service (PaaS) resources and solutions can be deployed using ARM \n",
      "templates. One of the main resources related to PaaS is Azure Web Apps, and in this \n",
      "section, we will focus on creating web apps on Azure using ARM templates.\n",
      "The template expects a few parameters to be supplied while executing it. The\n",
      "---------------\n",
      "parameters needed are the SKU for the App Service plan, the Azure region hosting the \n",
      "resources, and the SKU capacity of the App Service plan.\n",
      "There are a couple of variables declared within the template to make it generic and \n",
      "maintainable. The first one, hostingPlanName, is for the App Service plan name, and the \n",
      "next one, webSiteName, is for the app service itself. \n",
      "There are at minimum two resources that should be declared and provisioned for a \n",
      "working web app in Azure. They are the following:\n",
      "• The Azure App Service plan\n",
      "• Azure App Service\n",
      "The first step in creating a web app on Azure is defining the configuration for an Azure \n",
      "App Service plan. The following code defines a new App Service plan. It is important to \n",
      "note that the resource type is Microsoft.Web/serverfarms. Most of the configuration \n",
      "values of the plan, such as location, name, and capacity, come as parameters to the ARM \n",
      "template:\n",
      "---------------\n",
      "PaaS solutions using ARM templates | 533\n",
      "  {\n",
      "\"apiVersion\": \"2019-08-01\",\n",
      "\"name\": \"[variables('hostingPlanName')]\",\n",
      "\"type\": \"Microsoft.Web/serverfarms\",\n",
      "\"location\": \"[parameters('location')]\",\n",
      "\"tags\": {\n",
      "\"displayName\": \"HostingPlan\"\n",
      "      },\n",
      "\"sku\": {\n",
      "\"name\": \"[parameters('skuName')]\",\n",
      "\"capacity\": \"[parameters('skuCapacity')]\"\n",
      "      },\n",
      "\"properties\": {\n",
      "\"name\": \"[variables('hostingPlanName')]\"\n",
      "      }\n",
      "    },\n",
      "The next resource that should be provisioned after a plan is the app service itself. It is \n",
      "important that a dependency between both these resources is created such that a plan \n",
      "is already created before the app service itself is created:\n",
      "  {\n",
      "\"apiVersion\": \"2019-08-01\",\n",
      "\"name\": \"[variables('webSiteName')]\",\n",
      "\"type\": \"Microsoft.Web/sites\",\n",
      "\"location\": \"[parameters('location')]\",\n",
      "\"dependsOn\": [\n",
      "\"[variables('hostingPlanName')]\"\n",
      "      ],\n",
      "\"properties\": {\n",
      "\"name\": \"[variables('webSiteName')]\",\n",
      "\"serverFarmId\": \"[resourceId('Microsoft.Web/serverfarms', \n",
      "variables('hostingPlanName'))]\"\n",
      "      },\n",
      "---------------\n",
      "\"resources\": [\n",
      "---------------\n",
      "534 | Cross-subscription deployments using ARM templates\n",
      "        {\n",
      "\"apiVersion\": \"2019-08-01\",\n",
      "\"type\": \"config\",\n",
      "\"name\": \"connectionstrings\",\n",
      "\"dependsOn\": [\n",
      "\"[variables('webSiteName')]\"\n",
      "          ],\n",
      "\"properties\": {\n",
      "\"DefaultConnection\": {\n",
      "\"value\": \"[concat( 'sql connection string here')]\",\n",
      "\"type\": \"SQLAzure\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "In the preceding code, a resource of type Microsoft.Web/sites is defined and it has a \n",
      "dependency on the plan. It is also using the App Service plan and is associated with \n",
      "it using serverFarmId. It further declares a connection string that can be used for \n",
      "connecting to SQL Server.\n",
      "This section showed an example of creating a PaaS solution on Azure using an ARM \n",
      "template. Similarly, other PaaS solutions, including Azure Function apps, Kubernetes \n",
      "Service, and Service Fabric, among many others, can be created using ARM templates.\n",
      "Data-related solutions using ARM templates\n",
      "---------------\n",
      "There are many resources in Azure related to data management and storage. Some of \n",
      "the important data-related resources include Azure SQL, Azure Cosmos DB, Azure Data \n",
      "Lake Storage, Data Lake Analytics, Azure Synapsis, Databricks, and Data Factory.\n",
      "All of these resources can be provisioned and configured using an ARM template. In \n",
      "this section, we will create an ARM template to provision a Data Factory resource \n",
      "responsible for migrating data from Azure Blob storage to Azure SQL Database using \n",
      "stored procedures. \n",
      "You will find the parameters file along with the template. These values might change \n",
      "from one deployment to another; we'll keep the template generic so that you can \n",
      "customize and use it easily with other deployments as well.\n",
      "---------------\n",
      "Data-related solutions using ARM templates | 535\n",
      "The entire code for this section can be found at https:/ /github.com/Azure/azure-\n",
      "quickstart-templates/blob/master/101-data-factory-blob-to-sql-copy-stored-proc.\n",
      "The first step is to declare the configuration for the data factory in the ARM template, \n",
      "as shown in the following code: \n",
      "\"name\": \"[variables('dataFactoryName')]\",\n",
      "\"apiVersion\": \"2018-06-01\",\n",
      "\"type\": \"Microsoft.DataFactory/datafactories\",\n",
      "\"location\": \"[parameters('location')]\",\n",
      "Each data factory has multiple linked services. These linked services act as connectors \n",
      "to get data into the data factory, or the data factory can send data to them. The \n",
      "following code listing creates a linked service for the Azure storage account from which \n",
      "the blobs will be read into the data factory, and another linked service for Azure SQL \n",
      "Database:\n",
      "  {\n",
      "\"type\": \"linkedservices\",\n",
      "\"name\": \"[variables('storageLinkedServiceName')]\",\n",
      "\"apiVersion\": \"2018-06-01\",\n",
      "\"dependsOn\": [\n",
      "---------------\n",
      "\"[variables('dataFactoryName')]\"\n",
      "          ],\n",
      "\"properties\": {\n",
      "\"type\": \"AzureStorage\",\n",
      "\"description\": \"Azure Storage Linked Service\",\n",
      "\"typeProperties\": {\n",
      "\"connectionString\":\n",
      "\"[concat('DefaultEndpointsProtocol=https; \n",
      "AccountName=',parameters('storageAccountName'),'; \n",
      "AccountKey=',parameters('storageAccountKey'))]\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        {\n",
      "\"type\": \"linkedservices\",\n",
      "---------------\n",
      "536 | Cross-subscription deployments using ARM templates\n",
      "\"name\": \"[variables('sqlLinkedServiceName')]\",\n",
      "\"apiVersion\": \"2018-06-01\",\n",
      "\"dependsOn\": [\n",
      "\"[variables('dataFactoryName')]\"\n",
      "          ],\n",
      "\"properties\": {\n",
      "\"type\": \"AzureSqlDatabase\",\n",
      "\"description\": \"Azure SQL linked service\",\n",
      "\"typeProperties\": {\n",
      "\"connectionString\": \"[concat('Data Source=tcp:', parameters('sqlServerName'), \n",
      "'.database.windows.net,1433;Initial Catalog=', parameters('sqlDatabaseName'), \n",
      "';Integrated Security=False;User ID=', parameters('sqlUserId'), ';Password=', \n",
      "parameters('sqlPassword'), ';Connect Timeout=30;Encrypt=True')]\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "After linked services, it's time to define the datasets for Azure Data Factory. Datasets \n",
      "help in identifying the data that should be read and placed in the data factory. They \n",
      "could also represent the temporary data that needs to be stored by the Data Factory \n",
      "during the transformation, or even the destination location where the data will be\n",
      "---------------\n",
      "written. The next code block creates three datasets—one for each of the aspects of \n",
      "datasets that were just mentioned.\n",
      "The read dataset is shown in the following code block:\n",
      " {\n",
      "\"type\": \"datasets\",\n",
      "\"name\": \"[variables('storageDataset')]\",\n",
      "\"dependsOn\": [\n",
      "\"[variables('dataFactoryName')]\",\n",
      "\"[variables('storageLinkedServiceName')]\"\n",
      "          ],\n",
      "\"apiVersion\": \"2018-06-01\",\n",
      "\"properties\": {\n",
      "\"type\": \"AzureBlob\",\n",
      "\"linkedServiceName\": \"[variables('storageLinkedServiceName')]\",\n",
      "---------------\n",
      "Data-related solutions using ARM templates | 537\n",
      "\"typeProperties\": {\n",
      "\"folderPath\": \"[concat(parameters('sourceBlobContainer'), '/')]\",\n",
      "\"fileName\": \"[parameters('sourceBlobName')]\",\n",
      "\"format\": {\n",
      "\"type\": \"TextFormat\"\n",
      "              }\n",
      "            },\n",
      "\"availability\": {\n",
      "\"frequency\": \"Hour\",\n",
      "\"interval\": 1\n",
      "            },\n",
      "\"external\": true\n",
      "          }\n",
      "        },\n",
      "The intermediate dataset is shown in the following lines of code:\n",
      " {\n",
      "\"type\": \"datasets\",\n",
      "\"name\": \"[variables('intermediateDataset')]\",\n",
      "\"dependsOn\": [\n",
      "\"[variables('dataFactoryName')]\",\n",
      "\"[variables('sqlLinkedServiceName')]\"\n",
      "          ],\n",
      "\"apiVersion\": \"2018-06-01\",\n",
      "\"properties\": {\n",
      "\"type\": \"AzureSqlTable\",\n",
      "\"linkedServiceName\": \"[variables('sqlLinkedServiceName')]\",\n",
      "\"typeProperties\": {\n",
      "\"tableName\": \"[variables('intermediateDataset')]\"\n",
      "            },\n",
      "\"availability\": {\n",
      "\"frequency\": \"Hour\",\n",
      "\"interval\": 1\n",
      "---------------\n",
      "538 | Cross-subscription deployments using ARM templates\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "Finally, the dataset used for the destination is shown here:\n",
      " {\n",
      "\"type\": \"datasets\",\n",
      "\"name\": \"[variables('sqlDataset')]\",\n",
      "\"dependsOn\": [\n",
      "\"[variables('dataFactoryName')]\",\n",
      "\"[variables('sqlLinkedServiceName')]\"\n",
      "          ],\n",
      "\"apiVersion\": \"2018-06-01\",\n",
      "\"properties\": {\n",
      "\"type\": \"AzureSqlTable\",\n",
      "\"linkedServiceName\": \"[variables('sqlLinkedServiceName')]\",\n",
      "\"typeProperties\": {\n",
      "\"tableName\": \"[parameters('sqlTargetTable')]\"\n",
      "            },\n",
      "\"availability\": {\n",
      "\"frequency\": \"Hour\",\n",
      "\"interval\": 1\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "Finally, we need a pipeline in Data Factory that can bring together all the datasets and \n",
      "linked services, and help in creating extract-transform-load data solutions. A pipeline \n",
      "consists of multiple activities, each fulfilling a particular task. All these activities can be \n",
      "defined within the ARM template, as you'll see now. The first activity copies the blobs in\n",
      "---------------\n",
      "the storage account to an intermediate SQL Server, as shown in the following code:\n",
      "{\n",
      "\"type\": \"dataPipelines\",\n",
      "\"name\": \"[variables('pipelineName')]\",\n",
      "\"dependsOn\": [\n",
      "---------------\n",
      "Data-related solutions using ARM templates | 539\n",
      "\"[variables('dataFactoryName')]\",\n",
      "\"[variables('storageLinkedServiceName')]\",\n",
      "\"[variables('sqlLinkedServiceName')]\",\n",
      "\"[variables('storageDataset')]\",\n",
      "\"[variables('sqlDataset')]\"\n",
      "          ],\n",
      "\"apiVersion\": \"2018-06-01\",\n",
      "\"properties\": {\n",
      "\"description\": \"Copies data from Azure Blob to Sql DB while invoking stored \n",
      "procedure\",\n",
      "\"activities\": [\n",
      "              {\n",
      "\"name\": \"BlobtoSqlTableCopyActivity\",\n",
      "\"type\": \"Copy\",\n",
      "\"typeProperties\": {\n",
      "\"source\": {\n",
      "\"type\": \"BlobSource\"\n",
      "                  },\n",
      "\"sink\": {\n",
      "\"type\": \"SqlSink\",\n",
      "\"writeBatchSize\": 0,\n",
      "\"writeBatchTimeout\": \"00:00:00\"\n",
      "                  }\n",
      "                },\n",
      "\"inputs\": [\n",
      "                  {\n",
      "\"name\": \"[variables('storageDataset')]\"\n",
      "                  }\n",
      "                ],\n",
      "\"outputs\": [\n",
      "                  {\n",
      "\"name\": \"[variables('intermediateDataset')]\"\n",
      "---------------\n",
      "540 | Cross-subscription deployments using ARM templates\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "\"name\": \"SqlTabletoSqlDbSprocActivity\",\n",
      "\"type\": \"SqlServerStoredProcedure\",\n",
      "\"inputs\": [\n",
      "                  {\n",
      "\"name\": \"[variables('intermediateDataset')]\"\n",
      "                  }\n",
      "                ],\n",
      "\"outputs\": [\n",
      "                  {\n",
      "\"name\": \"[variables('sqlDataset')]\"\n",
      "                  }\n",
      "                ],\n",
      "\"typeProperties\": {\n",
      "\"storedProcedureName\": \"[parameters('sqlWriterStoredProcedureName')]\"\n",
      "                },\n",
      "\"scheduler\": {\n",
      "\"frequency\": \"Hour\",\n",
      "\"interval\": 1\n",
      "                },\n",
      "\"policy\": {\n",
      "\"timeout\": \"02:00:00\",\n",
      "\"concurrency\": 1,\n",
      "\"executionPriorityOrder\": \"NewestFirst\",\n",
      "\"retry\": 3\n",
      "                }\n",
      "              }\n",
      "            ],\n",
      "---------------\n",
      "Creating an IaaS solution on Azure with Active Directory and DNS | 541\n",
      "\"start\": \"2020-10-01T00:00:00Z\",\n",
      "\"end\": \"2020-10-02T00:00:00Z\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "The last activity copies data from the intermediate dataset to the final destination \n",
      "dataset. \n",
      "There are also start and end times during which the pipeline should be running.\n",
      "This section focused on creating an ARM template for a data-related solution. In the \n",
      "next section, we will deal with ARM templates for creating datacenters on Azure with \n",
      "Active Directory and DNS.\n",
      "Creating an IaaS solution on Azure with Active Directory and DNS\n",
      "Creating an IaaS solution on Azure means creating multiple virtual machines, promoting \n",
      "a virtual machine to be a domain controller, and making other virtual machines join \n",
      "the domain controller as domain-joined nodes. It also means installing a DNS server \n",
      "for name resolution and, optionally, a jump server for accessing these virtual machines \n",
      "securely.\n",
      "---------------\n",
      "The template creates an Active Directory forest on the virtual machines. It creates \n",
      "multiple virtual machines based on the parameters supplied. \n",
      "The template creates:\n",
      "• A couple of availability sets\n",
      "• A virtual network\n",
      "• Network security groups to define the allowed and disallowed ports and \n",
      "IP addresses\n",
      "The template then does the following:\n",
      "• Provisions one or two domains. The root domain is created by default; the child \n",
      "domain is optional\n",
      "• Provisions two domain controllers per domain\n",
      "• Executes the desired state configuration scripts to promote a virtual machine to \n",
      "be a domain controller\n",
      "---------------\n",
      "542 | Cross-subscription deployments using ARM templates\n",
      "We can create multiple virtual machines using the approach discussed in the Virtual \n",
      "machine solutions using ARM templates section. However, these virtual machines should \n",
      "be part of an availability set if they need to be highly available. It is to be noted that \n",
      "availability sets provide 99.95% availability for applications deployed on these virtual \n",
      "machines, while Availability Zones provide 99.99% availability.\n",
      "An availability set can be configured as shown in the following code:\n",
      "  {\n",
      "\"name\": \"[variables('adAvailabilitySetNameRoot')]\",\n",
      "\"type\": \"Microsoft.Compute/availabilitySets\",\n",
      "\"apiVersion\": \"2019-07-01\",\n",
      "\"location\": \"[parameters('location')]\",\n",
      "\"sku\": {\n",
      "\"name\": \"Aligned\"\n",
      "      },\n",
      "\"properties\": {\n",
      "\"PlatformUpdateDomainCount\": 3,\n",
      "\"PlatformFaultDomainCount\": 2\n",
      "      }\n",
      "    },\n",
      "Once the availability set is created, an additional profile should be added to the virtual\n",
      "---------------\n",
      "machine configuration to associate the virtual machine with the availability set, as \n",
      "shown in the following code:\n",
      "\"availabilitySet\" : {\n",
      "\"id\": \"[resourceId('Microsoft.Compute/availabilitySets', \n",
      "parameters('adAvailabilitySetName'))]\"\n",
      "      }\n",
      "You should note that availability sets are mandatory in order to use load balancers with \n",
      "virtual machines.\n",
      "---------------\n",
      "Creating an IaaS solution on Azure with Active Directory and DNS | 543\n",
      "Another change needed in the virtual network configuration is adding DNS information, \n",
      "as shown in the following code:\n",
      "  {\n",
      "\"name\": \"[parameters('virtualNetworkName')]\",\n",
      "\"type\": \"Microsoft.Network/virtualNetworks\",\n",
      "\"location\": \"[parameters('location')]\",\n",
      "\"apiVersion\": \"2019-09-01\",\n",
      "\"properties\": {\n",
      "\"addressSpace\": {\n",
      "\"addressPrefixes\": [\n",
      "\"[parameters('virtualNetworkAddressRange')]\"\n",
      "          ]\n",
      "        },\n",
      "\"dhcpOptions\": {\n",
      "\"dnsServers\": \"[parameters('DNSServerAddress')]\"\n",
      "        },\n",
      "\"subnets\": [\n",
      "          {\n",
      "\"name\": \"[parameters('subnetName')]\",\n",
      "\"properties\": {\n",
      "\"addressPrefix\": \"[parameters('subnetRange')]\"\n",
      "            }\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "Finally, to convert a virtual machine into Active Directory, a PowerShell script or \n",
      "desired state configuration (DSC) script should be executed on the virtual machine. \n",
      "Even for joining other virtual machines to the domain, another set of scripts should be\n",
      "---------------\n",
      "executed on those virtual machines.\n",
      "---------------\n",
      "544 | Cross-subscription deployments using ARM templates\n",
      "Scripts can be executed on the virtual machine using the CustomScriptExtension \n",
      "resource, as shown in the following code:\n",
      " {\n",
      "\"type\": \"Microsoft.Compute/virtualMachines/extensions\",\n",
      "\"name\": \"[concat(parameters('adNextDCVMName'),'/PrepareNextDC')]\",\n",
      "\"apiVersion\": \"2018-06-01\",\n",
      "\"location\": \"[parameters('location')]\",\n",
      "\"properties\": {\n",
      "\"publisher\": \"Microsoft.Powershell\",\n",
      "\"type\": \"DSC\",\n",
      "\"typeHandlerVersion\": \"2.21\",\n",
      "\"autoUpgradeMinorVersion\": true,\n",
      "\"settings\": {\n",
      "\"modulesURL\": \"[parameters('adNextDCConfigurationModulesURL')]\",\n",
      "\"configurationFunction\": \"[parameters('adNextDCConfigurationFunction')]\",\n",
      "\"properties\": {\n",
      "\"domainName\": \"[parameters('domainName')]\",\n",
      "\"DNSServer\": \"[parameters('DNSServer')]\",\n",
      "\"DNSForwarder\": \"[parameters('DNSServer')]\",\n",
      "\"adminCreds\": {\n",
      "\"userName\": \"[parameters('adminUserName')]\",\n",
      "\"password\": \"privateSettingsRef:adminPassword\"\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "\"protectedSettings\": {\n",
      "\"items\": {\n",
      "---------------\n",
      "\"adminPassword\": \"[parameters('adminPassword')]\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "---------------\n",
      "Summary | 545\n",
      "In this section, we created a datacenter on Azure using the IaaS paradigm. We created \n",
      "multiple virtual machines and converted one of them into domain controller, installed \n",
      "DNS, and assigned a domain to it. Now, other virtual machines on the network can be \n",
      "joined to this domain and they can form a complete datacenter on Azure.\n",
      "Please refer to https:/ /github.com/Azure/azure-quickstart-templates/tree/\n",
      "master/301-create-ad-forest-with-subdomain for the complete code listing for \n",
      "creating a datacenter on Azure.\n",
      "Summary\n",
      "The option to deploy resources using a single deployment to multiple subscriptions, \n",
      "resource groups, and regions provides enhanced abilities to deploy, reduce bugs in \n",
      "deployment, and access advanced benefits, such as creating disaster recovery sites and \n",
      "achieving high availability. \n",
      "In this chapter, you saw how to create a few different kinds of solution using ARM \n",
      "templates. This included creating an infrastructure-based solution comprising virtual\n",
      "---------------\n",
      "machines; a platform-based solution using Azure App Service; a data-related solution \n",
      "using the Data Factory resource (including its configuration); and a datacenter on Azure \n",
      "with virtual machines, Active Directory, and DNS installed on top of the virtual machine.\n",
      "In the next chapter, we will focus on creating modular ARM templates, an essential \n",
      "skill for architects who really want to take their ARM templates to the next level. The \n",
      "chapter will also show you various ways to design ARM templates and create reusable \n",
      "and modular ARM templates.\n",
      "---------------\n",
      "We know that there are multiple ways to author an Azure Resource Manager (ARM) \n",
      "template. It is quite easy to author one that provisions all of the necessary resources in \n",
      "Azure using Visual Studio and Visual Studio Code. A single ARM template can consist of \n",
      "all the required resources for a solution on Azure. This single ARM template could be as \n",
      "small as a few resources, or it could be a larger one consisting of many resources.\n",
      "While authoring a single template consisting of all resources is quite tempting, it is \n",
      "advisable to plan an ARM template implementation divided into multiple smaller ARM \n",
      "templates beforehand, so that future troubles related to them can be avoided.\n",
      "In this chapter, we will look at how to write ARM templates in a modular way so that \n",
      "they can evolve over a period of time with minimal involvement in terms of changes and \n",
      "effort in testing and deployment.\n",
      "However, before writing modular templates, it is best to understand the problems\n",
      "---------------\n",
      "solved by writing them in a modular fashion.\n",
      "ARM template \n",
      "modular design and \n",
      "implementation\n",
      "16\n",
      "---------------\n",
      "548 | ARM template modular design and implementation\n",
      "The following topics will be covered in this chapter:\n",
      "• Problems with a single template\n",
      "• Understanding nested and linked deployment\n",
      "• Linked templates\n",
      "• Nested templates\n",
      "• Free-flow configurations\n",
      "• Known configurations\n",
      "Now, let's explore the aforementioned topics in detail, which will help you to write \n",
      "modular templates using industry best practices.\n",
      "Problems with the single template approach\n",
      "On the surface, it might not sound like a single large template consisting of all \n",
      "resources will have problems, but there are issues that could arise in the future. Let's \n",
      "discuss the issues that might arise when using single large templates.\n",
      "Reduced flexibility in changing templates\n",
      "Using a single large template with all resources makes it difficult to change it in the \n",
      "future. With all dependencies, parameters, and variables in a single template, changing \n",
      "the template can take a considerable amount of time compared to smaller templates.\n",
      "---------------\n",
      "The change could have an impact on other sections of the template, which might go \n",
      "unnoticed, as well as introducing bugs.\n",
      "Troubleshooting large templates\n",
      "Large templates are difficult to troubleshoot. This is a known fact. The larger the \n",
      "number of resources in a template, the more difficult it is to troubleshoot the template. \n",
      "A template deploys all the resources in it, and finding a bug involves deploying the \n",
      "template quite often. Developers would have reduced productivity while waiting for the \n",
      "completion of template deployment.\n",
      "Also, deploying a single template is more time-consuming than deploying smaller \n",
      "templates. Developers have to wait for resources containing errors to be deployed \n",
      "before taking any action.\n",
      "---------------\n",
      "Problems with the single template approach | 549\n",
      "Dependency abuse\n",
      "The dependencies between resources also tend to become more complex in larger \n",
      "templates. It is quite easy to abuse the usage of the dependsOn feature in ARM templates \n",
      "because of the way they work. Every resource in a template can refer to all its prior \n",
      "resources rather than building a tree of dependencies. ARM templates do not complain \n",
      "if a single resource is dependent on all other resources in the ARM template, even \n",
      "though those other resources might have inter-dependencies within themselves. This \n",
      "makes changing ARM templates bug prone and, at times, it is not even possible to \n",
      "change them.\n",
      "Reduced agility\n",
      "Generally, there are multiple teams in a project, with each owning their own resources \n",
      "in Azure. These teams will find it difficult to work with a single ARM template because \n",
      "a single developer should be updating them. Updating a single template with multiple\n",
      "---------------\n",
      "teams might induce conflict and difficult-to-solve merges. Having multiple smaller \n",
      "templates can enable each team to author their own piece of an ARM template.\n",
      "No reusability\n",
      "If you have a single template, then that's all that you have, and using this template \n",
      "means deploying all resources. There is no possibility, out of the box, to select individual \n",
      "resources without some maneuvering, such as adding conditional resources. A single \n",
      "large template loses reusability because you take all the resources or none of them.\n",
      "Knowing that single large templates have so many issues, it is good practice to author \n",
      "modular templates so that we get benefits such as the following:\n",
      "• Multiple teams can work on their templates in isolation.\n",
      "• Templates can be reused across projects and solutions.\n",
      "• Templates are easy to debug and troubleshoot.\n",
      "Now that we have covered some of the issues with single large templates, in the\n",
      "---------------\n",
      "next section, we will consider the crux of modular templates and how they may help \n",
      "developers to implement efficient deployments.\n",
      "---------------\n",
      "550 | ARM template modular design and implementation\n",
      "Understanding the Single Responsibility Principle\n",
      "The Single Responsibility Principle is one of the core principles of the SOLID design \n",
      "principles. It states that a class or code segment should be responsible for a single \n",
      "function and that it should own that functionality completely. The code should change \n",
      "or evolve only if there is a functional change or bug in the current functionality and not \n",
      "otherwise. This code should not change because of changes in some other component \n",
      "or code that is not part of the current component.\n",
      "Applying the same principle to ARM templates helps us to create templates that \n",
      "have the sole responsibility of deploying a single resource or functionality instead of \n",
      "deploying all resources and a complete solution.\n",
      "Using this principle will help you create multiple templates, each responsible for a \n",
      "single resource or a smaller group of resources rather than all resources.\n",
      "---------------\n",
      "Faster troubleshooting and debugging\n",
      "Each template deployment is a distinct activity within Azure and is a separate instance \n",
      "consisting of inputs, outputs, and logs. When multiple templates are deployed for \n",
      "deploying a solution, each template deployment has separate log entries along with its \n",
      "input and output descriptions. It is much easier to isolate bugs and troubleshoot issues \n",
      "using these independent logs from multiple deployments compared to a single large \n",
      "template.\n",
      "Modular templates\n",
      "When a single large template is decomposed into multiple templates where each \n",
      "smaller template takes care of its own resources, and those resources are solely owned, \n",
      "maintained, and are the responsibility of the template containing it, we can say we \n",
      "have modular templates. Each template within these templates follows the Single \n",
      "Responsibility Principle.\n",
      "Before learning how to divide a large template into multiple smaller reusable templates,\n",
      "---------------\n",
      "it is important to understand the technology behind creating smaller templates and \n",
      "how to compose them to deploy complete solutions.\n",
      "---------------\n",
      "Understanding the Single Responsibility Principle | 551\n",
      "Deployment resources\n",
      "ARM provides a facility to link templates. Although we have already gone through linked \n",
      "templates in detail, I will mention it here to help you understand how linking templates \n",
      "helps us achieve modularity, composition, and reusability.\n",
      "ARM templates provide specialized resources known as deployments, which are \n",
      "available from the Microsoft.Resources namespace. A deployment resource in an ARM \n",
      "template looks very similar to the code segment that follows:\n",
      "\"resources\": [ \n",
      "  { \n",
      "      \"apiVersion\": \"2019-10-01\", \n",
      "      \"name\": \"linkedTemplate\", \n",
      "      \"type\": \"Microsoft.Resources/deployments\", \n",
      "      \"properties\": { \n",
      "          \"mode\": \"Incremental\", \n",
      "          <nested-template-or-external-template> \n",
      "      } \n",
      "  } \n",
      "]\n",
      "This template is self-explanatory, and the two most important configurations in \n",
      "the template resource are the type and the properties. The type here refers to the\n",
      "---------------\n",
      "deployment resource rather than any specific Azure resource (storage, virtual machine, \n",
      "and so on) and the properties specify the deployment configuration, including a linked \n",
      "template deployment or a nested template deployment.\n",
      "However, what does the deployment resource do? The job of a deployment resource \n",
      "is to deploy another template. Another template could be an external template in a \n",
      "separate ARM template file, or it could be a nested template. It means that it is possible \n",
      "to invoke other templates from a template, just like a function call.\n",
      "---------------\n",
      "552 | ARM template modular design and implementation\n",
      "There can be nested levels of deployments in ARM templates. What this means is that \n",
      "a single template can call another template, and the called template can call another \n",
      "template, and this can go on for five levels of nested callings:\n",
      "Figure 16.1: Template decomposition into smaller templates\n",
      "Now that we understand that large templates can be modular with separate resources \n",
      "in separate templates, we need to link and bring them together to deploy resources on \n",
      "Azure. Linked and nested templates are ways to compose multiple templates together.\n",
      "Linked templates\n",
      "Linked templates are templates that invoke external templates. External templates are \n",
      "stored in different ARM template files. An example of linked templates follows:\n",
      "\"resources\": [ \n",
      "  { \n",
      "     \"apiVersion\": \"2019-10-01\", \n",
      "     \"name\": \"linkedTemplate\", \n",
      "     \"type\": \"Microsoft.Resources/deployments\", \n",
      "     \"properties\": { \n",
      "       \"mode\": \"Incremental\",\n",
      "---------------\n",
      "\"templateLink\": { \n",
      "         \"uri\":\"https://mystorageaccount.blob.core.windows.net/\n",
      "AzureTemplates/newStorageAccount.json\", \n",
      "          \"contentVersion\":\"1.0.0.0\" \n",
      "       }, \n",
      "       \"parametersLink\": { \n",
      "         \"uri\":\"https://mystorageaccount.blob.core.windows.net/\n",
      "AzureTemplates/newStorageAccount.parameters.json\", \n",
      "          \"contentVersion\":\"1.0.0.0\"\n",
      "---------------\n",
      "Linked templates | 553\n",
      "       } \n",
      "     } \n",
      "  } \n",
      "]\n",
      "Important additional properties in this template compared to the previous template \n",
      "are templateLink and parametersLink. Now, templateLink refers to the actual URL of \n",
      "the location of the external template file, and parametersLink is the URL location for \n",
      "the corresponding parameters file. It is important to note that the caller template \n",
      "should have access rights to the location of the called template. For example, if the \n",
      "external templates are stored in Azure Blob storage, which is protected by keys, then \n",
      "the appropriate Secure Access Signature (SAS) keys must be available to the caller \n",
      "template to be able to access the linked templates.\n",
      "It is also possible to provide explicit inline parameters instead of the parametersLink \n",
      "value, as shown here:\n",
      "\"resources\": [ \n",
      "  { \n",
      "     \"apiVersion\": \"2019-10-01\", \n",
      "     \"name\": \"linkedTemplate\", \n",
      "     \"type\": \"Microsoft.Resources/deployments\", \n",
      "     \"properties\": { \n",
      "       \"mode\": \"Incremental\",\n",
      "---------------\n",
      "\"templateLink\": { \n",
      "         \"uri\":\"https://mystorageaccount.blob.core.windows.net/\n",
      "AzureTemplates/newStorageAccount.json\", \n",
      "          \"contentVersion\":\"1.0.0.0\" \n",
      "       }, \n",
      "       \"parameters\": { \n",
      "          \"StorageAccountName\":{\"value\": \"   \n",
      "                            [parameters('StorageAccountName')]\"} \n",
      "        } \n",
      "     } \n",
      "  } \n",
      "]\n",
      "You now have a good understanding of linked templates. A closely related topic is \n",
      "nested templates, which the next section will discuss in detail.\n",
      "---------------\n",
      "554 | ARM template modular design and implementation\n",
      "Nested templates\n",
      "Nested templates are a relatively new feature in ARM templates compared to external \n",
      "linked templates.\n",
      "Nested templates do not define resources in external files. The resources are defined \n",
      "within the caller template itself and within the deployment resource, as shown here:\n",
      "\"resources\": [ \n",
      "  { \n",
      "    \"apiVersion\": \"2019-10-01\", \n",
      "    \"name\": \"nestedTemplate\", \n",
      "    \"type\": \"Microsoft.Resources/deployments\", \n",
      "    \"properties\": { \n",
      "      \"mode\": \"Incremental\", \n",
      "      \"template\": { \n",
      "        \"$schema\": \"https://schema.management.azure.com/schemas/2015- \n",
      "              01-01/deploymentTemplate.json#\", \n",
      "        \"contentVersion\": \"1.0.0.0\", \n",
      "        \"resources\": [ \n",
      "          { \n",
      "            \"type\": \"Microsoft.Storage/storageAccounts\", \n",
      "            \"name\": \"[variables('storageName')]\", \n",
      "            \"apiVersion\": \"2019-04-01\", \n",
      "            \"location\": \"West US\", \n",
      "            \"properties\": {\n",
      "---------------\n",
      "\"accountType\": \"Standard_LRS\" \n",
      "            } \n",
      "          } \n",
      "        ] \n",
      "      } \n",
      "    } \n",
      "  } \n",
      "]\n",
      "In this code segment, we can see that the storage account resource is nested within \n",
      "the original template as part of the deployments resource. Instead of using the \n",
      "templateLink and parametersLink attributes, a resources array is used to create multiple \n",
      "resources as part of a single deployment. The advantage of using a nested deployment \n",
      "is that resources within a parent can be used to reconfigure them by using their names. \n",
      "Usually, a resource with a name can exist only once within a template. Nested templates \n",
      "allow us to use them within the same template and ensure that all templates are self-\n",
      "sufficient rather than being stored separately, and they may or may not be accessible to \n",
      "those external files.\n",
      "---------------\n",
      "Nested templates | 555\n",
      "Now that we understand the technology behind modular ARM templates, how should \n",
      "we divide a large template into smaller templates?\n",
      "There are multiple ways a large template can be decomposed into smaller templates. \n",
      "Microsoft recommends the following pattern for the decomposition of ARM templates:\n",
      "Figure 16.2: Template decomposition strategy\n",
      "When we decompose a large template into smaller templates, there is always the \n",
      "main template, which is used for deploying the solution. This main or master template \n",
      "internally invokes other nested or linked templates and they, in turn, invoke other \n",
      "templates, and finally, the templates containing Azure resources are deployed.\n",
      "The main template can invoke a known configuration resource template, which, in turn, \n",
      "will invoke templates comprising Azure resources. The known configuration resource \n",
      "template is specific to a project or solution and it does not have many reusable factors\n",
      "---------------\n",
      "associated with it. The member resource templates are reusable templates invoked by \n",
      "the known configuration resource template.\n",
      "Optionally, the master template can invoke shared resource templates and other \n",
      "resource templates if they exist.\n",
      "It is important to understand known configurations. Templates can be authored as \n",
      "known configurations or as free-flow configurations.\n",
      "---------------\n",
      "556 | ARM template modular design and implementation\n",
      "Free-flow configurations\n",
      "ARM templates can be authored as generic templates where most, if not all, of the \n",
      "values assigned to variables are obtained as parameters. This allows the person using \n",
      "the template to pass any value they deem necessary to deploy resources in Azure. For \n",
      "example, the person deploying the template could choose a virtual machine of any size, \n",
      "any number of virtual machines, and any configuration for its storage and networks. \n",
      "This is known as free-flow configuration, where most of the configuration is allowed \n",
      "and the templates come from the user instead of being declared within the template.\n",
      "There are challenges with this kind of configuration. The biggest one is that not all \n",
      "configurations are supported in every Azure region and datacenter in Azure. The \n",
      "templates will fail to create resources if those resources are not allowed to be created\n",
      "---------------\n",
      "in specific locations or regions. Another issue with free-flow configuration is that users \n",
      "can provide any value they deem necessary and a template will honor them, thereby \n",
      "increasing both the cost and deployment footprint even though they are not completely \n",
      "required.\n",
      "Known configurations\n",
      "Known configurations, on the other hand, are specific pre-determined configurations \n",
      "for deploying an environment using ARM templates. These pre-determined \n",
      "configurations are known as T-shirt sizing configurations. Similar to the way a T-shirt \n",
      "is available in a pre-determined configuration such as small, medium, and large, ARM \n",
      "templates can be pre-configured to deploy a small, medium, or large environment \n",
      "depending on the requirements. This means that users cannot determine any \n",
      "random custom size for the environment, but they can choose from various provided \n",
      "options, and ARM templates executed during runtime will ensure that an appropriate \n",
      "configuration of the environment is provided.\n",
      "---------------\n",
      "So, the first step in creating a modular ARM template is deciding on the known \n",
      "configurations for an environment.\n",
      "As an example, here is the configuration of a datacenter deployment on Azure:\n",
      "Table 16.1: Configuration of a datacenter deployment on Azure\n",
      "Now that we know the configurations, we can create modular ARM templates.\n",
      "T-Shirt Size\n",
      "Small Four virtual machines with 7 GB of memory along with four CPU cores\n",
      "Medium Eight virtual machines with 14 GB of memory along with eight CPU cores\n",
      "Large 16 virtual machines with 28 GB of memory along with eight CPU cores\n",
      "---------------\n",
      "Known configurations | 557\n",
      "There are two ways to write modular ARM templates:\n",
      "• Composed templates: Composed templates link to other templates. Examples of \n",
      "composed templates are master and intermediate templates.\n",
      "• Leaf-level templates: Leaf-level templates are templates that contain a single \n",
      "Azure resource.\n",
      "ARM templates can be divided into modular templates based on the following:\n",
      "• Technology\n",
      "• Functionality\n",
      "An ideal way to decide on the modular method to author an ARM template is as follows:\n",
      "• Define resource- or leaf-level templates consisting of single resources. In the \n",
      "upcoming diagram, the extreme right templates are leaf-level templates. Within \n",
      "the diagram, virtual machines, virtual network, storage, and others in the same \n",
      "column represent leaf-level templates.\n",
      "• Compose environment-specific templates using leaf-level templates. These \n",
      "environment-specific templates provide an Azure environment, such as a SQL\n",
      "---------------\n",
      "Server environment, an App Service environment, or a datacenter environment. \n",
      "Let's drill down a bit more into this topic. Let's take the example of an Azure \n",
      "SQL environment. To create an Azure SQL environment, multiple resources are \n",
      "needed. At a bare minimum, a logical SQL Server, a SQL database, and a few \n",
      "SQL firewall resources should be provisioned. All these resources are defined in \n",
      "individual templates at the leaf level. These resources can be composed together \n",
      "in a single template that has the capability to create an Azure SQL environment. \n",
      "Anybody wanting to create an SQL environment can use this composed template. \n",
      "Figure 16.3 has Data center, Messaging, and App Service as environment-specific \n",
      "templates.\n",
      "• Create templates with higher abstraction composing multiple environment-\n",
      "specific templates into solutions. These templates are composed of environment-\n",
      "specific templates that were created in the previous step. For example, to create\n",
      "---------------\n",
      "an e-commerce inventory solution that needs an App Service environment and a \n",
      "SQL environment, two environment templates, App Service and SQL Server, can \n",
      "be composed together. Figure 16.3 has Functional 1 and Functional 2 templates, \n",
      "which are composed of child templates.\n",
      "• Finally, a master template should be created, which should be composed of \n",
      "multiple templates where each template is capable of deploying a solution.\n",
      "---------------\n",
      "558 | ARM template modular design and implementation\n",
      "The preceding steps for creating a modular designed template can be easily understood \n",
      "by means of Figure 16.3:\n",
      "Figure 16.3: Template and resource mapping\n",
      "Now, let's implement a part of the functionality shown in the previous diagram. In \n",
      "this implementation, we will provide a virtual machine with a script extension using a \n",
      "modular approach. The custom script extension deploys Docker binaries and prepares a \n",
      "container environment on a Windows Server 2016 virtual machine.\n",
      "---------------\n",
      "Known configurations | 559\n",
      "Now, we are going to create a solution using ARM templates using a modular approach. \n",
      "As mentioned before, the first step is to create individual resource templates. These \n",
      "individual resource templates will be used to compose additional templates capable of \n",
      "creating an environment. These templates will be needed to create a virtual machine. \n",
      "All ARM templates shown here are available in the accompanying chapter code. The \n",
      "names and code of these templates are as follows:\n",
      "• Storage.json\n",
      "• virtualNetwork.json\n",
      "• PublicIPAddress.json\n",
      "• NIC.json\n",
      "• VirtualMachine.json\n",
      "• CustomScriptExtension.json\n",
      "First, let's look at the code for the Storage.json template. This template provides a \n",
      "storage account, which every virtual machine needs for storing its OS and data disk \n",
      "files:\n",
      " \n",
      "{ \n",
      "    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01- \n",
      "01/deploymentTemplate.json#\", \n",
      "    \"contentVersion\": \"1.0.0.0\", \n",
      "  \"parameters\": { \n",
      "    \"storageAccountName\": {\n",
      "---------------\n",
      "\"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "    }, \n",
      "    \"storageType\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "    }, \n",
      "   ... \n",
      "  \"outputs\": { \n",
      "    \"resourceDetails\": { \n",
      "      \"type\": \"object\", \n",
      "      \"value\": \"[reference(parameters('storageAccountName'))]\" \n",
      "    } \n",
      "  } \n",
      "}\n",
      "---------------\n",
      "560 | ARM template modular design and implementation\n",
      "Next, let's look at the code for the public IP address template. A virtual machine that \n",
      "should be accessible over the internet needs a public IP address resource assigned to its \n",
      "network interface card. Although exposing a virtual machine to the internet is optional, \n",
      "this resource might get used for creating a virtual machine. The following code is \n",
      "available in the PublicIPAddress.json file:\n",
      "{ \n",
      "    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01- \n",
      "01/deploymentTemplate.json#\", \n",
      "    \"contentVersion\": \"1.0.0.0\", \n",
      "  \"parameters\": { \n",
      "    \"publicIPAddressName\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "    }, \n",
      "    \"publicIPAddressType\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "   ... \n",
      "      } \n",
      "    } \n",
      "  ], \n",
      "  \"outputs\": { \n",
      "    \"resourceDetails\": { \n",
      "      \"type\": \"object\", \n",
      "      \"value\": \"[reference(parameters('publicIPAddressName'))]\" \n",
      "    } \n",
      "  } \n",
      "}\n",
      "---------------\n",
      "Known configurations | 561\n",
      "Next, let's look at the code for the virtual network. Virtual machines on Azure need \n",
      "a virtual network for communication. This template will be used to create a virtual \n",
      "network on Azure with a pre-defined address range and subnets. The following code is \n",
      "available in the virtualNetwork.json file:\n",
      "{ \n",
      "    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01- \n",
      "01/deploymentTemplate.json#\", \n",
      "    \"contentVersion\": \"1.0.0.0\", \n",
      "  \"parameters\": { \n",
      "    \"virtualNetworkName\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      " ... \n",
      "    }, \n",
      "    \"subnetPrefix\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "    }, \n",
      "    \"resourceLocation\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "    } \n",
      " ... \n",
      "        \"subnets\": [ \n",
      "          { \n",
      "            \"name\": \"[parameters('subnetName')]\", \n",
      "            \"properties\": { \n",
      "              \"addressPrefix\": \"[parameters('subnetPrefix')]\" \n",
      "            } \n",
      "          } \n",
      "        ] \n",
      "      } \n",
      "    } \n",
      "  ], \n",
      "  \"outputs\": {\n",
      "---------------\n",
      "\"resourceDetails\": { \n",
      "      \"type\": \"object\", \n",
      "      \"value\": \"[reference(parameters('virtualNetworkName'))]\" \n",
      "    } \n",
      "  } \n",
      "}\n",
      "---------------\n",
      "562 | ARM template modular design and implementation\n",
      "Next, let's look at the code for the network interface card. A virtual network card is \n",
      "needed by a virtual machine to connect to a virtual network and to accept and send \n",
      "requests to and from the internet. The following code is available in the NIC.json file:\n",
      "{ \n",
      "    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01- \n",
      "01/deploymentTemplate.json#\", \n",
      "    \"contentVersion\": \"1.0.0.0\", \n",
      "  \"parameters\": { \n",
      "    \"nicName\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "    }, \n",
      "    \"publicIpReference\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      " ... \n",
      "[resourceId(subscription().subscriptionId,resourceGroup().name, 'Microsoft.\n",
      "Network/publicIPAddresses', parameters('publicIpReference'))]\", \n",
      "    \"vnetRef\": \"[resourceId(subscription().\n",
      "subscriptionId,resourceGroup().name, 'Microsoft.Network/virtualNetworks', \n",
      "parameters('virtualNetworkReference'))]\", \n",
      "    \"subnet1Ref\": \"[concat(variables('vnetRef'),'/subnets/',\n",
      "---------------\n",
      "parameters('subnetReference'))]\" \n",
      "  }, \n",
      " ... \n",
      "                \"id\": \"[variables('subnet1Ref')]\" \n",
      "              } \n",
      "            } \n",
      "          } \n",
      "        ] \n",
      "      } \n",
      "    } \n",
      "  ], \n",
      "  \"outputs\": { \n",
      "    \"resourceDetails\": { \n",
      "      \"type\": \"object\", \n",
      "      \"value\": \"[reference(parameters('nicName'))]\" \n",
      "    } \n",
      "  } \n",
      "}\n",
      "---------------\n",
      "Known configurations | 563\n",
      "Next, let's look at the code for creating a virtual machine. Each virtual machine is a \n",
      "resource in Azure, and note that this template has no reference to storage, network, \n",
      "public IP addresses, or other resources created earlier. This reference and composition \n",
      "will happen later in this section using another template. The following code is available \n",
      "in the VirtualMachine.json file:\n",
      "{ \n",
      "  \"$schema\": \"https://schema.management.azure.com/schemas/2015-01 \n",
      "01/deploymentTemplate.json#\", \n",
      "  \"contentVersion\": \"1.0.0.0\", \n",
      "  \"parameters\": { \n",
      "    \"vmName\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "   ... \n",
      "    }, \n",
      "    \"imageOffer\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "    }, \n",
      "    \"windowsOSVersion\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "    }, \n",
      "   ... \n",
      "  \"outputs\": { \n",
      "    \"resourceDetails\": { \n",
      "      \"type\": \"object\", \n",
      "      \"value\": \"[reference(parameters('vmName'))]\" \n",
      "    } \n",
      "  }\n",
      "}\n",
      "---------------\n",
      "564 | ARM template modular design and implementation\n",
      "Next, let's look at the code for creating a custom script extension. This resource \n",
      "executes a PowerShell script on a virtual machine after it is provisioned. This resource \n",
      "provides an opportunity to execute post-provisioning tasks in Azure Virtual Machines. \n",
      "The following code is available in the CustomScriptExtension.json file:\n",
      "{ \n",
      "    \"$schema\": \"http://schema.management.azure.com/schemas/2015-01-01/\n",
      "deploymentTemplate.json#\", \n",
      "    \"contentVersion\": \"1.0.0.0\", \n",
      "    \"parameters\": { \n",
      "      \"VMName\": { \n",
      "        \"type\": \"string\", \n",
      "        \"defaultValue\": \"sqldock\", \n",
      "        \"metadata\": { \n",
      "... \n",
      "            \"commandToExecute\": \"[concat('powershell -ExecutionPolicy \n",
      "Unrestricted -file docker.ps1')]\" \n",
      "          }, \n",
      "          \"protectedSettings\": { \n",
      "          } \n",
      "        } \n",
      "      } \n",
      "    ], \n",
      "    \"outputs\": { \n",
      "    } \n",
      "}\n",
      "Next, we'll look at the custom script extension PowerShell code that prepares the\n",
      "---------------\n",
      "Docker environment. Please note that a virtual machine reboot might happen while \n",
      "executing the PowerShell script, depending on whether the Windows containers \n",
      "feature is already installed or not. The following script installs the NuGet package, the \n",
      "DockerMsftProvider provider, and the Docker executable. The docker.ps1 file is available \n",
      "with the accompanying chapter code:\n",
      "# \n",
      "# docker.ps1 \n",
      "# \n",
      "Install-PackageProvider -Name Nuget -Force -ForceBootstrap -Confirm:$false \n",
      "Install-Module -Name DockerMsftProvider -Repository PSGallery -Force \n",
      "-Confirm:$false -verboseInstall-Package -Name docker -ProviderName \n",
      "DockerMsftProvider -Force -ForceBootstrap -Confirm:$false\n",
      "---------------\n",
      "Known configurations | 565\n",
      "All the previously seen linked templates should be uploaded to a container within an \n",
      "Azure Blob storage account. This container can have a private access policy applied, as \n",
      "you saw in the previous chapter; however, for this example, we will set the access policy \n",
      "as container. This means these linked templates can be accessed without an SAS token. \n",
      "Finally, let's focus on writing the master template. Within the master template, all \n",
      "the linked templates are composed together to create a solution—to deploy a virtual \n",
      "machine and execute a script within it. The same approach can be used for creating \n",
      "other solutions, such as providing a datacenter consisting of multiple inter-connected \n",
      "virtual machines. The following code is available in the Master.json file:\n",
      "{ \n",
      "    \"$schema\": \"https://schema.management.azure.com/schemas/2015-01-01/\n",
      "deploymentTemplate.json#\", \n",
      "    \"contentVersion\": \"1.0.0.0\", \n",
      "  \"parameters\": { \n",
      "    \"storageAccountName\": {\n",
      "---------------\n",
      "\"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "   ... \n",
      "    }, \n",
      "    \"subnetName\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "    }, \n",
      "    \"subnetPrefix\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "  ... \n",
      "    \"windowsOSVersion\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "    }, \n",
      "    \"vhdStorageName\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "    }, \n",
      "    \"vhdStorageContainerName\": { \n",
      "      \"type\": \"string\", \n",
      "      \"minLength\": 1 \n",
      "   ...[concat('https://',parameters('storageAccountName'),'armtfiles.blob.\n",
      "core.windows.net/',variables('containerName'),'/Storage.json')]\", \n",
      "          \"contentVersion\": \"1.0.0.0\"\n",
      "---------------\n",
      "566 | ARM template modular design and implementation\n",
      "        }, \n",
      "        \"parameters\": { \n",
      "          \"storageAccountName\": { \n",
      "            \"value\": \"[parameters('storageAccountName')]\" \n",
      "          }, \n",
      "          \"storageType\": { \n",
      "            \"value\": \"[parameters('storageType')]\" \n",
      "          }, \n",
      "          \"resourceLocation\": { \n",
      "            \"value\": \"[resourceGroup().location]\" \n",
      "      ... \n",
      "  \"outputs\": { \n",
      "    \"resourceDetails\": { \n",
      "      \"type\": \"object\", \n",
      "      \"value\": \"[reference('GetVM').outputs.resourceDetails.value]\" \n",
      " } \n",
      " } \n",
      "}\n",
      "The master templates invoke the external templates and also co-ordinate inter-\n",
      "dependencies among them.\n",
      "The external templates should be available in a well-known location so that the master \n",
      "template can access and invoke them. In this example, the external templates are \n",
      "stored in the Azure Blob storage container and this information was passed to the ARM \n",
      "template by means of parameters.\n",
      "---------------\n",
      "The external templates in Azure Blob storage could be access-protected by setting \n",
      "up access policies. The command used to deploy the master template is shown \n",
      "next. It might look like a complex command, but a majority of the values are used \n",
      "as parameters. You are advised to change the values of these parameters before \n",
      "running it. The linked templates have been uploaded to a storage account named \n",
      "st02gvwldcxm5suwe within the armtemplates container. The resource group should \n",
      "be created if it does not currently exist. The first command is used to create a new \n",
      "resource group in the West Europe region:\n",
      "New-AzResourceGroup -Name \"testvmrg\" -Location \"West Europe\" -Verbose\n",
      "---------------\n",
      "Understanding copy and copyIndex | 567\n",
      "The rest of the parameter values are needed to configure each resource. The storage \n",
      "account name and the dnsNameForPublicIP value should be unique within Azure:\n",
      "New-AzResourceGroupDeployment -Name \"testdeploy1\" -ResourceGroupName \n",
      "testvmrg -Mode Incremental -TemplateFile \"C:\\chapter 05\\Master.json\" \n",
      "-storageAccountName \"st02gvwldcxm5suwe\" -storageType \"Standard_LRS\" \n",
      "-publicIPAddressName \"uniipaddname\" -publicIPAddressType \"Dynamic\" \n",
      "-dnsNameForPublicIP \"azureforarchitectsbook\" -virtualNetworkName \n",
      "vnetwork01 -addressPrefix \"10.0.1.0/16\" -subnetName \"subnet01\" -subnetPrefix \n",
      "\"10.0.1.0/24\" -nicName nic02 -vmSize \"Standard_DS1\" -adminUsername \"sysadmin\" \n",
      "-adminPassword $(ConvertTo-SecureString -String sysadmin@123 -AsPlainText \n",
      "-Force) -vhdStorageName oddnewuniqueacc -vhdStorageContainerName vhds \n",
      "-OSDiskName mynewvm -vmName vm10 -windowsOSVersion 2012-R2-Datacenter \n",
      "-imagePublisher MicrosoftWindowsServer -imageOffer WindowsServer\n",
      "---------------\n",
      "-containerName armtemplates -Verbose\n",
      "In this section, we covered best practices for decomposing large templates into smaller \n",
      "reusable templates and combining them together at runtime to deploy complete \n",
      "solutions on Azure. As we progress through the book, we will modify the ARM template \n",
      "step by step until we have explored its core parts. We used Azure PowerShell cmdlets to \n",
      "initiate the deployment of templates on Azure. \n",
      "Let's move on to the topic of copy and copyIndex. \n",
      "Understanding copy and copyIndex\n",
      "There are many times when multiple instances of a particular resource or a group of \n",
      "resources are needed. For example, you may need to provision 10 virtual machines of \n",
      "the same type. In such cases, it is not prudent to deploy templates 10 times to create \n",
      "these instances. A better alternate approach is to use the copy and copyIndex features of \n",
      "ARM templates.\n",
      "copy is an attribute of every resource definition. This means it can be used to create\n",
      "---------------\n",
      "multiple instances of any resource type.\n",
      "Let's understand this with the help of an example of creating multiple storage accounts \n",
      "within a single ARM template deployment.\n",
      "---------------\n",
      "568 | ARM template modular design and implementation\n",
      "The next code snippet creates 10 storage accounts serially. They could have been \n",
      "created in parallel by using Parallel instead of Serial for the mode property:\n",
      "\"resources\": [\n",
      "      {\n",
      "          \"apiVersion\": \"2019-06-01\",\n",
      "          \"type\": \"Microsoft.Storage/storageAccounts\",\n",
      "          \"location\": \"[resourceGroup().location]\",\n",
      "          \"name\": \"[concat(variables('storageAccountName'), copyIndex())]\",\n",
      "          \"tags\":{\n",
      "              \"displayName\": \"[variables('storageAccountName')]\"\n",
      "          },\n",
      "          \"sku\":{\n",
      "              \"name\":\"Premium_ZRS\"\n",
      "          },\n",
      "          \"kind\": \"StorageV2\",\n",
      "          \"copy\":{\n",
      "              \"name\": \"storageInstances\",\n",
      "              \"count\": 10,\n",
      "              \"mode\": \"Serial\"\n",
      "          }        \n",
      "      }  \n",
      "    ],\n",
      "In the preceding code, copy has been used to provision 10 instances of the storage \n",
      "account serially, that is, one after another. The storage account names must be unique\n",
      "---------------\n",
      "for all 10 instances, and copyIndex has been used to make them unique by concatenating \n",
      "the original storage name with the index value. The value returned by the copyIndex \n",
      "function changes in every iteration; it will start at 0 and go on for 10 iterations. This \n",
      "means it will return 9 for the last iteration.\n",
      "Now that we have learned how to create multiple instances of an ARM template, let's \n",
      "dive into securing these templates from known vulnerabilities.\n",
      "---------------\n",
      "Securing ARM templates | 569\n",
      "Securing ARM templates\n",
      "Another important aspect related to creating enterprise ARM templates is securing \n",
      "them appropriately. ARM templates contain the resource configuration and vital \n",
      "information about infrastructure, and so they should not be compromised or accessible \n",
      "to unauthorized people.\n",
      "The first step in securing ARM templates is storing them in storage accounts and \n",
      "stopping any anonymous access to the storage account container. Moreover, SAS \n",
      "tokens should be generated for storage accounts and used in ARM templates to \n",
      "consume linked templates. This will ensure that only the holders of SAS tokens can \n",
      "access the templates. Moreover, these SAS tokens should be stored in Azure Key Vault \n",
      "instead of being hardcoded into ARM templates. This will ensure that even the people \n",
      "responsible for deployment do not have access to the SAS token.\n",
      "Another step in securing ARM templates is ensuring that any sensitive information and\n",
      "---------------\n",
      "secrets, such as database connection strings, Azure subscription and tenant identifiers, \n",
      "service principal identifiers, IP addresses, and so on, should not be hardcoded in ARM \n",
      "templates. They should all be parameterized, and the values should be fetched at \n",
      "runtime from Azure Key Vault. However, before using this approach, it is important that \n",
      "these secrets are stored in Key Vault prior to executing any ARM templates.\n",
      "The following code shows one of the ways that values can be extracted from Azure Key \n",
      "Vault at runtime using the parameters file:\n",
      "{ \n",
      "    \"$schema\": https://schema.management.azure.com/schemas/2016-01-01/\n",
      "deploymentParameters.json#, \n",
      "    \"contentVersion\": \"1.0.0.0\", \n",
      "    \"parameters\": { \n",
      "        \"storageAccountName\": { \n",
      "            \"reference\": { \n",
      "                \"keyVault\": { \n",
      "                    \"id\": \"/subscriptions/--subscription id --/\n",
      "resourceGroups/rgname/providers/Microsoft.KeyVault/vaults/keyvaultbook\"), \n",
      "                \"secretName\": \"StorageAccountName\"\n",
      "---------------\n",
      "} \n",
      "            } \n",
      "        } \n",
      "    } \n",
      "}\n",
      "In this code listing, a parameter is defined that references Azure Key Vault to fetch \n",
      "values at runtime during deployment. The Azure Key Vault identifier and the secret \n",
      "name have been provided as input values.\n",
      "---------------\n",
      "570 | ARM template modular design and implementation\n",
      "Now that you have learned how to secure ARM templates, let's take a look at identifying \n",
      "the various dependencies between them and how we can enable communication \n",
      "between multiple templates.\n",
      "Using outputs between ARM templates\n",
      "One of the important aspects that can easily be overlooked while using linked templates \n",
      "is that there might be resource dependencies within linked templates. For example, a \n",
      "SQL Server resource might be in a linked template that is different to that of a virtual \n",
      "machine resource. If we want to open the SQL Server firewall for the virtual machine IP \n",
      "address, then we should be able to dynamically pass this information to the SQL Server \n",
      "firewall resource after provisioning the virtual machine.\n",
      "This could be done using the simple method of referring to the IP address resource \n",
      "using the REFERENCES function if the SQL Server and virtual machine resources are in the \n",
      "same template.\n",
      "---------------\n",
      "It becomes slightly more complex in the case of linked templates if we want to share \n",
      "runtime property values from one resource to another when they are in different \n",
      "templates.\n",
      "ARM templates provide an outputs configuration, which is responsible for generating \n",
      "outputs from the current template deployment and returning them to the user. For \n",
      "example, we might output a complete object, as shown in the following code listing, \n",
      "using the reference function, or we might just output an IP address as a string value:\n",
      "\"outputs\": {\n",
      "    \"storageAccountDetails\": {\n",
      "        \"type\": \"object\",\n",
      "        \"value\": \"[reference(resourceid \n",
      "            ('Microsoft.Storage/storageAccounts',  \n",
      "            variables('storageAccountName')))]\",\n",
      "    \"virtualMachineIPAddress\": {\n",
      "        \"type\": \"string\",\n",
      "        \"value\": \"[reference(variables \n",
      "            ('publicIPAddressName')).properties.ipAddress]\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "---------------\n",
      "Using outputs between ARM templates | 571\n",
      "Parameters within a linked template can be utilized by the master template. When a \n",
      "linked template is called, the output is available to the master template that can be \n",
      "supplied as a parameter to the next linked or nested template. This way, it is possible to \n",
      "send the runtime configuration values of resources from one template to another.\n",
      "The code in the master template would be similar to what's shown here; this is the code \n",
      "that's used to call the first template:\n",
      "{ \n",
      "    \"type\": \"Microsoft.Resources/deployments\",  \n",
      "    \"apiVersion\": \"2017-05-10\",  \n",
      "    \"name\": \"createvm\",  \n",
      "    \"resoureceGroup\": \"myrg\",  \n",
      "    \"dependsOn\": [ \n",
      "        \"allResourceGroups\" \n",
      "    ], \n",
      "    \"properties\":{ \n",
      "        \"mode\": \"Incremental\", \n",
      "        \"templateLink\":{ \n",
      "            \"uri\": \"[variables( \n",
      "                'templateRefSharedServicesTemplateUri')]\", \n",
      "            \"contentVersion\": \"1.0.0.0\" \n",
      "        }, \n",
      "        \"parameters\": { \n",
      "            \"VMName\": {\n",
      "---------------\n",
      "\"value\": \"[variables('VmName')]\" \n",
      "            } \n",
      "        } \n",
      "    } \n",
      "}\n",
      "---------------\n",
      "572 | ARM template modular design and implementation\n",
      "The preceding code snippet from the master template is calling a nested template \n",
      "responsible for provisioning a virtual machine. The nested template has an output \n",
      "section that provides the IP address of the virtual machine. The master template will \n",
      "have another deployment resource in its template that will take the output value and \n",
      "send it as a parameter to the next nested template, passing the IP address at runtime. \n",
      "This is shown in the following code:\n",
      "{ \n",
      "    \"type\": \"Microsoft,Resources/deployments\", \n",
      "    \"apiVersion\": \"2017-05-10\", \n",
      "    \"name\": \"createSQLServer\", \n",
      "    \"resourceGroup\": \"myrg\", \n",
      "    \"dependsOn\": [ \n",
      "        \"createvm\" \n",
      "    ], \n",
      "    \"properties\": { \n",
      "        \"mode\": \"Incremental\", \n",
      "        \"templateLink\": { \n",
      "            \"uri\": \"[variables('templateRefsql')]\", \n",
      "            \"contentVersion\": \"1.0.0.0\" \n",
      "        }, \n",
      "        \"parameters\": { \n",
      "            \"VMName\": { \n",
      "                \"value\": \"[reference\n",
      "---------------\n",
      "('createvm').outputs.virtualMachineIPAddress.value]\" \n",
      "            } \n",
      "        } \n",
      "    } \n",
      "}\n",
      "In the preceding code listing, a nested template is being invoked and a parameter \n",
      "is being passed to it. The value of the parameter is derived from the previous linked \n",
      "template's output, which is named virtualMachineIPAddress. Now, the nested template \n",
      "will get the IP address of the virtual machine dynamically and it can use it as a \n",
      "whitelisted IP address.\n",
      "Using this approach, we can pass runtime values from one nested template to another.\n",
      "---------------\n",
      "Summary | 573\n",
      "Summary\n",
      "ARM templates are the preferred means of provisioning resources in Azure. They \n",
      "are idempotent in nature, bringing consistency, predictability, and reusability to \n",
      "environment creation. In this chapter, we looked at how to create a modular ARM \n",
      "template. It is important for teams to spend quality time designing ARM templates \n",
      "in an appropriate way, so that multiple teams can work on them together. They are \n",
      "highly reusable and require minimal changes to evolve. In this chapter, we learned \n",
      "how to create templates that are secure by design, how to provision multiple resource \n",
      "instances in a single deployment, and how to pass outputs from one nested template to \n",
      "another using the outputs section of ARM templates.\n",
      "The next chapter will move on to a different and very popular strand of technology \n",
      "known as serverless within Azure. Azure Functions is one of the major serverless \n",
      "resources of Azure, and this will be covered in complete depth, including Durable\n",
      "---------------\n",
      "Functions.\n",
      "---------------\n",
      "In the previous chapter, you learned about ARM templates, and so far, we have been \n",
      "dealing with architectural concerns and their solutions in Azure in general. However, \n",
      "this chapter is not based on generalized architecture. In fact, it explores one of the \n",
      "most disruptive technologies of this century. This chapter will discuss the details of the \n",
      "Internet of Things (IoT) and Azure.\n",
      "Azure IoT refers to a collection of Microsoft-managed cloud services that can connect, \n",
      "monitor, and control billions of IoT assets. In other words, an IoT solution comprises \n",
      "one or more IoT devices that constantly communicate with one or more back-end \n",
      "servers in the cloud.\n",
      "Designing IoT \n",
      "solutions\n",
      "17\n",
      "---------------\n",
      "576 | Designing IoT solutions\n",
      "This chapter will cover the following topics:\n",
      "• Azure and IoT\n",
      "• An overview of Azure IoT\n",
      "• Device management\n",
      "• Registering devices\n",
      "• Device-to-IoT-hub communication\n",
      "• Scaling IoT solutions\n",
      "• High availability for IoT solutions\n",
      "• IoT protocols\n",
      "• Using message properties to route messages\n",
      "IoT\n",
      "The internet was invented in the 1980s and later became widely available. Almost \n",
      "everyone moved toward having a presence on the internet and started creating their \n",
      "own static web pages. Eventually, the static content became dynamic and could be \n",
      "generated on the fly, based on context. In nearly all cases, a browser was needed to \n",
      "access the internet. There were a plethora of browsers available, and without them, \n",
      "using the internet was a challenge.\n",
      "During the first decade of this century, there was an interesting development that was \n",
      "emerging—the rise of handheld devices, such as mobile phones and tablets. Mobile\n",
      "---------------\n",
      "phones started becoming cheaper and were available ubiquitously. The hardware and \n",
      "software capabilities of these handheld devices were improving considerably, and so \n",
      "much so that people started using browsers on their mobile devices rather than on \n",
      "their desktops. But one particularly distinct change was the rise of mobile apps. These \n",
      "mobile apps were downloaded from a store and connected to the internet to talk to \n",
      "back-end systems. Toward the end of the last decade, there were millions of apps \n",
      "available with almost every conceivable functionality built into them. The back-end \n",
      "system for these apps was built on the cloud so that they could be scaled rapidly. This \n",
      "was the age of connecting applications and servers.\n",
      "---------------\n",
      "IoT architecture | 577\n",
      "But was this the pinnacle of innovation? What was the next evolution of the internet? \n",
      "Well, another paradigm has now been taking center stage: IoT. Instead of just mobile \n",
      "and tablet devices connecting to the internet, why can't other devices connect to the \n",
      "internet? Previously, such devices were available only in select markets; they were \n",
      "costly, not available to the masses, and had limited hardware and software capabilities. \n",
      "However, since the first part of this decade, the commercialization of these devices \n",
      "has been growing on a grand scale. These devices are becoming smaller and smaller, \n",
      "are more capable in terms of hardware and software, have more storage and compute \n",
      "power, can connect to the internet on various protocols, and can be attached to almost \n",
      "anything. This is the age of connecting devices to servers, applications, and other \n",
      "devices.\n",
      "This has led to the formulation of the idea that IoT applications can change the way\n",
      "---------------\n",
      "that industries are operating. Newer solutions that were previously unheard of are \n",
      "beginning to be realized. Now, these devices can be attached to anything; they can get \n",
      "information and send it to a back-end system that can assimilate information from all \n",
      "the devices and either take action on or report incidents.\n",
      "IoT sensors and controls can be leveraged in many business use cases. For example, \n",
      "they can be used in vehicle tracking systems, which can track all the vital parameters \n",
      "of a vehicle and send those details to a centralized data store for analysis. Smart \n",
      "city initiatives can also make use of various sensors to track pollution levels, \n",
      "temperature, and street congestion. IoT has also found its way into agriculture-\n",
      "related activities, such as measuring soil fertility, humidity, and more. You can \n",
      "visit Microsoft Technical Case Studies for IoT, at https:/ /microsoft.github.io/\n",
      "techcasestudies/#technology=IoT&sortBy=featured, for real-life examples of how\n",
      "---------------\n",
      "organizations leverage Azure IoT. \n",
      "Before we explore the tools and services related to IoT, we will first cover IoT \n",
      "architecture in detail.\n",
      "IoT architecture\n",
      "Before getting into Azure and its features and services regarding IoT, it is important \n",
      "to understand the various components that are needed to create end-to-end IoT \n",
      "solutions.\n",
      "Consider that IoT devices across the globe are sending millions of messages every \n",
      "second to a centralized database. Why is this data being collected? Well, the answer \n",
      "is to extract rich information about events, anomalies, and outliers that are to do with \n",
      "whatever those devices are monitoring.\n",
      "---------------\n",
      "578 | Designing IoT solutions\n",
      "Let's understand this in more detail.\n",
      "IoT architecture can be divided into distinct phases, as follows:\n",
      "1. Connectivity: This phase involves a connection being made between a device and \n",
      "the IoT service.\n",
      "2. Identity: After connecting to the IoT service, the first thing that happens is the \n",
      "identification of the device and ensuring that it is allowed to send device telemetry \n",
      "to the IoT service. This is done using an authentication process.\n",
      "3. Capture: During this phase, the device telemetry is captured and received by the \n",
      "IoT service.\n",
      "4. Ingestion: In this phase, the IoT service ingests the device telemetry.\n",
      "5. Storage: The device telemetry is stored. It could be a temporary or permanent \n",
      "store.\n",
      "6. Transformation: The telemetry data is transformed for further processing. This \n",
      "includes augmenting existing data and inferring data.\n",
      "7. Analytics: The transformed data is used to find patterns, anomalies, and insights.\n",
      "---------------\n",
      "8. Presentation: The insights are shown as dashboards and reports. Additionally, new \n",
      "alerts can be generated that could invoke automation scripts and processes.\n",
      "Figure 17.1 shows a generic IoT-based architecture. Data is generated or collected by \n",
      "devices and sent over to the cloud gateway. The cloud gateway, in turn, sends the data \n",
      "to multiple back-end services for processing. Cloud gateways are optional components; \n",
      "they should be used when the devices themselves are not capable of sending requests \n",
      "to back-end services, either because of resource constraints or the lack of a reliable \n",
      "network. These cloud gateways can collate data from multiple devices and send it to \n",
      "back-end services. The data can then be processed by back-end services and shown as \n",
      "insights or dashboards to users:\n",
      "---------------\n",
      "IoT architecture | 579\n",
      "Figure 17.1: A generic IoT application architecture\n",
      "Now that we are clear about the architecture, let's go ahead and understand how IoT \n",
      "devices communicate with other devices.\n",
      "Connectivity\n",
      "IoT devices need to communicate to connect to other devices. There are various \n",
      "connectivity types; for example, connectivity could exist between devices in a region, \n",
      "between devices and a centralized gateway, and between devices and an IoT platform.\n",
      "In all such cases, IoT devices need connectivity capability. This capability could be \n",
      "in the form of internet connectivity, Bluetooth, infrared, or any other near-device \n",
      "communication.\n",
      "However, some IoT devices might not have the capability to connect to the internet. In \n",
      "these cases, they can connect to a gateway that in turn has connectivity to the internet.\n",
      "IoT devices use protocols to send messages. The major protocols are the Advanced \n",
      "Message Queuing Protocol (AMQP) and the Message Queue Telemetry\n",
      "---------------\n",
      "Transport (MQTT) protocol.\n",
      "---------------\n",
      "580 | Designing IoT solutions\n",
      "Device data should be sent to an IT infrastructure. The MQTT protocol is a device-\n",
      "to-server protocol that devices can use to send telemetry data and other information \n",
      "to servers. Once the server receives a message through the MQTT protocol, it needs \n",
      "to transport the message to other servers using a reliable technology that is based on \n",
      "messages and queues. AMQP is the preferred protocol for moving messages between \n",
      "servers in an IT infrastructure in a reliable and predictable manner:\n",
      "Figure 17.2: Workings of the MQTT and AMQP protocols\n",
      "Servers receiving initial messages from IoT devices should send those messages to \n",
      "other servers for whatever processing is necessary, such as saving to logs, evaluation, \n",
      "analytics, and presentation.\n",
      "Some devices do not have the capability to connect to the internet or do not support \n",
      "protocols that are compatible with other server technologies. To enable these devices\n",
      "---------------\n",
      "to work with an IoT platform and the cloud, intermediate gateways can be used. \n",
      "Gateways help in onboarding devices whose connectivity and networking capability is \n",
      "slow and not consistent; such devices may use protocols that are not standard, or their \n",
      "capabilities may be limited in terms of resources and power.\n",
      "In such circumstances, when devices need additional infrastructure to connect to back-\n",
      "end services, client gateways can be deployed. These gateways receive messages from \n",
      "near devices, and forward (or push) them to the IT infrastructure and the IoT platform \n",
      "for further consumption. These gateways are capable of protocol translation if required. \n",
      "In this section, you learned about how communication is implemented with other \n",
      "devices and about the role that gateways play in terms of communication. In the next \n",
      "section, we're going to talk about identity.\n",
      "---------------\n",
      "IoT architecture | 581\n",
      "Identity\n",
      "IoT devices should be registered with a cloud platform. Devices that are not registered \n",
      "should not be allowed to connect to a cloud platform. The devices should be \n",
      "registered and be assigned an identity. A device should send its identity information \n",
      "when connecting to the cloud. If the device fails to send this identity information, \n",
      "the connectivity should fail. You will see, later in this chapter, how to generate an \n",
      "identity for a device using a simulated application. As you already know, IoT devices are \n",
      "deployed to capture information, and in the next section, we will briefly talk about the \n",
      "capture process.\n",
      "Capture\n",
      "IoT devices should be able to capture information. They should have the capability, for \n",
      "example, to read or monitor the moisture content in the air or in soil. This information \n",
      "can be captured based on frequency—maybe even once per second. Once the\n",
      "---------------\n",
      "information is captured, the device should be able to send it across to the IoT platform \n",
      "for processing. If a device can't connect to the IoT platform directly, it can connect to \n",
      "an intermediary cloud gateway instead and have that push the captured information.\n",
      "The size of captured data and the frequency of capture are the most important things \n",
      "for the device. Whether a device should have local storage to be able to temporarily \n",
      "store captured data is another important aspect that should be considered. For \n",
      "instance, a device can work in offline mode if there is enough local storage available. \n",
      "Even mobile devices sometimes act as IoT devices connected to various instruments \n",
      "and have the capability to store data. Once we have captured the data, we need to \n",
      "ingest it to an IoT platform for further analysis, and in the next section, we will explore \n",
      "ingestion.\n",
      "Ingestion\n",
      "Data captured and generated by devices should be sent to an IoT platform that is\n",
      "---------------\n",
      "capable of ingesting and consuming this data to extract meaningful information and \n",
      "insights out of it. The ingestion service is a crucial service because its availability and \n",
      "scalability affect the throughput of incoming data. If data starts getting throttled due \n",
      "to scalability issues, or if data is not able to be ingested due to availability issues, then it \n",
      "will be lost and the dataset might get biased or skewed. We have the data captured and \n",
      "we need a place to store this data. In the next section, you'll learn about storage.\n",
      "---------------\n",
      "582 | Designing IoT solutions\n",
      "Storage\n",
      "IoT solutions generally deal with millions or even billions of records, spanning terabytes \n",
      "or even petabytes of data. This is valuable data that can provide insights on operations \n",
      "and their health. This data needs to be stored in such a way that analytics can be \n",
      "performed on it. Storage should be readily available for analytics, applications, and \n",
      "services to consume it. Storage solutions should provide adequate throughput and \n",
      "latency from a performance perspective, and be highly available, scalable, and secure. \n",
      "The next section deals with data transformation, which is needed to store and analyze \n",
      "data. \n",
      "Transformation\n",
      "IoT solutions are generally data-driven and have considerably high volumes of data \n",
      "to deal with. Imagine that every car has a device and each one sends messages every \n",
      "five seconds. If there were a million cars sending messages, this would be equal to 288\n",
      "---------------\n",
      "million messages per day and 8 billion messages per month. Together, this data has lots \n",
      "of hidden information and insights; however, making sense of this kind of data just by \n",
      "looking at it is difficult.\n",
      "The data that is captured and stored by IoT devices can be consumed to solve business \n",
      "problems, but not all data that is captured is of importance. Just a subset of data might \n",
      "be needed to solve a problem. Additionally, the data that the IoT devices gather might \n",
      "not be consistent either. To ensure that the data is consistent and not biased or skewed, \n",
      "appropriate transformations should be executed upon it to make it ready for analysis. \n",
      "During transformation, data is filtered, sorted, removed, enriched, and transformed \n",
      "into a structure, such that the data can be consumed by components and applications \n",
      "further downstream. We need to perform some analytics with the transformed data \n",
      "before presenting it. As the next step in the workflow, we will discuss analytics.\n",
      "Analytics\n",
      "---------------\n",
      "The data transformed in the previous step becomes the input for the analytics step. \n",
      "Depending on the problem at hand, there are different types of analytics that can be \n",
      "performed on transformed data.\n",
      "The following are the different types of analytics that can be performed:\n",
      "• Descriptive analytics: This type of analytics helps in finding patterns and details \n",
      "about the status of the IoT devices and their overall health. This stage identifies \n",
      "and summarizes the data for further consumption by more advanced stages of \n",
      "analytics. It will help in summarization, finding statistics related to probability, \n",
      "identifying deviation, and other statistical tasks.\n",
      "---------------\n",
      "IoT architecture | 583\n",
      "• Diagnostic analytics: This type of analytics is more advanced than descriptive \n",
      "analytics. It builds on descriptive analytics and tries to answer queries about why \n",
      "certain things have happened. That is, it tries to find the root causes of events. It \n",
      "tries to find answers using advanced concepts, such as hypothesis and correlation.\n",
      "• Predictive analytics: This type of analytics tries to predict things that have a high \n",
      "probability of happening in the future. It generates predictions that are based on \n",
      "past data; regression is one of the examples that is based on past data. Predictive \n",
      "analytics could, for example, predict the price of a car, the behavior of stock on the \n",
      "stock market, when a car tire will burst, and more.\n",
      "• Prescriptive analytics: This type of analytics is the most advanced. This stage \n",
      "helps in identifying the action that should be performed to ensure that the health\n",
      "---------------\n",
      "of devices and solutions does not degrade, and identifying proactive measures to \n",
      "undertake. The results of this stage of analytics can help in avoiding future issues \n",
      "and eliminating problems at their root causes.\n",
      "In the final stage, the output from analytics is presented in a human-readable manner \n",
      "for a wider audience to understand and interpret. In the next part, we will discuss \n",
      "presentation.\n",
      "Presentation\n",
      "Analytics help in identifying answers, patterns, and insights based on data. These \n",
      "insights also need to be available to all stakeholders in formats that they can \n",
      "understand. To this end, dashboards and reports can be generated, statistically or \n",
      "dynamically, and then be presented to stakeholders. Stakeholders can consume these \n",
      "reports for further action and improve their solutions continuously.\n",
      "As a quick recap of all the preceding steps, we started off this section by looking at \n",
      "connectivity, where we introduced gateways for sending the data from devices that\n",
      "---------------\n",
      "don't support the standard protocols. Then, we talked about identity and how data is \n",
      "captured. The captured data is then ingested and stored for further transformation. \n",
      "After transformation, analytics is done on the data before it's presented to all \n",
      "stakeholders. As we are working on Azure, in the next section, we will cover what Azure \n",
      "IoT is and consider the basics concepts that we have learned about so far from an Azure \n",
      "standpoint.\n",
      "---------------\n",
      "584 | Designing IoT solutions\n",
      "Azure IoT\n",
      "Now you've learned about the various stages of end-to-end IoT solutions; each of these \n",
      "stages is crucial and their proper implementation is a must for any solution's success. \n",
      "Azure provides lots of services for each of these stages. Apart from these services, \n",
      "Azure provides Azure IoT Hub, which is Azure's core IoT service and platform. It is \n",
      "capable of hosting complex, highly available, and scalable IoT solutions. We will dive \n",
      "deep into IoT Hub after going through some other services:\n",
      "Figure 17.3: List of devices and services for IoT solutions\n",
      "In the next section, we will follow a similar pattern as we did for our coverage of IoT \n",
      "architecture to learn about communication, identity, capture, ingestion, storage, \n",
      "transformation, analytics, and presentation with Azure IoT.\n",
      "Connectivity\n",
      "IoT Hub provides all the important protocol suites for devices to connect to IoT hubs.  \n",
      "It offers:\n",
      "---------------\n",
      "• HTTPS: The HyperText Transport Protocol Secure method uses certificates \n",
      "consisting of a pair of keys, known as private-public keys, that are used to \n",
      "encrypt and decrypt data between a device and IoT Hub. It provides one-way \n",
      "communication from a device to the cloud.\n",
      "• AMQP: AMQP is an industry standard for sending and receiving messages between \n",
      "applications. It provides a rich infrastructure for the security and reliability of \n",
      "messages, and that is one of the reasons why it is quite widely used within the IoT \n",
      "space. It provides both device-to-Hub as well as Hub-to-device capabilities, and \n",
      "devices can use it to authenticate using Claims-Based Security (CBS) or Simple \n",
      "Authentication and Security Layer (SASL). It is used primarily in scenarios where \n",
      "there are field gateways, and a single identity related to multiple devices can \n",
      "transmit telemetry data to the cloud.\n",
      "---------------\n",
      "Azure IoT | 585\n",
      "• MQTT: MQTT is an industry standard for sending and receiving messages between \n",
      "applications. It provides both device-to-Hub as well as Hub-to-device capabilities. \n",
      "It is used primarily in scenarios where each device has its own identity and \n",
      "authenticates directly with the cloud.\n",
      "In the next section, we will discuss identity and how devices are authenticated.\n",
      "Identity\n",
      "IoT Hub provides services for authenticating devices. It offers an interface for \n",
      "generating unique identity hashes for each device. When devices send messages \n",
      "containing a hash, IoT Hub can authenticate them, after checking in its own database \n",
      "for the existence of such a hash. Now let's see how data is captured.\n",
      "Capture\n",
      "Azure provides IoT gateways, which enable devices that do not comply with IoT Hub \n",
      "to be adapted and to push data. Local or intermediary gateways can be deployed near \n",
      "devices in such a way that multiple devices can connect to a single gateway to capture\n",
      "---------------\n",
      "and send their information. Similarly, multiple clusters of devices with local gateways \n",
      "can also be deployed. There can be a cloud gateway deployed on the cloud itself, which \n",
      "is capable of capturing and accepting data from multiple sources and ingesting it for IoT \n",
      "Hub. As discussed previously, we need to ingest the data that we capture. In the next \n",
      "section, you will learn about ingestion with IoT Hub.\n",
      "Ingestion\n",
      "An IoT hub can be a single point of contact for devices and other applications. In other \n",
      "words, the ingestion of IoT messages is the responsibility of the IoT Hub service. There \n",
      "are other services, such as Event Hubs and the Service Bus messaging infrastructure, \n",
      "that can ingest incoming messages; however, the benefits and advantages of using \n",
      "IoT Hub to ingest IoT data far outweigh those of using Event Hubs and Service \n",
      "Bus messaging. In fact, IoT Hub was made specifically for the purpose of ingesting\n",
      "---------------\n",
      "IoT messages within the Azure ecosystem so that other services and components \n",
      "can act on them. The ingested data is stored to storage. Before we do any kind of \n",
      "transformation or analytics, let's explore the role of storage in the workflow in the next \n",
      "section.\n",
      "---------------\n",
      "586 | Designing IoT solutions\n",
      "Storage\n",
      "Azure provides multiple ways of storing messages from IoT devices. These include \n",
      "storing relational data, schema-less NoSQL data, and blobs:\n",
      "• SQL Database: SQL Database provides storage for relational data, JSON, and XML \n",
      "documents. It provides a rich SQL-query language and it uses a full-blown SQL \n",
      "server as a service. Data from devices can be stored in SQL databases if it is well \n",
      "defined and the schema will not need to undergo changes frequently.\n",
      "• Azure Storage: Azure Storage provides Table and Blob storage. Table storage \n",
      "helps in storing data as entities, where the schema is not important. Blobs help in \n",
      "storing files in containers as blobs.\n",
      "• Cosmos DB: Cosmos DB is a full-blown enterprise-scale NoSQL database. It is \n",
      "available as a service that is capable of storing schema-less data. It is a globally \n",
      "distributed database that can span continents, providing high availability and \n",
      "scalability of data.\n",
      "---------------\n",
      "• External data sources: Apart from Azure services, customers can bring or use \n",
      "their own data stores, such as a SQL server on Azure virtual machines, and can use \n",
      "them to store data in a relational format.\n",
      "The next section is on transformation and analytics.\n",
      "Transformation and analytics\n",
      "Azure provides multiple resources to execute jobs and activities on ingested data. Some \n",
      "of them are listed here:\n",
      "• Data Factory: Azure Data Factory is a cloud-based data integration service that \n",
      "allows us to create data-driven workflows in the cloud for orchestrating and \n",
      "automating data movement and data transformation. Azure Data Factory helps \n",
      "to create and schedule data-driven workflows (called pipelines) that can ingest \n",
      "data from disparate data stores; process and transform data by using compute \n",
      "services such as Azure HDInsight, Hadoop, Spark, Azure Data Lake Analytics, \n",
      "Azure Synapse Analytics, and Azure Machine Learning; and publish output data\n",
      "---------------\n",
      "to a data warehouse for Business Intelligence (BI) applications rather than a \n",
      "traditional Extract-Transform-Load (ETL) platform.\n",
      "• Azure Databricks: Databricks provides a complete, managed, end-to-end Spark \n",
      "environment. It can help in the transformation of data using Scala and Python. It \n",
      "also provides a SQL library to manipulate data using traditional SQL syntax. It is \n",
      "more performant than Hadoop environments.\n",
      "---------------\n",
      "Azure IoT | 587\n",
      "• Azure HDInsight: Microsoft and Hortonworks have come together to help \n",
      "companies by offering a big data analytics platform with Azure. HDInsight \n",
      "is a high-powered, fully managed cloud service environment powered by \n",
      "Apache Hadoop and Apache Spark using Microsoft Azure HDInsight. It helps in \n",
      "accelerating workloads seamlessly using Microsoft and Hortonworks' industry-\n",
      "leading big data cloud service.\n",
      "• Azure Stream Analytics: This is a fully managed, real-time, data analytics service \n",
      "that helps in performing computation and transformation on streaming data. \n",
      "Stream Analytics can examine high volumes of data flowing from devices or \n",
      "processes, extract information from the data stream, and look for patterns, trends, \n",
      "and relationships.\n",
      "• Machine Learning: Machine learning is a data science technique that allows \n",
      "computers to use existing data to forecast future behaviors, outcomes, and trends.\n",
      "---------------\n",
      "Using machine learning, computers learn behaviors based on the model we create. \n",
      "Azure Machine Learning is a cloud-based predictive analytics service that makes \n",
      "it possible to quickly create and deploy predictive models as analytics solutions. \n",
      "It provides a ready-to-use library of algorithms to create models on an internet-\n",
      "connected PC and deploy predictive solutions quickly.\n",
      "• Azure Synapse Analytics: Formerly known as Azure Data Warehouse. Azure \n",
      "Synapse Analytics provides analytics services that are ideal for enterprise data \n",
      "warehousing and big data analytics. It supports direct streaming ingestion, which \n",
      "can be integrated with Azure IoT Hub.\n",
      "Now that you are familiar with the transformation and analytics tools used in Azure for \n",
      "the data ingested by IoT devices, let's go ahead and learn about how this data can be \n",
      "presented.\n",
      "Presentation\n",
      "After appropriate analytics have been conducted on data, the data should be presented\n",
      "---------------\n",
      "to stakeholders in a format that is consumable by them. There are numerous ways in \n",
      "which insights from data can be presented. These include presenting data through web \n",
      "applications that are deployed using Azure App Service, sending data to notification \n",
      "hubs that can then notify mobile applications, and more. However, the ideal approach \n",
      "for presenting and consuming insights is using Power BI reports and dashboards. \n",
      "Power BI is a Microsoft visualization tool that is used to render dynamic reports and \n",
      "dashboards on the internet.\n",
      "---------------\n",
      "588 | Designing IoT solutions\n",
      "To conclude, Azure IoT is closely aligned with the basic concepts of IoT architecture. \n",
      "It follows the same process; however, Azure gives us the freedom to choose different \n",
      "services and dependencies based on our requirements. In the next section, we will \n",
      "focus on Azure IoT Hub, a service hosted in the cloud and completely managed by \n",
      "Azure.\n",
      "Azure IoT Hub\n",
      "IoT projects are generally complex in nature. The complexity arises because of the \n",
      "high volume of devices and data. Devices are embedded across the world, for example, \n",
      "monitoring and auditing devices that are used to store data, transform and analyze \n",
      "petabytes of data, and finally take actions based on insights. Moreover, these projects \n",
      "have long gestation periods, and their requirements keep changing because of \n",
      "timelines.\n",
      "If an enterprise wants to embark on a journey with an IoT project sooner rather than \n",
      "later, then it will quickly realize that the problems we have mentioned are not easily\n",
      "---------------\n",
      "solved. These projects require enough hardware in terms of computing and storage to \n",
      "cope, and services that can work with high volumes of data.\n",
      "IoT Hub is a platform that is built to enable the faster, better, and easier delivery of IoT \n",
      "projects. It provides all the necessary features and services, including the following:\n",
      "• Device registration\n",
      "• Device connectivity\n",
      "• Field gateways\n",
      "• Cloud gateways\n",
      "• Implementation of industry protocols, such as AMQP and the MQTT protocol\n",
      "• A hub for storing incoming messages\n",
      "• The routing of messages based on message properties and content\n",
      "• Multiple endpoints for different types of processing\n",
      "• Connectivity to other services on Azure for real-time analytics and more\n",
      "We have covered an overview of Azure IoT Hub, so let's take a deep dive to understand \n",
      "more about the protocols and how the devices are registered with Azure IoT Hub.\n",
      "---------------\n",
      "Azure IoT Hub | 589\n",
      "Protocols\n",
      "Azure IoT Hub natively supports communication over the MQTT, AMQP, and HTTP \n",
      "protocols. In some cases, devices or field gateways might not be able to use one of \n",
      "these standard protocols and will require protocol adaptation. In such cases, custom \n",
      "gateways can be deployed. The Azure IoT protocol gateway can enable protocol \n",
      "adaptation for IoT Hub endpoints by bridging the traffic to and from IoT Hub. In the \n",
      "next section, we will discuss how devices are registered with Azure IoT Hub.\n",
      "Device registration\n",
      "Devices should be registered before they can send messages to IoT Hub. The \n",
      "registration of devices can be done manually using the Azure portal or it can be \n",
      "automated using the IoT Hub SDK. Azure also provides sample simulation applications, \n",
      "through which it becomes easy to register virtual devices for development and testing \n",
      "purposes. There is also a Raspberry Pi online simulator that can be used as a virtual\n",
      "---------------\n",
      "device, and then, obviously, there are other physical devices that can be configured to \n",
      "connect to IoT Hub.\n",
      "If you want to simulate a device from a local PC that is generally used for development \n",
      "and testing purposes, then there are tutorials available in the Azure documentation in \n",
      "multiple languages. These are available at https:/ /docs.microsoft.com/azure/iot-hub/\n",
      "iot-hub-get-started-simulated.\n",
      "The Raspberry Pi online simulator is available at https:/ /docs.microsoft.com/azure/\n",
      "iot-hub/iot-hub-raspberry-pi-web-simulator-get-started, and for physical devices \n",
      "that need to be registered with IoT Hub, the steps given at https:/ /docs.microsoft.com/\n",
      "azure/iot-hub/iot-hub-get-started-physical should be used.\n",
      "To manually add a device using the Azure portal, IoT Hub provides the IoT devices \n",
      "menu, which can be used to configure a new device. Selecting the New option will let \n",
      "you create a new device as shown in Figure 17.4:\n",
      "---------------\n",
      "590 | Designing IoT solutions\n",
      "Figure 17.4: Adding a device via the Azure portal\n",
      "After the device identity is created, a primary key connection string for IoT Hub should \n",
      "be used in each device to connect to it:\n",
      "Figure 17.5: Creating connecting strings for each device\n",
      "At this stage, the device has been registered with IoT Hub, and our next mission is to \n",
      "make communication happen between the device and IoT Hub. The next section will \n",
      "give you a good understanding of how message management is done.\n",
      "Message management\n",
      "After devices are registered with IoT Hub, they can start interacting with it. Message \n",
      "management refers to how the communication or interaction between the IoT device \n",
      "and IoT Hub is done. This interaction could be from the device to the cloud, or from the \n",
      "cloud to the device.\n",
      "---------------\n",
      "Azure IoT Hub | 591\n",
      "Device-to-cloud messaging\n",
      "One of the best practices that must be followed in this communication is that although \n",
      "the device might be capturing a lot of information, only data that is of any importance \n",
      "should be transmitted to the cloud. The size of the message is very important in IoT \n",
      "solutions since IoT solutions generally have very high volumes of data. Even 1 KB of \n",
      "extra data flowing in can result in a GB of storage and processing wasted. Each message \n",
      "has properties and payloads; properties define the metadata for the message. This \n",
      "metadata contains data about the device, identification, tags, and other properties that \n",
      "are helpful in routing and identifying messages.\n",
      "Devices or cloud gateways should connect to IoT Hub to transfer data. IoT Hub provides \n",
      "public endpoints that can be utilized by devices to connect and send data. IoT Hub \n",
      "should be considered as the first point of contact for back-end processing. IoT Hub is\n",
      "---------------\n",
      "capable of further transmission and routing of these messages to multiple services. By \n",
      "default, the messages are stored in event hubs. Multiple event hubs can be created for \n",
      "different kinds of messages. The built-in endpoints used by the devices to send and \n",
      "receive data can be seen in the Built-in endpoints blade in IoT Hub. Figure 17.6 shows \n",
      "how you can find the built-in endpoints: \n",
      "Figure 17.6: Creating multiple event hubs\n",
      "---------------\n",
      "592 | Designing IoT solutions\n",
      "Messages can be routed to different endpoints based on the message header and body \n",
      "properties, as shown in Figure 17.7:\n",
      "Figure 17.7: Adding a new route to different endpoints\n",
      "Messages in an IoT hub stay there for seven days by default, and their size can go up to \n",
      "256 KB.\n",
      "There is a sample simulator provided by Microsoft for simulating sending messages to \n",
      "the cloud. It is available in multiple languages; the C# version can be viewed at https:/ /\n",
      "docs.microsoft.com/azure/iot-hub/iot-hub-csharp-csharp-c2d.\n",
      "Cloud-to-device messaging\n",
      "IoT Hub is a managed service providing a bi-directional messaging infrastructure. \n",
      "Messages can be sent from the cloud to devices, and then based on the message, the \n",
      "devices can act on them.\n",
      "There are three types of cloud-to-device messaging patterns:\n",
      "• Direct methods require immediate confirmation of results. Direct methods are \n",
      "often used for the interactive control of devices, such as opening and closing\n",
      "---------------\n",
      "garage shutters. They follow the request-response pattern.\n",
      "• Setting up device properties using Azure IoT provides device twin properties. \n",
      "For example, you can set the telemetry-sending interval to 30 minutes. Device \n",
      "twins are JSON documents that store device state information (such as metadata, \n",
      "configurations, and conditions). An IoT hub persists a device twin for each device \n",
      "in the IoT hub.\n",
      "---------------\n",
      "Azure IoT Hub | 593\n",
      "• Cloud-to-device messages are used for one-way notifications to the device app. \n",
      "This follows the fire-and-forget pattern.\n",
      "In every organization, security is a big concern, and even in the case of IoT devices and \n",
      "data, this concern is still there. We will be discussing security in the next section.\n",
      "Security\n",
      "Security is an important aspect of IoT-based applications. IoT-based applications \n",
      "comprise devices that use the public internet for connectivity to back-end applications. \n",
      "Securing devices, back-end applications, and connectivity from malicious users and \n",
      "hackers should be considered a top priority for the success of these applications.\n",
      "Security in IoT\n",
      "IoT applications are primarily built around the internet, and security should play a \n",
      "major role in ensuring that a solution is not compromised. Some of the most important \n",
      "security decisions affecting IoT architecture are listed here:\n",
      "• Regarding devices using HTTP versus HTTPS REST endpoints, REST endpoints\n",
      "---------------\n",
      "secured by certificates ensure that messages transferred from a device to the \n",
      "cloud and vice versa are well encrypted and signed. The messages should make no \n",
      "sense to an intruder and should be extremely difficult to crack.\n",
      "• If devices are connected to a local gateway, the local gateway should connect to \n",
      "the cloud using a secure HTTP protocol.\n",
      "• Devices should be registered to IoT Hub before they can send any messages.\n",
      "• The information passed to the cloud should be persisted into storage that is well \n",
      "protected and secure. Appropriate SAS tokens or connection strings that are \n",
      "stored in Azure Key Vault should be used for connection.\n",
      "• Azure Key Vault should be used to store all secrets, passwords, and credentials, \n",
      "including certificates.\n",
      "• Azure Security Center for IoT provides threat prevention and analysis for every \n",
      "device, IoT Edge, and IoT Hub across your IoT assets. We can build our own \n",
      "dashboards in Azure Security Center based on the security assessments. Some key\n",
      "---------------\n",
      "features include central management from Azure Security Center, adaptive threat \n",
      "protection, and intelligent threat detection. It's a best practice to consider Azure \n",
      "Security Center while implementing secured IoT solutions.\n",
      "Next, we are going to look at the scalability aspect of IoT Hub.\n",
      "---------------\n",
      "594 | Designing IoT solutions\n",
      "Scalability\n",
      "Scalability for IoT Hub is a bit different than for other services. In IoT Hub, there are \n",
      "two types of messages:\n",
      "• Incoming: Device-to-cloud messages\n",
      "• Outgoing: Cloud-to-device messages\n",
      "Both need to be accounted for in terms of scalability.\n",
      "IoT Hub provides a couple of configuration options during provision time to configure \n",
      "scalability. These options are also available post-provisioning and can be updated to \n",
      "better suit the solution requirements in terms of scalability.\n",
      "The scalability options that are available for IoT Hub are as follows:\n",
      "• The Stock Keeping Unit (SKU) edition, which is the size of IoT Hub\n",
      "• The number of units\n",
      "We will first look into the SKU edition option.\n",
      "The SKU edition\n",
      "The SKU in IoT Hub determines the number of messages a hub can handle per unit per \n",
      "day, and this includes both incoming and outgoing messages. There are four tiers, as \n",
      "follows:\n",
      "---------------\n",
      "• Free: This allows 8,000 messages per unit per day and allows both incoming \n",
      "and outgoing messages. A maximum of 1 unit can be provisioned. This edition \n",
      "is suitable for gaining familiarity and testing out the capabilities of the IoT Hub \n",
      "service.\n",
      "• Standard (S1): This allows 400,000 messages per unit per day and allows both \n",
      "incoming and outgoing messages. A maximum of 200 units can be provisioned. \n",
      "This edition is suitable for a small number of messages.\n",
      "• Standard (S2): This allows 6 million messages per unit per day and allows both \n",
      "incoming and outgoing messages. A maximum of 200 units can be provisioned. \n",
      "This edition is suitable for a large number of messages.\n",
      "• Standard (S3): This allows 300 million messages per unit per day and allows both \n",
      "incoming and outgoing messages. A maximum of 10 units can be provisioned. This \n",
      "edition is suitable for a very large number of messages.\n",
      "The upgrade and scaling options are available in the Azure portal, under the Pricing\n",
      "---------------\n",
      "and scale blade of IoT Hub. The options will be presented to you as shown in Figure 17.8:\n",
      "---------------\n",
      "Azure IoT Hub | 595\n",
      "Figure 17.8: Choosing a pricing and scale tier\n",
      "You may notice that the Standard S3 tier allows a maximum of only 10 units, compared \n",
      "to other standard units that allow 200 units. This is directly related to the size of the \n",
      "compute resources that are provisioned to run IoT services. The size and capability \n",
      "of virtual machines for Standard S3 are significantly higher compared to other tiers, \n",
      "where the size remains the same.\n",
      "Units\n",
      "Units define the number of instances of each SKU running behind the service. For \n",
      "example, 2 units of the Standard S1 SKU tier will mean that the IoT hub is capable of \n",
      "handling 400K * 2 = 800K messages per day.\n",
      "More units will increase the scalability of the application. Figure 17.9 is from the Pricing \n",
      "and scale blade of IoT Hub, where you can see the current pricing tier and number of \n",
      "units:\n",
      "Figure 17.9: Options to adjust or migrate IoT Hub units\n",
      "---------------\n",
      "596 | Designing IoT solutions\n",
      "One of the booming services in Azure IoT Hub right now is Azure IoT Edge, which is a \n",
      "completely managed service built on Azure IoT Hub. We will be exploring what Azure \n",
      "IoT Edge is in the next section.\n",
      "Azure IoT Edge\n",
      "Microsoft Azure IoT Edge leverages edge compute to implement IoT solutions. Edge \n",
      "compute refers to the compute resources that are available on your on-premises \n",
      "network, right at the end of your network, where the public internet starts. This can be \n",
      "deployed on your main network or a guest network with firewall isolation.\n",
      "Azure IoT Edge comprises the IoT Edge runtime, which needs to be installed on a \n",
      "computer or a device. Docker will be installed on the computer; the computer can run \n",
      "either Windows or Linux. The role of Docker is to run the IoT Edge modules.\n",
      "Azure IoT Edge relies on the hybrid cloud concept, where you can deploy and manage \n",
      "IoT solutions on on-premises hardware and easily integrate them with Microsoft Azure.\n",
      "---------------\n",
      "Microsoft provides comprehensive documentation for Azure IoT Edge, with quick-start \n",
      "templates and guidance on how to install the modules. The link to the documentation is \n",
      "https:/ /docs.microsoft.com/azure/iot-edge.\n",
      "In the next section, we will look at how infrastructure is managed in the case of Azure \n",
      "IoT Hub and how high availability is provided to customers.\n",
      "High availability\n",
      "IoT Hub is a platform as a service (PaaS) offering from Azure. Customers and users do \n",
      "not directly interact with the underlying number and size of virtual machines on which \n",
      "the IoT Hub service runs. Users decide on the region, the SKU of the IoT hub, and the \n",
      "number of units for their application. The rest of the configuration is determined and \n",
      "executed by Azure behind the scenes. Azure ensures that every PaaS service is highly \n",
      "available by default. It does so by ensuring that multiple virtual machines provisioned \n",
      "behind the service are on separate racks in the datacenter. It does this by placing those\n",
      "---------------\n",
      "virtual machines on an availability set and on a separate fault and update domain. This \n",
      "helps ensure high availability for both planned and unplanned maintenance. Availability \n",
      "sets take care of high availability at the datacenter level.\n",
      "In the next section, we will discuss Azure IoT Central.\n",
      "---------------\n",
      "Azure IoT Central | 597\n",
      "Azure IoT Central\n",
      "Azure IoT Central provides a platform to build enterprise-grade IoT applications to \n",
      "meet your business requirements in a secure, reliable, and scalable fashion. IoT Central \n",
      "eliminates the cost of developing, maintaining, and managing IoT solutions.\n",
      "IoT Central provides centralized management by which you can manage and monitor \n",
      "devices, device conditions, rule creation, and device data. In Figure 17.10, you can see \n",
      "some of the templates that are available in the Azure portal during the creation of IoT \n",
      "Central applications: \n",
      "Figure 17.10 Creating an Azure IoT Central application\n",
      "---------------\n",
      "598 | Designing IoT solutions\n",
      "Templates will give you a head start and you can customize them as per your \n",
      "requirements. This will save you a lot of time during the development phase.\n",
      "IoT Central offers a seven-day trial at the time of writing, and you can see the pricing \n",
      "for this service here: https:/ /azure.microsoft.com/pricing/details/iot-central/?rtc=1.\n",
      "Azure IoT Central is a boon for every organization that is developing IoT applications. \n",
      "Summary\n",
      "IoT is one of the biggest emerging technologies of this decade and it is already \n",
      "disrupting industries. Things that sounded impossible before are now suddenly \n",
      "possible. \n",
      "In this chapter, we explored IoT Hub and discussed the delivery of IoT solutions to \n",
      "the customer in a faster, better, and cheaper way than alternative solutions. We also \n",
      "covered how IoT can fast-track the entire development life cycle and help expedite \n",
      "time-to-market for companies. Finally, you learned about Azure IoT Edge and Azure IoT \n",
      "Central.\n",
      "---------------\n",
      "To help you effectively analyze ever-growing volumes of data, we will discuss Azure \n",
      "Synapse Analytics in the next chapter.\n",
      "---------------\n",
      "Azure Synapse Analytics is a groundbreaking evolution of Azure SQL Data Warehouse. \n",
      "Azure Synapse is a fully managed, integrated data analytics service that blends data \n",
      "warehousing, data integration, and big data processing with accelerating time to insight \n",
      "to form a single service.\n",
      "Azure Synapse \n",
      "Analytics for \n",
      "architects\n",
      "18\n",
      "---------------\n",
      "602 | Azure Synapse Analytics for architects\n",
      "In this chapter, we will explore Azure Synapse Analytics by covering the following \n",
      "topics:\n",
      "• An overview of Azure Synapse Analytics\n",
      "• Introduction to Synapse workspaces and Synapse Studio\n",
      "• Migrating from existing legacy systems to Azure Synapse Analytics\n",
      "• Migrating existing data warehouse schemas and data to Azure Synapse Analytics\n",
      "• Re-developing scalable ETL processes using Azure Data Factory\n",
      "• Common migration issues and resolutions\n",
      "• Security considerations\n",
      "• Tools to help migrate to Azure Synapse Analytics\n",
      "Azure Synapse Analytics\n",
      "Nowadays, with inexpensive storage and high elastic storage capacities, organizations \n",
      "are amassing more data than ever before. Architecting a solution to analyze such \n",
      "massive volumes of data to deliver meaningful insights about a business can be a \n",
      "challenge. One obstacle that many businesses face is the need to manage and maintain \n",
      "two types of analytics systems:\n",
      "---------------\n",
      "• Data warehouses: These provide critical insights about the business.\n",
      "• Data lakes: These provide meaningful insights about customers, products, \n",
      "employees, and processes through various analytics methodologies.\n",
      "Both of these analytics systems are critical to businesses, yet they operate \n",
      "independently of one another. Meanwhile, businesses need to gain insights from all \n",
      "their organizational data in order to stay competitive and to innovate processes to \n",
      "obtain better results.\n",
      "For architects who need to build their own end-to-end data pipelines, the following \n",
      "steps must be taken:\n",
      "1. Ingest data from various data sources.\n",
      "2. Load all these data sources into a data lake for further processing.\n",
      "3. Perform data cleaning over a range of different data structures and types.\n",
      "4. Prepare, transform, and model the data.\n",
      "5. Serve the cleansed data to thousands of users through BI tools and applications.\n",
      "---------------\n",
      "A common scenario for architects | 603\n",
      "Until now, each of these steps has required a different tool. Needless to say, with so \n",
      "many different services, applications, and tools available on the market, choosing the \n",
      "best-suited ones can be a daunting task.\n",
      "There are numerous services available for ingesting, loading, preparing, and serving \n",
      "data. There are countless services for data cleansing based on the developer's language \n",
      "of choice. Furthermore, some developers might prefer to use SQL, some might want to \n",
      "use Spark, while others might prefer to use code-free environments to transform data.\n",
      "Even after the seemingly proper collection of tools has been selected, there is often \n",
      "a steep learning curve for these tools. Additionally, architects could encounter \n",
      "unexpected logistical challenges in maintaining a data pipeline over dissimilar platforms \n",
      "and languages due to incompatibilities. With such a range of issues, implementing and\n",
      "---------------\n",
      "maintaining a cloud-based analytics platform can be a difficult task.\n",
      "Azure Synapse Analytics solves these problems and more. It simplifies the entire \n",
      "modern data warehouse pattern, allowing architects to focus on building end-to-end \n",
      "analytics solutions within a unified environment.\n",
      "A common scenario for architects\n",
      "One of the most common scenarios that an architect faces is having to conjure up a \n",
      "plan for migrating existing legacy data warehouse solutions to a modern enterprise \n",
      "analytics solution. With its limitless scalability and unified experience, Azure Synapse \n",
      "has become one of the top choices for many architects to consider. Later in this \n",
      "chapter, we will also discuss common architectural considerations for migrating from \n",
      "an existing legacy data warehouse solution to Azure Synapse Analytics.\n",
      "In the next section, we will provide a technical overview of the key features of Azure \n",
      "Synapse Analytics. Architects who are new to the Azure Synapse ecosystem will gain\n",
      "---------------\n",
      "the necessary knowledge about Synapse after reading this chapter.\n",
      "An overview of Azure Synapse Analytics\n",
      "Azure Synapse Analytics enables data professionals to build end-to-end analytics \n",
      "solutions while leveraging a unified experience. It delivers rich functionalities for SQL \n",
      "developers, serverless on-demand querying, machine learning support, the ability to \n",
      "embed Spark natively, collaborative notebooks, and data integration within a single \n",
      "service. Developers can choose from a variety of supported languages (for example, C#, \n",
      "SQL, Scala, and Python) through different engines.\n",
      "---------------\n",
      "604 | Azure Synapse Analytics for architects\n",
      "Some of the main capabilities of Azure Synapse Analytics include:\n",
      "• SQL Analytics with pools (fully provisioned) and on-demand (serverless).\n",
      "• Spark with full support for Python, Scala, C#, and SQL.\n",
      "• Data Flow with code-free big data transformation experience.\n",
      "• Data integration and orchestration to integrate data and operationalize code \n",
      "development.\n",
      "• A cloud-native version of Hybrid Transactional/Analytical Processing (HTAP), \n",
      "delivered by Azure Synapse Link.\n",
      "To access all of the aforementioned capabilities, Azure Synapse Studio provides a single \n",
      "unified web UI.\n",
      "This single integrated data service is advantageous to enterprises as it accelerates the \n",
      "delivery of BI, AI, machine learning, Internet of Things, and intelligent applications.\n",
      "Azure Synapse Analytics can derive and deliver insights from all your data residing in \n",
      "the data warehouse and big data analytics systems at lightning-fast speeds. It enables\n",
      "---------------\n",
      "data professionals to use familiar languages, such as SQL, to query both relational \n",
      "and non-relational databases at petabyte scale. In addition, advanced features such as \n",
      "limitless concurrency, intelligent workload management, and workload isolation help \n",
      "optimize the performance of all queries for mission-critical workloads.\n",
      "What is workload isolation?\n",
      "One of the key features of running enterprise data warehouses at scale is workload \n",
      "isolation. This is the ability to guarantee resource reservations within a compute cluster \n",
      "so that multiple teams can work on the data without getting in each other's way, as \n",
      "illustrated in Figure 18.1:\n",
      "Figure 18.1: Example of workload isolation\n",
      "Workload Isolation\n",
      "40%\n",
      "Data\n",
      "Warehouse\n",
      "60%\n",
      "Sales\n",
      "Marketing\n",
      "40%\n",
      "Local In-Memory + SSD Cache\n",
      "Compute 1000c DWU\n",
      "---------------\n",
      "An overview of Azure Synapse Analytics | 605\n",
      "You can create workload groups within a cluster by setting a couple of simple \n",
      "thresholds. These are automatically adjusted depending on the workload and \n",
      "the cluster, but they always guarantee a quality experience for users running the \n",
      "workloads. Refer to https:/ /techcommunity.microsoft.com/t5/data-architecture-blog/\n",
      "configuring-workload-isolation-in-azure-synapse-analytics/ba-p/1201739 to read \n",
      "more about configuring workload isolation in Azure Synapse Analytics.\n",
      "To fully appreciate the benefits of Azure Synapse, we will first take a look at Synapse \n",
      "workspaces and Synapse Studio.\n",
      "Introduction to Synapse workspaces and Synapse Studio\n",
      "At the heart of Azure Synapse is the workspace. The workspace is the top-level \n",
      "resource that comprises your analytics solution in a data warehouse. The Synapse \n",
      "workspace supports both relational and big data processing.\n",
      "Azure Synapse provides a unified web UI experience for data preparation, data\n",
      "---------------\n",
      "management, data warehousing, big data analytics, BI, and AI tasks known as Synapse \n",
      "Studio. Together with Synapse workspaces, Synapse Studio is an ideal environment for \n",
      "data engineers and data scientists to share and collaborate their analytics solutions, as \n",
      "shown in Figure 18.2:\n",
      "Figure 18.2: A Synapse workspace in Azure Synapse Studio\n",
      "The following sections highlight the capabilities, key features, platform details, and end \n",
      "user services of Synapse workspaces and Synapse Studio:\n",
      "---------------\n",
      "606 | Azure Synapse Analytics for architects\n",
      "Capabilities:\n",
      "• A fast, highly elastic, and secure data warehouse with industry-leading \n",
      "performance and security\n",
      "• The ability to explore Azure Data Lake Storage and data warehouses using familiar \n",
      "T-SQL syntax using SQL on-demand (serverless) and SQL queries\n",
      "• Apache Spark integrated with Azure Machine Learning\n",
      "• Hybrid data integration to accelerate data ingestion and the operationalization of \n",
      "the analytics process (ingest, prepare, transform, and serve)\n",
      "• Business report generation and serving with Power BI integration\n",
      "Key features:\n",
      "• Create and operationalize pipelines for data ingestion and orchestration.\n",
      "• Directly explore data in your Azure Data Lake Storage or data warehouse, as well \n",
      "as any external connections to the workspace, using Synapse Studio.\n",
      "• Write code using notebooks and T-SQL query editors.\n",
      "• Code-free data transformation tool, if you prefer not to write your own code.\n",
      "---------------\n",
      "• Monitor, secure, and manage your workspaces without leaving the environment.\n",
      "• Web-based development experience for the entire analytics solution.\n",
      "• The backup and restore feature in the Azure Synapse SQL pool allows restore \n",
      "points to be created to make it easy to recover or copy a data warehouse to a \n",
      "previous state.\n",
      "• The ability to run concurrent T-SQL queries through SQL pools across petabytes \n",
      "of data to serve BI tools and applications.\n",
      "• SQL on-demand provides serverless SQL queries for ease of exploration and \n",
      "data analysis in Azure Data Lake Storage without any setup or maintenance of \n",
      "infrastructure.\n",
      "• Meets the full range of analytics needs, from data engineering to data science, \n",
      "using a variety of languages, such as Python, Scala, C#, and Spark SQL.\n",
      "• Spark pools, which alleviate the complex setup and maintenance of clusters and \n",
      "simplify the development of Spark applications and usage of Spark notebooks.\n",
      "---------------\n",
      "An overview of Azure Synapse Analytics | 607\n",
      "• Offers deep integration between Spark and SQL, allowing data engineers to \n",
      "prepare data in Spark, write the processed results in SQL Pool, and use any \n",
      "combination of Spark with SQL for data engineering and analysis, with built-in \n",
      "support for Azure Machine Learning.\n",
      "• Highly scalable, hybrid data integration capability that accelerates data ingestion \n",
      "and operationalization through automated data pipelines.\n",
      "• Provides a friction-free integrated service with unified security, deployment, \n",
      "monitoring, and billing.\n",
      "Platform\n",
      "• Supports both provisioned and serverless compute. Examples of provisioned \n",
      "compute include SQL compute and Spark compute.\n",
      "• Provisioned compute allows teams to segment their compute resources so that \n",
      "they can control cost and usage to better align with their organizational structure.\n",
      "• Serverless compute, on the other hand, allows teams to use the service\n",
      "---------------\n",
      "on-demand without provisioning or managing any underlying infrastructure.\n",
      "• Deep integration between Spark and SQL engines.\n",
      "In the following section, we will cover the other features of Azure Synapse, including \n",
      "Apache Spark for Synapse, Synapse SQL, SQL on-demand, Synapse pipelines, and Azure \n",
      "Synapse Link for Cosmos DB.\n",
      "Apache Spark for Synapse\n",
      "For customers who want Apache Spark, Azure Synapse has first-party support through \n",
      "Azure Databricks and is fully managed by Azure. The latest version of Apache Spark \n",
      "will automatically be made available to users, along with all security patches. You can \n",
      "quickly create notebooks with your choice of language, such as Python, Scala, Spark \n",
      "SQL, and .NET for Spark.\n",
      "If you use Spark within Azure Synapse Analytics, it is provided as a Software as a \n",
      "Service offering. For example, you can use Spark without setting up or managing your \n",
      "own services, such as a virtual network. Azure Synapse Analytics will take care of the\n",
      "---------------\n",
      "underlying infrastructure for you. This allows you to use Spark immediately in your \n",
      "Azure Synapse Analytics environment.\n",
      "In the next section, we will explore Synapse SQL.\n",
      "---------------\n",
      "608 | Azure Synapse Analytics for architects\n",
      "Synapse SQL\n",
      "Synapse SQL allows the use of T-SQL to query and analyze data. There are two models \n",
      "to choose from:\n",
      "1. Fully provisioned model\n",
      "2. SQL on-demand (serverless) model\n",
      "SQL on-demand\n",
      "SQL on-demand provides serverless SQL queries. This allows easier exploration \n",
      "and data analysis in Azure Data Lake Storage without any setup or infrastructure \n",
      "maintenance:\n",
      "Table 18.1: Comparison between different infrastructures\n",
      "Key Features:\n",
      "• Analysts can focus on analyzing the data without worrying about managing any \n",
      "infrastructure.\n",
      "• Customers can benefit from a simple and flexible pricing model, as they only pay \n",
      "for what they use.\n",
      "Traditional IT IaaS PaaS ServerlessS aaS\n",
      "Application Application ApplicationA pplication Application\n",
      "Data Data Data Data Data\n",
      "Runtime Runtime Runtime Runtime Runtime\n",
      "MiddlewareM iddlewareM iddlewareM iddlewareM iddleware\n",
      "OS OS OS OS OS\n",
      "Virtualization Virtualization Virtualization Virtualization Virtualization\n",
      "---------------\n",
      "Servers Servers Servers Servers Servers\n",
      "StorageS torage StorageS torage Storage\n",
      "Networking Networking Networking Networking Networking\n",
      "You Manage\n",
      "---------------\n",
      "An overview of Azure Synapse Analytics | 609\n",
      "• It uses the familiar T-SQL language syntax and the best SQL Query Optimizer on \n",
      "the market. The SQL Query Optimizer is the brain behind the query engine.\n",
      "• You can easily scale your compute and storage, independently of one another, as \n",
      "your needs grow.\n",
      "• Seamlessly integrate with SQL Analytics Pool and Spark via metadata sync and \n",
      "native connectors.\n",
      "Synapse pipelines\n",
      "Synapse pipelines allow developers to build end-to-end workflows for data movement \n",
      "and data processing scenarios. Azure Synapse Analytics uses the Azure Data Factory \n",
      "(ADF) technology to provide data integration features. The key features of ADF that \n",
      "are essential to the modern data warehouse pipeline are available in Azure Synapse \n",
      "Analytics. All these features are wrapped with a common security model, Role-Based \n",
      "Access Control (RBAC), in the Azure Synapse Analytics workspace.\n",
      "Figure 18.3 shows an example of a data pipeline and the activities from ADF that are\n",
      "---------------\n",
      "directly integrated inside the Azure Synapse Analytics environment:\n",
      "Figure 18.3: Data pipelines in Azure Synapse Analytics\n",
      "---------------\n",
      "610 | Azure Synapse Analytics for architects\n",
      "Key Features:\n",
      "• Integrated platform services for management, security, monitoring, and metadata \n",
      "management.\n",
      "• Native integration between Spark and SQL. Use a single line of code to read and \n",
      "write with Spark from/into SQL analytics.\n",
      "• The ability to create a Spark table and query it instantaneously with SQL Analytics \n",
      "without defining a schema.\n",
      "• \"Key-free\" environment. With Single Sign-On and Azure Active Directory pass-\n",
      "through, no key or login is needed to interact with Azure Data Lake Storage \n",
      "(ADLS)/databases.\n",
      "In the next section, we will cover Azure Synapse Link for Cosmos DB.\n",
      "Azure Synapse Link for Cosmos DB\n",
      "Azure Synapse Link is a cloud-native version of HTAP. It is an extension of Azure \n",
      "Synapse. As we have learned earlier, Azure Synapse is a single managed service for \n",
      "performing analytics over data lakes and data warehouses, using both serverless \n",
      "and provisioned compute. With Azure Synapse Link, this reach can be extended to\n",
      "---------------\n",
      "operational data sources as well.\n",
      "Azure Synapse Link eliminates the bottleneck that is found in traditional operational \n",
      "and analytical systems. Azure Synapse makes this possible by separating compute \n",
      "from storage across all of its data services. On the transactional side, Cosmos DB is \n",
      "a high-performance, geo-replicated, multi-model database service. On the analytics \n",
      "side, Azure Synapse provides limitless scalability. You can scale the resources for \n",
      "transactions and for analytics independently. Together, this makes cloud-native HTAP \n",
      "a reality. As soon as the user indicates what data in Cosmos DB they wish to make \n",
      "available for analytics, the data becomes available in Synapse. It takes the operational \n",
      "data you wish to analyze and automatically maintains an analytics-oriented columnar \n",
      "version of it. As a result, any changes to the operational data in Cosmos DB are \n",
      "continuously updated to the Link data and Synapse.\n",
      "---------------\n",
      "The biggest benefit of using Azure Synapse Link is that it alleviates the need for \n",
      "scheduled batch processing or having to build and maintain operational pipelines.\n",
      "As mentioned previously, Azure Synapse is the most chosen platform by architects for \n",
      "migrating existing legacy data warehouse solutions to a modern enterprise analytics \n",
      "solution. In the next section, we will discuss common architectural considerations for \n",
      "migrating from an existing legacy data warehouse solution to Azure Synapse Analytics.\n",
      "---------------\n",
      "Migrating from existing legacy systems to Azure Synapse Analytics | 611\n",
      "Migrating from existing legacy systems to Azure Synapse Analytics\n",
      "Today, many organizations are migrating their legacy data warehouse solutions to Azure \n",
      "Synapse Analytics to gain the benefits of the high availability, security, speed, scalability, \n",
      "cost savings, and performance of Azure Synapse.\n",
      "For companies running legacy data warehouse systems such as Netezza, the situation \n",
      "is even more dire because IBM has announced the end of support for Netezza (https:/ /\n",
      "www.ibm.com/support/pages/end-support-dates-netezza-5200-netezza-8x50z-\n",
      "series-and-netezza-10000-series-appliances).\n",
      "Many decades ago, some companies chose Netezza to manage and analyze large \n",
      "volumes of data. Today, as technologies evolve, the benefits of having a cloud-based \n",
      "data warehouse solution far outweigh the on-premises counterparts. Azure Synapse is a \n",
      "limitless cloud-based analytics service with unmatched time to insight that accelerates\n",
      "---------------\n",
      "the delivery of BI, AI, and intelligent applications for enterprises. With its multi-cluster \n",
      "and separate compute and storage architecture, Azure Synapse can be scaled instantly \n",
      "in ways not possible with legacy systems such as Netezza.\n",
      "This section covers the architectural considerations and high-level methodology \n",
      "for planning, preparing, and executing a successful migration of an existing legacy \n",
      "data warehouse system to Azure Synapse Analytics. Whenever appropriate, specific \n",
      "examples and references to Netezza will be given. This chapter is not intended to be a \n",
      "comprehensive step-by-step manual for migration, but rather a practical overview to \n",
      "help with your migration planning and project scoping.\n",
      "This chapter also identifies some of the common migration issues and possible \n",
      "resolutions. It also provides technical details on the differences between Netezza \n",
      "and Azure Synapse Analytics. They should be taken into consideration as part of your \n",
      "migration plan.\n",
      "---------------\n",
      "Why you should migrate your legacy data warehouse to Azure Synapse \n",
      "Analytics\n",
      "By migrating to Azure Synapse Analytics, companies with legacy data warehouse \n",
      "systems can take advantage of the latest innovations in cloud technologies and delegate \n",
      "tasks such as infrastructure maintenance and platform upgrading to Azure.\n",
      "Customers who have migrated to Azure Synapse are already reaping many of its \n",
      "benefits, including the following.\n",
      "---------------\n",
      "612 | Azure Synapse Analytics for architects\n",
      "Performance\n",
      "Azure Synapse Analytics offers best-of-breed relational database performance by using \n",
      "techniques such as Massively Parallel Processing (MPP) and automatic in-memory \n",
      "caching. For more information, please review the Azure Synapse Analytics architecture \n",
      "(https:/ /docs.microsoft.com/azure/synapse-analytics/sql-data-warehouse/massively-\n",
      "parallel-processing-mpp-architecture).\n",
      "Speed\n",
      "Data warehousing is process intensive. It involves data ingestion, transforming data, \n",
      "cleansing data, aggregating data, integrating data, and producing data visualization \n",
      "and reports. The many processes involved in moving data from original sources to a \n",
      "data warehouse are complex and interdependent. A single bottleneck can slow the \n",
      "entire pipeline and an unexpected spike in data volume amplifies the need for speed. \n",
      "When timeliness of data matters, Azure Synapse Analytics meets the demand for fast \n",
      "processing.\n",
      "Improved security and compliance\n",
      "---------------\n",
      "Azure is a globally available, highly scalable, secure cloud platform. It offers many \n",
      "security features, including Azure Active Directory, RBAC, managed identities, and \n",
      "managed private endpoints. Azure Synapse Analytics, which resides inside the Azure \n",
      "ecosystem, inherits all of the aforementioned benefits.\n",
      "Elasticity and cost efficiencies\n",
      "In a data warehouse, the demands for workload processing can fluctuate. At times, \n",
      "these fluctuations can vary drastically between peaks and valleys. For example, sudden \n",
      "spikes in sales data volumes can occur during holiday seasons. Cloud elasticity allows \n",
      "Azure Synapse to quickly increase and decrease its capacity according to demand with \n",
      "no impact upon infrastructure availability, stability, performance, and security. Best of \n",
      "all, you only pay for your actual usage.\n",
      "Managed infrastructure\n",
      "Eliminating the overhead of data center management and operations for the data\n",
      "---------------\n",
      "warehouse allows companies to reallocate valuable resources to where value is \n",
      "produced and focus on using the data warehouse to deliver the best information and \n",
      "insight. This lowers the overall total cost of ownership and provides better cost control \n",
      "over your operating expenses.\n",
      "Scalability\n",
      "The volume of data in a data warehouse typically grows as time passes and as history \n",
      "is collected. Azure Synapse Analytics can scale to match this growth by incrementally \n",
      "adding resources as data and workload increase.\n",
      "---------------\n",
      "Migrating from existing legacy systems to Azure Synapse Analytics | 613\n",
      "Cost savings\n",
      "Running an on-premises legacy datacenter is expensive (considering the costs of \n",
      "servers and hardware, networking, physical room space, electricity, cooling, and \n",
      "staffing). These expenses can be substantially minimized with Azure Synapse Analytics. \n",
      "With the separation of the compute and storage layers, Azure Synapse offers a very \n",
      "lucrative price-performance ratio.\n",
      "Azure Synapse Analytics provides you with true pay-as-you-go cloud scalability without \n",
      "the need for complicated reconfiguration as your data or workloads grow.\n",
      "Now that you have learned why it is beneficial to migrate to Azure Synapse Analytics, \n",
      "we will begin our discussion of the migration process.\n",
      "The three-step migration process\n",
      "A successful data migration project starts with a well-designed plan. An effective plan \n",
      "accounts for the many components that need to be considered, paying particular\n",
      "---------------\n",
      "attention to architecture and data preparation. The following is the three-step \n",
      "migration process plan.\n",
      "Preparation\n",
      "• Define the scope of what is to be migrated.\n",
      "• Build an inventory of data and processes for migration.\n",
      "• Define the data model changes (if any).\n",
      "• Define the source data extraction mechanism.\n",
      "• Identify suitable Azure (and third-party) tools and services to be used.\n",
      "• Train staff early on the new platform.\n",
      "• Set up the Azure target platform.\n",
      "Migration\n",
      "• Start small and simple.\n",
      "• Automate wherever possible.\n",
      "• Leverage Azure built-in tools and features to reduce migration effort.\n",
      "• Migrate metadata for tables and views.\n",
      "• Migrate historical data to be maintained.\n",
      "• Migrate or refactor stored procedures and business processes.\n",
      "• Migrate or refactor ETL/ELT incremental load processes.\n",
      "---------------\n",
      "614 | Azure Synapse Analytics for architects\n",
      "Post-migration\n",
      "• Monitor and document all stages of the process.\n",
      "• Use the experience gained to build a template for future migrations.\n",
      "• Re-engineer the data model if required.\n",
      "• Test applications and query tools.\n",
      "• Benchmark and optimize query performance.\n",
      "Next, we will talk about the two types of migration strategies.\n",
      "The two types of migration strategies\n",
      "Architects should begin migration planning by assessing the existing data warehouse to \n",
      "determine which migration strategy works best for their situation. There are two types \n",
      "of migration strategies to consider.\n",
      "Lift and Shift strategy\n",
      "For the lift and shift strategy, the existing data model is migrated unchanged to the \n",
      "new Azure Synapse Analytics platform. This is done to minimize the risk and the time \n",
      "required for migration by reducing the scope of changes to the minimum.\n",
      "Lift and shift is a good strategy for legacy data warehouse environments such as\n",
      "---------------\n",
      "Netezza where any one of the following conditions applies:\n",
      "• A single data mart is to be migrated.\n",
      "• The data is already in a well-designed star or snowflake schema.\n",
      "• There are immediate time and cost pressures to move to a modern cloud \n",
      "environment.\n",
      "Redesign strategy\n",
      "In scenarios where the legacy data warehouse has evolved over time, it might be \n",
      "essential to re-engineer it to maintain the optimum performance levels or support new \n",
      "types of data. This could include a change in the underlying data model.\n",
      "To minimize risk, it is recommended to migrate first using the lift and shift strategy and \n",
      "then gradually modernize the data warehouse data model on Azure Synapse Analytics \n",
      "using the redesign strategy. A complete change in data model will increase risks \n",
      "because it will impact source-to-data warehouse ETL jobs and downstream data marts.\n",
      "In the next section, we will offer some recommendations on how to reduce the \n",
      "complexity of your existing legacy data warehouse before migrating.\n",
      "---------------\n",
      "Migrating from existing legacy systems to Azure Synapse Analytics | 615\n",
      "Reducing the complexity of your existing legacy data warehouse before \n",
      "migrating\n",
      "In the previous section, we presented the two migration strategies. As a best practice, \n",
      "during the initial assessment step, be cognizant of any ways to simplify your existing \n",
      "data warehouse and document them. The goal is to reduce the complexity of your \n",
      "existing legacy data warehouse system before the migration to make the migration \n",
      "process easier.\n",
      "Here are some recommendations on how to reduce the complexity of your existing \n",
      "legacy data warehouse:\n",
      "• Remove and archive unused tables before migrating: Avoid migrating data that is \n",
      "no longer in use. This will help reduce the overall data volume to migrate.\n",
      "• Convert physical data marts to virtual data marts: Minimize what you have to \n",
      "migrate, reduce the total cost of ownership, and improve agility.\n",
      "---------------\n",
      "In the next section, we will take a closer look at why you should consider converting a \n",
      "physical data mart to a virtual data mart.\n",
      "Converting physical data marts to virtual data marts\n",
      "Prior to migrating your legacy data warehouse, consider converting your current \n",
      "physical data marts to virtual data marts. By using virtual data marts, you can eliminate \n",
      "physical data stores and ETL jobs for data marts without losing any functionality prior \n",
      "to migration. The goal here is to reduce the number of data stores to migrate, reduce \n",
      "copies of data, reduce the total cost of ownership, and improve agility. To achieve this, \n",
      "you will need to switch from physical to virtual data marts before migrating your data \n",
      "warehouse. We can consider this as a data warehouse modernization step prior to \n",
      "migration.\n",
      "Disadvantages of physical data marts\n",
      "• Multiple copies of the same data\n",
      "• Higher total cost of ownership\n",
      "• Difficult to change as ETL jobs are impacted\n",
      "---------------\n",
      "616 | Azure Synapse Analytics for architects\n",
      "Advantages of virtual data marts\n",
      "• Simplifies data warehouse architecture\n",
      "• No need to store copies of data\n",
      "• More agility\n",
      "• Lower total cost of ownership\n",
      "• Uses pushdown optimization to leverage the power of Azure Synapse Analytics\n",
      "• Easy to change\n",
      "• Easy to hide sensitive data\n",
      "In the next section, we will talk about how to migrate existing data warehouse schemas \n",
      "to Azure Synapse Analytics.\n",
      "Migrating existing data warehouse schemas to Azure Synapse Analytics\n",
      "Migrating the schemas of an existing legacy data warehouse involves the migration of \n",
      "existing staging tables, legacy data warehouse, and dependent data mart schemas.\n",
      "To help you understand the magnitude and scope of your schema migration, we \n",
      "recommend that you create an inventory of your existing legacy data warehouse and \n",
      "data mart.\n",
      "Here is a checklist to help you collect the necessary information:\n",
      "• Row counts\n",
      "• Staging, data warehouse, and data mart data size: tables and indexes\n",
      "---------------\n",
      "• Data compression ratios\n",
      "• Current hardware configuration\n",
      "• Tables (including partitions): identify small dimension tables\n",
      "• Data types\n",
      "• Views\n",
      "• Indexes\n",
      "• Object dependencies\n",
      "• Object usage\n",
      "---------------\n",
      "Migrating from existing legacy systems to Azure Synapse Analytics | 617\n",
      "• Functions: both out-of-the-box functions and User-Defined Functions (UDFs)\n",
      "• Stored procedures\n",
      "• Scalability requirements\n",
      "• Growth projections\n",
      "• Workload requirements: Concurrent users\n",
      "With your inventory completed, you can now make decisions on scoping what schema \n",
      "you want to migrate. Essentially, there are four options for scoping your legacy data \n",
      "warehouse schema migration:\n",
      "1. Migrate one data mart at a time:\n",
      "Figure 18.4: Migrating one data mart at a time\n",
      "2. Migrate all data marts at once, then the data warehouse:\n",
      "Figure 18.5: Migrating all data marts at once, then the data warehouse\n",
      "---------------\n",
      "618 | Azure Synapse Analytics for architects\n",
      "3. Migrate both the data warehouse and the staging area:\n",
      "Figure 18.6: Migrating both the data warehouse and the staging area\n",
      "4. Migrate everything at once:\n",
      "Figure 18.7: Migrating everything at once\n",
      "Keep in mind when choosing your option that the goal is to achieve a physical database \n",
      "design that will match or exceed your current legacy data warehouse system in \n",
      "performance and preferably at a lower cost.\n",
      "---------------\n",
      "Migrating from existing legacy systems to Azure Synapse Analytics | 619\n",
      "To recap, here are some of the recommendations for the schema migration:\n",
      "• Avoid migrating unnecessary objects or processes.\n",
      "• Consider using virtual data marts to reduce or eliminate the number of physical \n",
      "data marts.\n",
      "• Automate whenever possible. Implementation of DataOps should be considered \n",
      "alongside the migration to Azure Synapse.\n",
      "• Use metadata from system catalog tables in the legacy data warehouse system to \n",
      "generate Data Definition Language (DDL) for Azure Synapse Analytics.\n",
      "• Perform any required data model changes or data mapping optimizations on Azure \n",
      "Synapse Analytics.\n",
      "In the next section, we will talk about how to migrate historical data from a legacy data \n",
      "warehouse to Azure Synapse Analytics.\n",
      "Migrating historical data from your legacy data warehouse to Azure Synapse \n",
      "Analytics\n",
      "Once the schema migration scope has been determined, we are now ready to make\n",
      "---------------\n",
      "decisions on how to migrate the historical data.\n",
      "The steps for migrating historical data are as follows:\n",
      "1. Create target tables on Azure Synapse Analytics.\n",
      "2. Migrate existing historical data.\n",
      "3. Migrate functions and stored procedures as required.\n",
      "4. Migrate incremental load (ETL/ELT) staging and processes for incoming data.\n",
      "5. Apply any performance tuning options that are required.\n",
      "---------------\n",
      "620 | Azure Synapse Analytics for architects\n",
      "Table 18.2 outlines the four data migration options and their pros and cons:\n",
      "Table 18.2: Data migration options with their pros and cons\n",
      "In the next section, we will talk about how to migrate existing ETL processes to Azure \n",
      "Synapse Analytics.\n",
      "Data migration \n",
      "option\n",
      "Pros Cons\n",
      "Migrate data \n",
      "followed by data \n",
      "warehouse data\n",
      "• Migrating data one data mart at \n",
      "a time is an incremental, low-\n",
      "risk approach. It will also provide \n",
      "faster proof of business case to \n",
      "departmental analytics end users.\n",
      "• Subsequent ETL migration is \n",
      "limited to only the data in the \n",
      "dependent data marts migrated.\n",
      "• Until your migration is complete, \n",
      "you will have some data that \n",
      "exists on-premises and on Azure.\n",
      "• ETL processing from the data \n",
      "warehouse to data marts would \n",
      "be changed to target Azure \n",
      "Synapse.\n",
      "Migrate data \n",
      "warehouse data \n",
      "data marts\n",
      "• All data warehouse historical data \n",
      "is migrated.\n",
      "• Leaving dependent data marts \n",
      "on-premisesis not ideal as ETLs\n",
      "---------------\n",
      "datacenter.\n",
      "• No real opportunity for \n",
      "incremental data migration.\n",
      "Migrate data \n",
      "warehouse \n",
      "and data marts \n",
      "together\n",
      "• All data is migrated in one go. • Potentially higher risk.\n",
      "• ETLs will most likely all have to \n",
      "be migrated together.\n",
      "Convert physical \n",
      "marts to virtual \n",
      "marts and only \n",
      "migrate the data \n",
      "warehouse\n",
      "• No data mart data stores to \n",
      "migrate.\n",
      "• No ETLs from the data \n",
      "warehouse tothe marts to \n",
      "migrate.\n",
      "• Only the data warehouse data to \n",
      "migrate.\n",
      "• Fewer copies of data.\n",
      "• No loss in functionality.\n",
      "• Lower total cost of ownership.\n",
      "• More agility.\n",
      "• Simpler overall data architecture.\n",
      "• May be possible with views in \n",
      "Azure Synapse.\n",
      "• If nested views are not capable \n",
      "of supporting virtual data marts, \n",
      "then third-party data virtualization \n",
      "software on Azure will likely be \n",
      "needed.\n",
      "• All marts would need to be \n",
      "converted before data warehouse \n",
      "data is migrated.\n",
      "• Virtual marts and data \n",
      "warehouse-to-virtual mart \n",
      "mappings will need to be ported \n",
      "to the data virtualization server\n",
      "---------------\n",
      "on Azure and redirected to Azure \n",
      "Synapse.\n",
      "---------------\n",
      "Migrating from existing legacy systems to Azure Synapse Analytics | 621\n",
      "Migrating existing ETL processes to Azure Synapse Analytics\n",
      "There are a number of options available for migrating your existing ETL processes to \n",
      "Azure Synapse Analytics. Table 18.3 outlines some of the ETL migration options based \n",
      "on how the existing ETL jobs are built:\n",
      "Table 18.3: ETL migration options\n",
      "In the next section, we will talk about how to re-develop scalable ETL processes \n",
      "using ADF.\n",
      "How are \n",
      "existing ETL \n",
      "jobs built?\n",
      "Migration options Why migrate and what to look \n",
      "out for\n",
      "Custom 3GL \n",
      "code and scripts\n",
      "• Plan to re-develop these using \n",
      "ADF.\n",
      "• Code provides no metadata \n",
      "lineage.\n",
      "• Hard to maintain if authors have \n",
      "gone.\n",
      "• If staging tables are in the legacy \n",
      "data warehouse and SQL is used \n",
      "to transform data, then resolve \n",
      "Stored \n",
      "procedures \n",
      "that run in your \n",
      "legacy data \n",
      "warehouse \n",
      "DBMS\n",
      "• Plan to re-develop these using \n",
      "ADF.\n",
      "• \n",
      "between the legacy data \n",
      "warehouse and Azure Synapse.\n",
      "• No metadata lineage.\n",
      "---------------\n",
      "• This needs careful evaluation, \n",
      "but the key advantage could be \n",
      "the Pipeline as Code approach, \n",
      "which is possible with ADF.\n",
      "Graphical ETL \n",
      "tool (such as \n",
      "Informatica or \n",
      "Talend)\n",
      "• Continue using your existing \n",
      "ETL tool and switch the target to \n",
      "Azure Synapse.\n",
      "• Possibly move to an Azure \n",
      "version of your existing ETL tool \n",
      "and port the metadata to run ELT \n",
      "jobs on Azure making sure you \n",
      "enable access to on-premises \n",
      "data sources.\n",
      "• Control the execution of ETL \n",
      "services using ADF.\n",
      "• Avoids re-development.\n",
      "• Minimizes risk and quicker to \n",
      "migrate.\n",
      "Data warehouse \n",
      "automation \n",
      "software\n",
      "• Continue using your existing ETL \n",
      "tool, switching the target and \n",
      "staging to Azure Synapse.\n",
      "• Avoids re-development.\n",
      "• Minimizes risk and quicker to \n",
      "migrate.\n",
      "---------------\n",
      "622 | Azure Synapse Analytics for architects\n",
      "Re-developing scalable ETL processes using ADF\n",
      "Another option for handling your existing legacy ETL processes is by re-developing \n",
      "them using ADF. ADF is an Azure data integration service for creating data-driven \n",
      "workflows (known as pipelines) to orchestrate and automate data movement and data \n",
      "transformation. You can use ADF to create and schedule pipelines to ingest data from \n",
      "different data stores. ADF can process and transform data by using compute services \n",
      "such as Spark, Azure Machine Learning, Azure HDInsight Hadoop, and Azure Data Lake \n",
      "Analytics:\n",
      "Figure 18.8: Re-developing scalable ETL processes using ADF\n",
      "The next section will offer some recommendations for migrating queries, BI reports, \n",
      "dashboards, and other visualizations.\n",
      "Recommendations for migrating queries, BI reports, dashboards, and other \n",
      "visualizations\n",
      "Migrating queries, BI reports, dashboards, and other visualizations from your legacy\n",
      "---------------\n",
      "data warehouse to Azure Synapse Analytics is straightforward if the legacy system uses \n",
      "standard SQL.\n",
      "---------------\n",
      "Migrating from existing legacy systems to Azure Synapse Analytics | 623\n",
      "However, often, this is not the case. In this situation, a different strategy must be taken:\n",
      "• Identify the high-priority reports to migrate first.\n",
      "• Use usage statistics to identify the reports that are never used. \n",
      "• Avoid migrating anything that is no longer in use.\n",
      "• Once you have produced the list of reports to migrate, their priorities, and the \n",
      "unused reports to be bypassed, confirm this list with the stakeholders.\n",
      "• For reports that you are migrating, identify incompatibilities early to gauge the \n",
      "migration effort.\n",
      "• Consider data virtualization to protect BI tools and applications from structural \n",
      "changes to the data warehouse and/or data mart data model that might occur \n",
      "during the migration.\n",
      "Common migration issues and resolutions\n",
      "During the migration process, you might encounter certain issues that you need to \n",
      "overcome. In this section, we will highlight some of the common issues and provide you\n",
      "---------------\n",
      "with resolutions that you can implement.\n",
      "Issue #1: Unsupported data types and workarounds\n",
      "Table 18.4 shows the data types from legacy data warehouse systems that are \n",
      "unsupported, as well as the suitable workarounds for Azure Synapse Analytics:\n",
      "Table 18.4: Unsupported data types and suitable workarounds in Azure Synapse Analytics\n",
      "Unsupported \n",
      "data type\n",
      "Workaround for Azure Synapse Analytics\n",
      "geometry varbinary\n",
      "geography varbinary\n",
      "hierarchyidn varchar(4000)\n",
      "image varbinary\n",
      "text varchar\n",
      "ntextn varchar\n",
      "sql_variant Split column into several strongly typed columns\n",
      "table Convert to temporary tables\n",
      "timestamp Rework code to use datetime2 and the CURRENT_TIMESTAMP function\n",
      "xml varchar\n",
      "Convert back to the native data type when possible\n",
      "---------------\n",
      "624 | Azure Synapse Analytics for architects\n",
      "Issue #2: Data type differences between Netezza and Azure Synapse\n",
      "Table 18.5 maps the Netezza data types to their Azure Synapse equivalent data types:\n",
      "Table 18.5: Netezza data types and their Azure Synapse equivalents\n",
      "Netezza data type Azure Synapse data type\n",
      "BIGINTB IGINT\n",
      "BINARY VARYING(n) VARBINARY(n)\n",
      "BOOLEAN BIT\n",
      "BYTEINT TINYINT\n",
      "CHARACTER VARYING(n) VARCHAR(n)\n",
      "CHARACTER(n) CHAR(n)\n",
      "DATE DATE\n",
      "DECIMAL(p,s)D ECIMAL(p,s)\n",
      "DOUBLE PRECISION FLOAT\n",
      "FLOAT(n) FLOAT(n)\n",
      "INTEGERI NT\n",
      "INTERVAL INTERVAL data types are not currently directly supported \n",
      "in Azure Synapse but can be calculated using temporal \n",
      "functions, such as DATEDIFF\n",
      "MONEYM ONEY\n",
      "NATIONAL CHARACTER \n",
      "VARYING(n)\n",
      "NVARCHAR(n)\n",
      "NATIONAL CHARACTER(n) NCHAR(n)\n",
      "NUMERIC(p,s) NUMERIC(p,s)\n",
      "REAL REAL\n",
      "SMALLINT SMALLINT\n",
      "ST_GEOMETRY(n)S patial data types such as ST_GEOMETRYarenot currently \n",
      "supported in Azure Synapse, but the data could be stored as \n",
      "VARCHAR or VARBINARY\n",
      "TIME TIME\n",
      "---------------\n",
      "TIME WITH TIME ZONED ATETIMEOFFSET\n",
      "TIMESTAMP DATETIME\n",
      "---------------\n",
      "Common SQL incompatibilities and resolutions | 625\n",
      "Issue #3: Integrity constraint differences\n",
      "Pay close attention to the integrity constraint differences between your legacy data \n",
      "warehouse or data mart and Azure Synapse Analytics. In Figure 18.9, the left side \n",
      "represents the old legacy data warehouse system with primary key and foreign key \n",
      "constraints, and on the right side is the new Azure Synapse Analytics environment:\n",
      "Figure 18.9: Integrity constraint differences\n",
      "The next sections will provide comprehensive coverage on how to resolve other \n",
      "common SQL incompatibilities during the migration from a legacy data warehouse to \n",
      "Azure Synapse Analytics.\n",
      "Common SQL incompatibilities and resolutions\n",
      "This section will provide technical details regarding common SQL incompatibilities and \n",
      "resolutions between legacy data warehouse systems and Azure Synapse Analytics. The \n",
      "section will explain and compare the differences and provide resolutions using a quick-\n",
      "---------------\n",
      "reference table that you can refer to later on as you embark on your migration project.\n",
      "The topics that we will cover are as follows:\n",
      "• SQL Data Definition Language (DDL) differences and resolutions\n",
      "• SQL Data Manipulation Language (DML) differences and resolutions\n",
      "• SQL Data Control Language (DCL) differences and resolutions\n",
      "• Extended SQL differences and workarounds\n",
      "---------------\n",
      "626 | Azure Synapse Analytics for architects\n",
      "SQL DDL differences and resolutions\n",
      "In this section, we will discuss the differences and resolutions for SQL DDL between \n",
      "legacy data warehouse systems and Azure Synapse Analytics.\n",
      "Table 18.6: SQL DDL differences between legacy systems and Azure Synapse\n",
      "Issues Legacy data warehouse system Resolutions\n",
      "Proprietary \n",
      "table types\n",
      "• On the legacy system, identify any \n",
      "use of proprietary table types.\n",
      "• Migrate to standard tables within \n",
      "Azure Synapse Analytics.\n",
      "• For time series, index or partition on \n",
      "the date/time column.\n",
      "• \n",
      "added into the relevant temporal \n",
      "queries.\n",
      "Views • Identify views from catalog tables \n",
      "and DDL scripts.\n",
      "• Views with proprietary SQL \n",
      "extensions or functions will have to \n",
      "be re-written.\n",
      "• Azure Synapse Analytics also \n",
      "supports materialized views and will \n",
      "automatically maintain and refresh \n",
      "these.\n",
      "Nulls • NULL values can be handled \n",
      " \n",
      "For example, in Oracle, an empty \n",
      "string is equivalent to a NULL value.\n",
      "---------------\n",
      "• Some DBMSes have proprietary \n",
      "SQL functions for handling NULLs;  \n",
      "for example, NVL in Oracle.\n",
      "• Generate SQL queries to test for \n",
      "NULL values.\n",
      "• Test reports that include nullable \n",
      "columns.\n",
      "---------------\n",
      "Common SQL incompatibilities and resolutions | 627\n",
      "SQL DML differences and resolutions\n",
      "In this section, we will discuss the differences and resolutions for SQL DML between \n",
      "legacy data warehouse systems and Azure Synapse Analytics:\n",
      "Table 18.7: SQL DML differences between Netezza and Azure Synapse\n",
      "Next, we will talk about the differences and resolutions of SQL DCL between legacy \n",
      "data warehouse systems and Azure Synapse Analytics.\n",
      "SQL DCL differences and resolutions\n",
      "In this section, we will discuss the differences and resolutions for SQL DCL between \n",
      "legacy data warehouse systems and Azure Synapse Analytics. Netezza supports two \n",
      "classes of access rights: admin and object. Table 18.8 map the Netezza access rights and \n",
      "their corresponding Azure Synapse equivalents for quick reference.\n",
      "Function Netezza Azure Synapse equivalent\n",
      "STRPOS SELECT STRPOS(‘ABCDEFG’, \n",
      "‘BCD’) …\n",
      "SELECT CHARINDEX(‘BCD’, \n",
      "‘ABCDEFG’) …\n",
      "AGE SELECT AGE(’25-12-1940’, ’25-12-\n",
      "2020’) FROM …\n",
      "SELECT DATEDIFF( day, ‘1940-12-\n",
      "---------------\n",
      "25’, ‘2020-12-25’) FROM …\n",
      "NOW()N OW() CURRENT_TIMESTAMP\n",
      "SEQUENCE CREATE SEQUENCE … Rewrite using IDENTITY columns on \n",
      "Azure Synapse Analytics.\n",
      "UDF Netezza UDFs are written in nzLua \n",
      "or C++.\n",
      "Rewrite in T-SQL on Azure Synapse \n",
      "Analytics.\n",
      "Stored \n",
      "Procedures\n",
      "Netezza stored procedures are \n",
      "written in NZPLSQL (based on \n",
      "Postgres PL/pgSQL).\n",
      "Rewrite in T-SQL on Azure Synapse \n",
      "Analytics.\n",
      "---------------\n",
      "628 | Azure Synapse Analytics for architects\n",
      "Mapping Netezza admin privileges to the Azure Synapse equivalents\n",
      "Table 18.8 maps the Netezza admin privileges to the Azure Synapse equivalents:\n",
      "Admin \n",
      "privilege\n",
      "Description Azure Synapse equivalent\n",
      "BackupA llows users to create backups and to run the \n",
      "nzbackup command.\n",
      "Backup and restore feature \n",
      "in Azure Synapse SQL pool\n",
      "[Create] \n",
      "Aggregate Aggregates (UDAs). Permission to operate on \n",
      "existing UDAs is controlled by object privileges.\n",
      "Azure Synapse’s CREATE \n",
      "FUNCTION feature \n",
      "incorporates Netezza \n",
      "aggregate functionality\n",
      "[Create] \n",
      "Database\n",
      "Allows the user to create databases. \n",
      "Permission to operate on existing databases is \n",
      "controlled by object privileges.\n",
      "CREATE DATABASE\n",
      "[Create] \n",
      "External \n",
      "Table\n",
      "Allows the user to create external tables. \n",
      "Permission to operate on existing tables is \n",
      "controlled by object privileges.\n",
      "CREATE TABLE\n",
      "[Create] \n",
      "Function\n",
      "Allows the user to create UDFs. Permission \n",
      "to operate on existing UDFs is controlled by\n",
      "---------------\n",
      "object privileges.\n",
      "CREATE FUNCTION\n",
      "[Create] \n",
      "Group\n",
      "Allows the user to create groups. Permission \n",
      "to operate on existing groups is controlled by \n",
      "object privileges.\n",
      "CREATE ROLE\n",
      "[Create] \n",
      "Index\n",
      "For system use only. Users cannot create \n",
      "indexes.\n",
      "CREATE INDEX\n",
      "[Create] \n",
      "Library\n",
      "Allows the user to create shared libraries. \n",
      "Permission to operate on existing shared \n",
      "libraries is controlled by object privileges.\n",
      "N/A\n",
      "[Create] \n",
      "Materialized \n",
      "View\n",
      "Allows the user to create materialized views. CREATE VIEW\n",
      "[Create] \n",
      "Procedure\n",
      "Allows the user to create stored procedures. \n",
      "Permission to operate on existing stored \n",
      "procedures is controlled by object privileges.\n",
      "CREATE PROCEDURE\n",
      "[Create] \n",
      "Schema\n",
      "Allows the user to create schemas. Permission \n",
      "to operate on existing schemas is controlled by \n",
      "object privileges.\n",
      "CREATE SCHEMA\n",
      "---------------\n",
      "Common SQL incompatibilities and resolutions | 629\n",
      "Table 18.8: Netezza admin privileges and their Azure Synapse equivalents\n",
      "Admin \n",
      "privilege\n",
      "Description Azure Synapse equivalent\n",
      "[Create] \n",
      "Sequence\n",
      "Allows the user to create database sequences.N /A\n",
      "[Create] \n",
      "Synonym\n",
      "Allows the user to create synonyms. CREATE SYNONYM\n",
      "[Create] \n",
      "Table\n",
      "Allows the user to create tables. Permission \n",
      "to operate on existing tables is controlled by \n",
      "object privileges.\n",
      "CREATE TABLE\n",
      "[Create] \n",
      "Temp Table\n",
      "Allows the user to create temporary tables. \n",
      "Permission to operate on existing tables is \n",
      "controlled by object privileges.\n",
      "CREATE TABLE\n",
      "[Create] \n",
      "User\n",
      "Allows the user to create users. Permission \n",
      "to operate on existing users is controlled by \n",
      "object privileges.\n",
      "CREATE USER\n",
      "[Create] \n",
      "View\n",
      "Allows the user to create views. Permission \n",
      "to operate on existing views is controlled by \n",
      "object privileges.\n",
      "CREATE VIEW\n",
      "[Manage] \n",
      "Hardware\n",
      "Allows the user to view hardware status, \n",
      "manage SPUs, manage topology and\n",
      "---------------\n",
      "mirroring, and run diagnostic tests.\n",
      "Automatically managed via \n",
      "the Azure portal in Azure \n",
      "Synapse\n",
      "[Manage] \n",
      "Security\n",
      "Allows the user to run commands and \n",
      "history databases; managing multi-level \n",
      "security objects, including specifying security \n",
      "for users and groups; and managing database \n",
      "keys for the digital signing of audit data.\n",
      "Automatically managed via \n",
      "the Azure portal in Azure \n",
      "Synapse\n",
      "[Manage] \n",
      "System\n",
      "Allows the user to do the following \n",
      "management operations: start/stop/pause/\n",
      "resume the system, abort sessions, and view \n",
      "the distribution map, system statistics, and \n",
      "logs. The user can use these commands: \n",
      "nzsystem, nzstate, nzstats, and nzsession.\n",
      "Automatically managed via \n",
      "the Azure portal in Azure \n",
      "Synapse\n",
      "Restore Allows the user to restore the system. The user \n",
      "can run the nzrestore command.\n",
      "Automatically handled in \n",
      "Azure Synapse\n",
      "UnfenceA llows the user to create or alter a UDF or \n",
      "aggregate to run in unfenced mode.\n",
      "N/A\n",
      "---------------\n",
      "630 | Azure Synapse Analytics for architects\n",
      "Mapping Netezza object privileges to their Azure Synapse equivalent\n",
      "Table 18.9 maps the Netezza object privileges to the Azure Synapse equivalents for quick \n",
      "reference:\n",
      "Table 18.9: Netezza object privileges and their Azure Synapse equivalents\n",
      "Object \n",
      "privilege\n",
      "Description Azure Synapse \n",
      "equivalent\n",
      "AbortA llows the user to abort sessions. Applies to groups and \n",
      "users.\n",
      "KILL DATABASE \n",
      "CONNECTION\n",
      "AlterA llows the user to modify object attributes. Applies to all \n",
      "objects.\n",
      "ALTER\n",
      "DeleteA llows the user to delete table rows. Applies only to tables.D ELETE\n",
      "Drop Allows the user to drop objects. Applies to all object types. DROP\n",
      "Execute Allows the user to run UDFs, UDAs, or stored procedures. EXECUTE\n",
      "GenStats Allows the user to generate statistics on tables or databases. \n",
      "The user can run the GENERATE STATISTICS command.\n",
      "Automatically \n",
      "handled in  \n",
      "Azure Synapse\n",
      "Groom Allows the user to reclaim disk space for deleted or outdated\n",
      "---------------\n",
      "rows, and reorganize a table by the organizing keys, or to \n",
      "migrate data for tables that have multiple stored versions.\n",
      "Automatically \n",
      "handled in  \n",
      "Azure Synapse\n",
      "Insert Allows the user to insert rows into a table. Applies only to \n",
      "tables.\n",
      "INSERT\n",
      "ListA llows the user to display an object name, either in a list or in \n",
      "another manner. Applies to all objects.\n",
      "LIST\n",
      "Select Allows the user to select (or query) rows within a table. \n",
      "Applies to tables and views.\n",
      "SELECT\n",
      "Truncate Allows the user to delete all rows from a table. Applies only to \n",
      "tables.\n",
      "TRUNCATE\n",
      "UpdateA llows the user to modify table rows. Applies only to tables.U PDATE\n",
      "---------------\n",
      "Common SQL incompatibilities and resolutions | 631\n",
      "Extended SQL differences and workarounds\n",
      "Table 18.10 describes the extended SQL differences and possible workarounds when \n",
      "migrating to Azure Synapse Analytics:\n",
      "Table 18.10: Extended SQL differences and workarounds\n",
      "SQL \n",
      "extension\n",
      "Description How to migrate\n",
      "UDFs•  Can contain arbitrary code\n",
      "• Can be coded in various \n",
      "languages (such as Lua and Java)\n",
      "• Can be called within a SQL \n",
      "SELECT statement in the same \n",
      "way that built-in functions such as \n",
      "SUM() and AVG() are used\n",
      "• Use CREATE FUNCTION and re-\n",
      "code in T-SQL.\n",
      "Stored \n",
      "procedures\n",
      "• Can contain one or more SQL \n",
      "statements as well as procedural \n",
      "logic around those SQL statements\n",
      "• Implemented in a standard \n",
      "language (such as Lua) or in a \n",
      "proprietary language (such as \n",
      "Oracle PL/SQL)\n",
      "• Recode in T-SQL.\n",
      "• Some third-party tools that can help \n",
      "with migration:\n",
      "• Datometry\n",
      "• WhereScape\n",
      "Triggers•  Not supported by Azure Synapse•  Equivalent functionality can be\n",
      "---------------\n",
      "achieved by using other parts of \n",
      "the Azure ecosystem. For example, \n",
      "for streamed input data, use Azure \n",
      "Stream Analytics.\n",
      "In-database \n",
      "analytics\n",
      "• Not supported by Azure Synapse•  Run advanced analytics such as \n",
      "machine learning models at scale to \n",
      "use Azure Databricks.\n",
      "• Azure Synapse opens the \n",
      "possibilityof performing machine \n",
      "learning functions with Spark MLlib.\n",
      "• Alternatively, migrate to Azure SQL \n",
      "Database and use the PREDICT \n",
      "function.\n",
      "Geospatial \n",
      "data types\n",
      "• Not supported by Azure Synapse•  Store geospatial data, such as \n",
      "latitude/longitude, and popular \n",
      "formats, such as Well-KnownText \n",
      "(WKT) and Well-Known Binary \n",
      "(WKB), in VARCHAR or VARBINARY \n",
      "columns and access them directly by \n",
      "using geospatial client tools.\n",
      "---------------\n",
      "632 | Azure Synapse Analytics for architects\n",
      "In this section, we talked about common migration issues that architects might \n",
      "encounter during a migration project and possible solutions. In the next section, we will \n",
      "take a look at security considerations that an architect should be mindful of.\n",
      "Security considerations\n",
      "Protecting and securing your data assets is paramount in any data warehouse system. \n",
      "When planning a data warehouse migration project, security, user access management, \n",
      "backup, and restore must also be taken into consideration. For instance, data \n",
      "encryption may be mandatory for industry and government regulations, such as HIPAA, \n",
      "PCI, and FedRAMP, as well as in non-regulated industries.\n",
      "Azure includes many features and functions as standard that would traditionally have \n",
      "to be custom-built in legacy data warehouse products. Azure Synapse supports data \n",
      "encryption at rest and data in motion as standard.\n",
      "Data encryption at rest\n",
      "---------------\n",
      "• Transparent Data Encryption (TDE) can be enabled to dynamically encrypt and \n",
      "decrypt Azure Synapse data, logs, and associated backups.\n",
      "• Azure Data Storage can also automatically encrypt non-database data.\n",
      "Data in motion\n",
      "All connections to Azure Synapse Analytics are encrypted by default, using industry-\n",
      "standard protocols such as TLS and SSH.\n",
      "In addition, Dynamic Data Masking (DDM) can be used to obfuscate data for given \n",
      "classes of users based on data masking rules.\n",
      "As a best practice, if your legacy data warehouse contains a complex hierarchy of \n",
      "permissions, users and roles, consider using automation techniques in your migration \n",
      "process. You can use existing metadata from your legacy system to generate the \n",
      "necessary SQL to migrate users, groups, and privileges on Azure Synapse Analytics.\n",
      "In the final section of this chapter, we will review some of the tools that architects \n",
      "can choose to help migrate from legacy data warehouse systems to Azure Synapse \n",
      "Analytics.\n",
      "---------------\n",
      "Tools to help migrate to Azure Synapse Analytics | 633\n",
      "Tools to help migrate to Azure Synapse Analytics\n",
      "Now that we have covered the planning and preparation and an overview of the \n",
      "migration process, let's have a look at the tools that you can use for migrating your \n",
      "legacy data warehouse to Azure Synapse Analytics. The tools that we will discuss are:\n",
      "• ADF\n",
      "• Azure Data Warehouse Migration Utility\n",
      "• Microsoft Services for Physical Data Transfer\n",
      "• Microsoft Services for Data Ingestion\n",
      "Let's get started. \n",
      "ADF\n",
      "ADF is a fully managed, pay-as-you-use, hybrid data integration service for cloud-scale \n",
      "ETL processing. It offers the following features:\n",
      "• Processes and analyzes data in memory and in parallel to scale and maximize \n",
      "throughput\n",
      "• Creates data warehouse migration pipelines that orchestrate and automate data \n",
      "movement, data transformation, and data loading into Azure Synapse Analytics\n",
      "• Can also be used to modernize your data warehouse by ingesting data into\n",
      "---------------\n",
      "Azure Data Lake Storage, processing and analyzing data at scale, and loading data \n",
      "into a data warehouse\n",
      "• Supports role-based user interfaces for mapping data flows for IT professionals \n",
      "and self-service data wrangling for business users\n",
      "• Can connect to multiple data stores spanning datacenters, clouds, and SaaS \n",
      "applications\n",
      "• Over 90 natively built and maintenance-free connectors available (https:/ /azure.\n",
      "microsoft.com/services/data-factory)\n",
      "• Can mix and match wrangling and mapping data flows in the same pipeline to \n",
      "prepare data at scale\n",
      "• ADF orchestration can control data warehouse migration to Azure Synapse \n",
      "Analytics\n",
      "• Can execute SSIS ETL packages\n",
      "---------------\n",
      "634 | Azure Synapse Analytics for architects\n",
      "Azure Data Warehouse Migration Utility\n",
      "Azure Data Warehouse Migration Utility can migrate data from an on-premises SQL \n",
      "Server–based data warehouse to Azure Synapse. It offers the following features:\n",
      "• Uses a wizard-like approach to perform a lift and shift migration of schema and \n",
      "data from an on-premises, SQL Server–based data warehouse.\n",
      "• You can select the on-premises database containing the table(s) that you want to \n",
      "export to Azure Synapse. Then, you can select the tables that you want to migrate \n",
      "and migrate the schema. \n",
      "• Automatically generates T-SQL code needed to create an equivalent empty \n",
      "database and tables on Azure Synapse. Once you provide connection details to \n",
      "Azure Synapse you can run the generated T-SQL to migrate the schema. \n",
      "• Following schema creation, you can use the utility to migrate the data. This \n",
      "exports the data from your on-premises SQL Server–based data warehouse and\n",
      "---------------\n",
      "generates Bulk Copy Program (BCP) commands to load that data into Azure \n",
      "Synapse.\n",
      "Microsoft Services for Physical Data Transfer\n",
      "In this section, we will look at common Microsoft services that can be used for physical \n",
      "data transfer, including Azure ExpressRoute, AzCopy, and Azure Databox.\n",
      "Azure ExpressRoute\n",
      "Azure ExpressRoute allows you to make private connections between your datacenters \n",
      "and Azure without going over the public Internet. It offers the following features:\n",
      "• Bandwidth of up to 100 Gbps\n",
      "• Low latency\n",
      "• Connects directly to your Wide-Area Network (WAN)\n",
      "• Private connections to Azure\n",
      "• Increased speed and reliability\n",
      "AzCopy\n",
      "AzCopy is a command-line tool for copying files and blobs to/from storage accounts. It \n",
      "offers the following features:\n",
      "• Ability to copy data to/from Azure via the Internet.\n",
      "• A combination of AzCopy with the necessary ExpressRoute bandwidth could be an \n",
      "optimal solution for data transfer to Azure Synapse.\n",
      "---------------\n",
      "Tools to help migrate to Azure Synapse Analytics | 635\n",
      "Azure Data Box\n",
      "Azure Data Box allows you to transfer large volumes of data to Azure quickly, reliably, \n",
      "and cost-effectively. It offers the following features:\n",
      "• Capable of transferring large volumes of data (tens of terabytes to hundreds to \n",
      "terabytes)\n",
      "• No network connectivity restrictions\n",
      "• Great for one-time migration and initial bulk transfer\n",
      "Microsoft Services for data ingestion\n",
      "In this section, we will look at common Microsoft services that can be used for data \n",
      "ingestion, including:\n",
      "• PolyBase\n",
      "• BCP\n",
      "• SqlBulkCopy API\n",
      "• Standard SQL\n",
      "PolyBase (recommended method)\n",
      "PolyBase provides the fastest and most scalable bulk data loading into Azure Synapse \n",
      "Analytics. It offers the following features:\n",
      "• Uses parallel loading to give the fastest throughput\n",
      "• Can read from flat files in Azure Blob storage or from external data sources via \n",
      "connectors\n",
      "• Tightly integrated with ADF\n",
      "• CREATE TABLE AS or INSERT … SELECT\n",
      "---------------\n",
      "• Can define a staging table as type HEAP for fast load\n",
      "• Support rows up to 1 MB in length\n",
      "---------------\n",
      "636 | Azure Synapse Analytics for architects\n",
      "BCP\n",
      "BCP can be used to import and export data from any SQL Server environment, including \n",
      "Azure Synapse Analytics. It offers the following features:\n",
      "• Supports rows larger than 1 MB in length\n",
      "• Originally developed for earlier versions of Microsoft SQL Server\n",
      "Refer to https:/ /docs.microsoft.com/sql/tools/bcp-utility to read more about the BCP \n",
      "utility.\n",
      "SqlBulkCopy API\n",
      "SqlBulkCopy API is the API equivalent of the BCP functionality. It offers the following \n",
      "features:\n",
      "• Allows the implementation of load processes programmatically\n",
      "• Ability to bulk load SQL Server tables with data from selected sources\n",
      "Refer to https:/ /docs.microsoft.com/dotnet/api/system.data.sqlclient.sqlbulkcopy to \n",
      "read more about this API.\n",
      "Standard SQL Support\n",
      "Azure Synapse Analytics supports standard SQL, including the ability to:\n",
      "• Load individual rows or results of SELECT statements into data warehouse tables.\n",
      "---------------\n",
      "• Bulk insert data from extracted data via external data sources into data warehouse \n",
      "tables using INSERT … SELECT statements within PolyBase.\n",
      "This section provided the architectural considerations and high-level methodology for \n",
      "planning, preparing, and executing a successful migration of an existing legacy data \n",
      "warehouse system to Azure Synapse Analytics. It contains a wealth of information that \n",
      "you can refer to later on as you embark on your migration project to Azure Synapse \n",
      "Analytics.\n",
      "Summary\n",
      "Azure Synapse Analytics is a limitless analytics service with unmatched time to insight \n",
      "that accelerates the delivery of BI, AI, and intelligent applications for enterprises. You \n",
      "will gain a lot of benefits by migrating your legacy data warehouse to Azure Synapse \n",
      "Analytics, including performance, speed, improved security and compliance, elasticity, \n",
      "managed infrastructure, scalability, and cost savings.\n",
      "---------------\n",
      "Summary | 637\n",
      "With Azure Synapse, data professionals of varying skillsets can collaborate, manage, \n",
      "and analyze their most important data with ease—all within the same service. From \n",
      "Apache Spark integration with the powerful and trusted SQL engine, to code-free data \n",
      "integration and management, Azure Synapse is built for every data professional.\n",
      "This chapter provided the architectural considerations and high-level methodology \n",
      "needed to prepare for and execute the migration of an existing legacy data warehouse \n",
      "system to Azure Synapse Analytics.\n",
      "Successful data migration projects start with a well-designed plan. An effective plan \n",
      "accounts for the many components that need to be considered, paying particular \n",
      "attention to architecture and data preparation.\n",
      "After you have successfully migrated to Azure Synapse, you can explore additional \n",
      "Microsoft technologies in the rich Azure analytical ecosystem to further modernize \n",
      "your data warehouse architecture.\n",
      "Here are some ideas to ponder:\n",
      "---------------\n",
      "• Offload your staging areas and ELT processing to Azure Data Lake Storage \n",
      "and ADF.\n",
      "• Build trusted data products once in common data model format and consume \n",
      "everywhere—not just in your data warehouse.\n",
      "• Enable collaborative development of data preparation pipelines by business and \n",
      "IT using ADF mapping and wrangling data flows.\n",
      "• Build analytical pipelines in ADF to analyze data in batch and real time.\n",
      "• Build and deploy machine learning models to add additional insights to what you \n",
      "already know.\n",
      "• Integrate your data warehouse with live streaming data.\n",
      "• Simplify access to data and insights in multiple Azure analytical data stores by \n",
      "creating a logical data warehouse using PolyBase.\n",
      "In the next chapter, you will learn in detail about Azure Cognitive Services, with a focus \n",
      "on architecting solutions that include intelligence as their core engine.\n",
      "---------------\n",
      "Cloud technology has changed a lot of things, including the creation of intelligent \n",
      "applications in an agile, scalable, and pay-as-you-go way. Applications prior to the \n",
      "rise of cloud technology generally did not incorporate intelligence within themselves, \n",
      "primarily because:\n",
      "• It was time-consuming and error-prone.\n",
      "• It was difficult to write, test, and experiment with algorithms on an ongoing basis.\n",
      "• There was a lack of sufficient data.\n",
      "• It was immensely costly.\n",
      "Over the last decade, two things have changed that have led to the creation of \n",
      "significantly more intelligent applications than in the past. These two things are the \n",
      "cost-effective, on-demand unlimited scalability of the cloud along with the availability \n",
      "of data in terms of volume, variety, and velocity.\n",
      "Architecting \n",
      "intelligent solutions\n",
      "19\n",
      "---------------\n",
      "640 | Architecting intelligent solutions\n",
      "In this chapter, we will look at architectures that can help build intelligent applications \n",
      "with Azure. Some of the topics covered in this chapter are:\n",
      "• The evolution of AI\n",
      "• Azure AI processes\n",
      "• Azure Cognitive Services\n",
      "• Building an optical character recognition service\n",
      "• Building a visual features service using the Cognitive Search .NET SDK\n",
      "The evolution of AI \n",
      "AI is not a new field of knowledge. In fact, the technology is a result of decades of \n",
      "innovation and research. However, its implementation in previous decades was a \n",
      "challenge for the following reasons:\n",
      "1. Cost: AI experiments were costly in nature and there was no cloud technology. All \n",
      "the infrastructure was either purchased or hired from a third party. Experiments \n",
      "were also time-consuming to set up and immense skills were needed to get \n",
      "started. A large amount of storage and compute power was also required, which\n",
      "---------------\n",
      "was generally missing in the community at large and held in the hands of just a \n",
      "few.\n",
      "2. Lack of data: There were hardly any smart handheld devices and sensors available \n",
      "generating data. Data was limited in nature and had to be procured, which again \n",
      "made AI applications costly. Data was also less reliable and there was a general \n",
      "lack of confidence in the data itself.\n",
      "3. Difficulty: AI algorithms were not documented enough and were primarily in \n",
      "the realms of mathematicians and statisticians. They were difficult to create \n",
      "and utilize within applications. Just imagine the creation of an optical character \n",
      "recognition (OCR) system 15 years ago. There were hardly any libraries, data, \n",
      "processing power, or the necessary skills to develop applications using OCR.\n",
      "Although the influx of data increased with time, there was still a lack of tools for making \n",
      "sense of the data in a way that added business value. In addition, good AI models\n",
      "---------------\n",
      "are based on sufficiently accurate data and trained with algorithms to be capable of \n",
      "resolving real-life problems. Both cloud technology and the large number of sensors \n",
      "and handheld devices have redefined this landscape.\n",
      "---------------\n",
      "Azure AI processes | 641\n",
      "With cloud technology, it is possible to provision on-demand storage and compute \n",
      "resources for AI-based applications. Cloud infrastructure provides lots of resources for \n",
      "data migration, storage, processing, and computation, as well as generating insights and \n",
      "eventually providing reports and dashboards. It does all this at a minimal cost in a faster \n",
      "way since there is nothing physical involved. Let's dive into understanding what goes on \n",
      "behind building an AI-based application.\n",
      "Azure AI processes\n",
      "Every AI-based project is required to go through a certain set of steps before being \n",
      "operational. Let's explore these seven phases:\n",
      "Data ingestion\n",
      "In this phase, data is captured from various sources and stored such that it can be \n",
      "consumed in the next phase. The data is cleaned before being stored and any deviations \n",
      "from the norm are disregarded. This is part of the preparation of data. The data could\n",
      "---------------\n",
      "have different velocity, variety, and volume. It can be structured similarly to relational \n",
      "databases, semi-structured like JSON documents, or unstructured like images, Word \n",
      "documents, and so on.\n",
      "Data transformation\n",
      "The data ingested is transformed into another format as it might not be consumable \n",
      "in its current format. The data transformation typically includes the cleaning and \n",
      "filtering of data, removing bias from the data, augmenting data by joining it with other \n",
      "datasets, creating additional data from existing data, and more. This is also part of the \n",
      "preparation of the data.\n",
      "Analysis\n",
      "The data from the last phase is reused for analysis. The analysis phase contains \n",
      "activities related to finding patterns within data, conducting exploratory data analysis, \n",
      "and generating further insights from it. These insights are then stored along with \n",
      "existing data for consumption in the next phase. This is part of the model packaging \n",
      "process.\n",
      "---------------\n",
      "642 | Architecting intelligent solutions\n",
      "Data modeling\n",
      "Once the data is augmented and cleaned, appropriate and necessary data is made \n",
      "available to the AI algorithms to generate a model that is conducive to achieving \n",
      "the overall aim. It is an iterative process known as experimentation by using various \n",
      "combinations of data (feature engineering) to ensure that the data model is robust. This \n",
      "is also part of the model packaging process.\n",
      "The data is fed into learning algorithms to identify patterns. This process is known as \n",
      "training the model. Later, test data is used on the model to check its effectiveness and \n",
      "efficiency.\n",
      "Validating the model\n",
      "Once the model is created, a set of test data is used to find its effectiveness. If the \n",
      "analysis obtained from the test data is reflective of reality, then the model is sound and \n",
      "usable. Testing is an important aspect of the AI process.\n",
      "Deployment\n",
      "The model is deployed to production so that real-time data can be fed into it to get the\n",
      "---------------\n",
      "predicted output. This output can then be used within applications.\n",
      "Monitoring\n",
      "The model deployed to production is monitored on an ongoing basis for the future \n",
      "analysis of all incoming data and to retrain and improve the effectiveness models.\n",
      "The AI stages and processes, by nature, are time-consuming and iterative. Thus, \n",
      "applications based on them have an inherent risk of being long-running, experimental, \n",
      "and resource-intensive, along with getting delayed with cost overruns and having low \n",
      "chances of success.\n",
      "Keeping these things in mind, there should be out-of-the-box AI-based solutions that \n",
      "developers can use in their applications to make them intelligent. These AI solutions \n",
      "should be easily consumable from applications and should have the following features:\n",
      "• Cross-platform: Developers using any platform should be able to consume these \n",
      "services. They should be deployed and consumed on Linux, Windows, or Mac \n",
      "without any compatibility problems.\n",
      "---------------\n",
      "• Cross-language: Developers should be able to use any language to consume these \n",
      "solutions. Not only will the developers encounter a shorter learning curve but they \n",
      "also won't need to change their preferred choice of language to consume these \n",
      "solutions.\n",
      "---------------\n",
      "Azure Cognitive Services | 643\n",
      "These solutions should be deployed as services using industry standards and protocols. \n",
      "Generally, these services are available as HTTP REST endpoints that can be invoked \n",
      "using any programming language and platform.\n",
      "There are many such types of service that can be modeled and deployed for developer \n",
      "consumption. Some examples include:\n",
      "• Language translation: In such services, the user provides text in one language and \n",
      "gets corresponding text in a different language as output.\n",
      "• Character recognition: These services accept images and return the text present \n",
      "in them.\n",
      "• Speech-to-text conversion: These services can convert input speech to text.\n",
      "Now that we have gone through the details of building an AI/ML-based project, let's \n",
      "dive into the applications of various cognitive services offered by Azure.\n",
      "Azure Cognitive Services\n",
      "Azure provides an umbrella service known as Azure Cognitive Services. Azure Cognitive\n",
      "---------------\n",
      "Services is a set of services that developers can consume within their applications to \n",
      "turn them into intelligent applications.\n",
      "Table 19.1: Azure Cognitive Services\n",
      "Vision Web Search Language Speech Decision\n",
      "• Computer Vision\n",
      "• Face\n",
      "• Video Indexer\n",
      "• Custom Vision\n",
      "• Form Recognizer \n",
      "(Preview)\n",
      "• Ink Recognizer \n",
      "(Preview)\n",
      "• Bing Autosuggest\n",
      "• Bing Custom Search\n",
      "• Bing Entity Search\n",
      "• Bing Image Search\n",
      "• Bing News Search\n",
      "• Bing Spell Check\n",
      "• Bing Video Search\n",
      "• Bing Web Search\n",
      "• Immersive \n",
      "Reader \n",
      "(Preview)\n",
      "• Language \n",
      "Understanding\n",
      "• QnA Maker\n",
      "• Text Analytics\n",
      "• Translator\n",
      "• Speech to \n",
      "Text\n",
      "• Text to \n",
      "Speech\n",
      "• Speech \n",
      "Translation\n",
      "• Speaker \n",
      "Recognition \n",
      "(Preview)\n",
      "• Anomaly \n",
      "Detector \n",
      "(Preview)\n",
      "• Content \n",
      "Moderator\n",
      "• Personalizer\n",
      "---------------\n",
      "644 | Architecting intelligent solutions\n",
      "The services have been divided into five main categories depending on their nature. \n",
      "These five categories are as follows:\n",
      "Vision\n",
      "This API provides algorithms for image classification and helps in image processing by \n",
      "providing meaningful information. Computer vision can provide a variety of information \n",
      "from images on different objects, people, characters, emotions, and more.\n",
      "Search\n",
      "These APIs help in search-related applications. They help with search based on text, \n",
      "images, video, and providing custom search options.\n",
      "Language\n",
      "These APIs are based on natural language processing and help extract information \n",
      "about the intent of user-submitted text along with entity detection. They also help in \n",
      "text analytics and translation to different languages.\n",
      "Speech\n",
      "These APIs help in translating speech to text, text to speech, and in speech translation. \n",
      "They can be used to ingest audio files and take actions based on the content on behalf\n",
      "---------------\n",
      "of users. Cortana is an example that uses similar services to take actions for users based \n",
      "on speech.\n",
      "Decision\n",
      "These APIs help in anomaly detection and content moderation. They can check \n",
      "for content within images, videos, and text and find out patterns that should be \n",
      "highlighted. An example of such an application is displaying a warning about adult \n",
      "content.\n",
      "Now that you have an understanding of the core of Cognitive Services, let's discuss how \n",
      "they work in detail.\n",
      "---------------\n",
      "Understanding Cognitive Services | 645\n",
      "Understanding Cognitive Services\n",
      "Azure Cognitive Services consists of HTTP endpoints that accept requests and send \n",
      "responses back to the caller. Almost all requests are HTTP POST requests and consist of \n",
      "both a header and a body.\n",
      "The provisioning of Cognitive Services generates two important artifacts that help a \n",
      "caller invoke an endpoint successfully. It generates an endpoint URL and a unique key. \n",
      "The format of the URL is https://{azure location}.api.cognitive.microsoft.com/\n",
      "{cognitive type}/{version}/{sub type of service}?{query parameters}. An example \n",
      "URL is:\n",
      "https://eastus.api.cognitive.microsoft.com/vision/v2.0/\n",
      "ocr?language=en&detectOrientation=true\n",
      "Cognitive Service is provisioned in the East US Azure region. The type of service is \n",
      "computer vision using version 2 and the subtype is OCR. There are generally a few \n",
      "subtypes for each top-level category. Lastly, there are a few query string parameters,\n",
      "---------------\n",
      "such as language and detectOrientation. These query parameters are different for each \n",
      "service category and subcategory.\n",
      "Either the header or the query parameters should provide the key value for the \n",
      "endpoint invocation to be successful.\n",
      "The key value should be assigned to the Ocp-Apim-Subscription-Key header key with the \n",
      "request.\n",
      "The content of the request body can be a simple string, a binary, or a combination of \n",
      "both. Depending on the value, the appropriate content-type header should be set in the \n",
      "request.\n",
      "The possible header values are:\n",
      "• Application/octet-stream\n",
      "• multipart/form-data\n",
      "• application/json\n",
      "Use octet-stream when sending binary data and json for sending string values. form-\n",
      "data can be used for sending multiple combination values of binary and text.\n",
      "The key is a unique string used to validate whether the caller has been given permission \n",
      "to invoke the URL. This key must be protected such that others who should not be able\n",
      "---------------\n",
      "to invoke the endpoints do not get access to it. Later in the chapter, you will see ways to \n",
      "safeguard these keys.\n",
      "---------------\n",
      "646 | Architecting intelligent solutions\n",
      "Consuming Cognitive Services\n",
      "There are two ways to consume Cognitive Services:\n",
      "• Using an HTTP endpoint directly: In this case, the endpoint is invoked directly \n",
      "by crafting both the header and body with appropriate values. The return value is \n",
      "then parsed and data is extracted out of it. All the AI services in Cognitive Services \n",
      "are REST APIs. They accept HTTP requests in JSON, as well as other formats, and \n",
      "replies in JSON format.\n",
      "• Using an SDK: Azure provides multiple software development kits (SDKs). There \n",
      "are SDKs available for the .NET, Python, Node.js, Java, and Go languages.\n",
      "In the following section, we will look into the utilization of one of the Cognitive Services \n",
      "using both ways. Let's explore this by building some AI services using HTTP endpoints.\n",
      "Building an OCR service\n",
      "In this section, we will be using some of the AI services using C# as well as PowerShell\n",
      "---------------\n",
      "to show their usage using the HTTP endpoint directly. The next section will concentrate \n",
      "on doing the same using a .NET SDK.\n",
      "Before getting into building a project using Cognitive Services, the first step is to \n",
      "provision the API itself.\n",
      "Optical character recognition is available as a Vision API and can be provisioned using \n",
      "the Azure portal, as shown next. Create a vision API by navigating to Cognitive Services \n",
      "> Compute Vision > Create, as shown in Figure 19.1:\n",
      "Figure 19.1: Create a Vision API\n",
      "---------------\n",
      "Building an OCR service | 647\n",
      "Once the API is provisioned, the overview page provides all the details for consuming \n",
      "the API. It provides the base URL and the key information. Make a note of the key as it \n",
      "will be used later: \n",
      "Figure 19.2: Overview page\n",
      "It also provides an API console to quickly test the API. Clicking on it opens a new \n",
      "window that has all the endpoints related to this service available. Clicking on OCR \n",
      "will provide a form that can be filled in with appropriate data and execute the service \n",
      "endpoints. It also provides a complete response. This is shown in Figure 19.3. The URL \n",
      "is available as a request URL, and the request is a typical HTTP request with a POST \n",
      "method. The URL points to the endpoint in the East US Azure region. It is also related to \n",
      "the Vision group of APIs, version 2, and the OCR endpoint.\n",
      "---------------\n",
      "648 | Architecting intelligent solutions\n",
      "The subscription key is passed in the header with the name ocp-apim-subscription-key. \n",
      "The header also contains the content-type key with application/json as a value. This is \n",
      "because the body of the request contains a JSON string. The body is in the form of JSON \n",
      "with the URL of the image from which text should be extracted:\n",
      "Figure 19.3: Request URL\n",
      "The request can be sent to the endpoint by clicking on the Send button. It will result in \n",
      "an HTTP response 200 OK, as shown next, if everything goes right. If there is an error in \n",
      "the request values, the response will be an error HTTP code:\n",
      "Figure 19.4: HTTP response 200 OK\n",
      "---------------\n",
      "Building an OCR service | 649\n",
      "The response consists of details related to billing usage, an internal request ID \n",
      "generated by the endpoint, the content length, the response content type (being JSON), \n",
      "and the data and time of the response. The content of the response consists of a JSON \n",
      "payload with the coordinates of the text and the actual text itself.\n",
      "Using PowerShell\n",
      "The same request can be created using PowerShell. The following PowerShell code can \n",
      "be executed using the PowerShell ISE.\n",
      "The code uses the Invoke-WebRequest cmdlet to invoke the Cognitive Services endpoint \n",
      "by passing the URL to the Uri parameter using the POST method, and adds both the \n",
      "appropriate headers as discussed in the last section, and finally, the body consisting of \n",
      "data in JSON format. The data is converted into JSON using the ConvertTo-Json cmdlet:\n",
      "$ret = Invoke-WebRequest -Uri \"https://eastus.api.cognitive.microsoft.\n",
      "com/vision/v2.0/ocr?language=en&detectOrientation=true\"  -Method Post\n",
      "---------------\n",
      "-Headers @{\"Ocp-Apim-Subscription-Key\"=\"ff0cd61f27d8452bbadad36942\n",
      "570c48\"; \"Content-type\"=\"application/json\"} -Body $(ConvertTo-Json \n",
      "-InputObject  @{\"url\"=\"https://ichef.bbci.co.uk/news/320/cpsprodpb/F944/\n",
      "production/_109321836_oomzonlz.jpg\"}) \n",
      "$val = Convertfrom-Json $ret.content\n",
      "foreach ($region in $val.regions) {\n",
      "    foreach($line in $region.lines) {\n",
      "        foreach($word in $line.words) {\n",
      "            $word.text\n",
      "        }\n",
      "    }\n",
      "} \n",
      "The response from the cmdlet is saved in a variable that also consists of data in JSON \n",
      "format. The data is converted into a PowerShell object using the Convertfrom-Json \n",
      "cmdlet and looped over to find the words in the text.\n",
      "---------------\n",
      "650 | Architecting intelligent solutions\n",
      "Using C#\n",
      "In this section, we will build a service that should accept requests from users, extract \n",
      "the URL of the image, construct the HTTP request, and send it to the Cognitive Services \n",
      "endpoint. The Cognitive Services endpoint returns a JSON response. The appropriate \n",
      "text content is extracted from the response and returned to the user.\n",
      "Architecture and design\n",
      "An intelligent application is an ASP.NET Core MVC application. An MVC application is \n",
      "built by a developer on a developer machine, goes through the continuous integration \n",
      "and delivery pipeline, generates a Docker image, and uploads the Docker image to \n",
      "Azure Container Registry. Here, the major components of the application are explained, \n",
      "along with their usage:\n",
      "Figure 19.5: Workflow of an intelligent application\n",
      "Azure\n",
      "Container Registry\n",
      "kube-proxy\n",
      "Pod\n",
      "Instance 2\n",
      "Pod\n",
      "Instance 1\n",
      "Load BalancerUsers\n",
      "kubelet\n",
      "Push Docker images\n",
      "Developer\n",
      "Kubernetes\n",
      "Worker nodes\n",
      "Virtual Network\n",
      "Kubernetes\n",
      "---------------\n",
      "Master nodes\n",
      "Scheduler\n",
      "API Server\n",
      "Controller\n",
      "Manager\n",
      "Pull images\n",
      "---------------\n",
      "Building an OCR service | 651\n",
      "Docker\n",
      "Docker is one of the major players within container technologies and is available cross-\n",
      "platform, including Linux, Windows, and Mac. Developing applications and services \n",
      "with containerization in mind provides the flexibility to deploy them across clouds \n",
      "and locations, as well as on-premises. It also removes any dependencies on the host \n",
      "platform, which again allows less reliance on platform as a service. Docker helps with \n",
      "the creation of custom images, and containers can be created out of these images. The \n",
      "images contain all the dependencies, binaries, and frameworks needed to make the \n",
      "application or service work, and they are completely self-reliant. This makes them a \n",
      "great deployment target for services such as microservices.\n",
      "Azure Container Registry\n",
      "Azure Container Registry is a registry that's similar to Docker Hub for the storage \n",
      "of container images in a repository. It is possible to create multiple repositories and\n",
      "---------------\n",
      "upload multiple images in them. An image has a name and a version number, together \n",
      "forming a fully qualified name used to refer to them in a Kubernetes Pod definition. \n",
      "These images can be accessed and downloaded by any Kubernetes ecosystem. A \n",
      "prerequisite of this is that appropriate secrets for pulling the image should already be \n",
      "created beforehand. It need not be on the same network as Kubernetes nodes and, in \n",
      "fact, there is no need for a network to create and use Azure Container Registry.\n",
      "Azure Kubernetes Service\n",
      "The intelligent application that accepts the URL of an image to retrieve the text in it \n",
      "can be hosted on vanilla virtual machines or even within Azure App Service. However, \n",
      "deploying in Azure Kubernetes Service offers lots of advantages, which was covered \n",
      "in Chapter 8, Architecting Azure Kubernetes Solutions. For now, it is important to know \n",
      "that these applications are self-healing in nature and a minimum number of instances is\n",
      "---------------\n",
      "automatically maintained by the Kubernetes master along with providing the flexibility \n",
      "to update them in a multitude of ways, including blue-green deployments and canary \n",
      "updates.\n",
      "Pods, replica sets, and deployments\n",
      "The developer also creates a Kubernetes deployment-related YAML file that references \n",
      "the images within the Pod specification and also provides a specification for the replica \n",
      "set. It provides its own specification related to the update strategy.\n",
      "---------------\n",
      "652 | Architecting intelligent solutions\n",
      "Runtime design\n",
      "The architecture and design remain the same as in the previous section; however, when \n",
      "the application or service is already live and up and running, it has already downloaded \n",
      "the images from Azure Container Registry and created Pods running containers \n",
      "in them. When a user provides an image URL for decoding the text it contains, the \n",
      "application in the Pod invokes the Azure Cognitive Services Computer Vision API and \n",
      "passes the URL to it and waits for a response from the service:\n",
      "Figure 19.6 Workflow of an intelligent application\n",
      "Once it receives the JSON response from the services, it can retrieve the information \n",
      "and return it to the user.\n",
      "The development process\n",
      "The development environment can be Windows or Linux. It will work with both \n",
      "Windows 10 and the Windows 2016/19 server. When using Windows, it is useful to \n",
      "deploy Docker for Windows so that it will create both a Linux and a Windows Docker \n",
      "environment. \n",
      "Azure\n",
      "---------------\n",
      "Cognitive Services\n",
      "kube-proxy\n",
      "Pod\n",
      "Instance 2\n",
      "Pod\n",
      "Instance 1\n",
      "Load BalancerUsers\n",
      "kubelet\n",
      "Push Docker images\n",
      "Developer\n",
      "Kubernetes\n",
      "Worker nodes\n",
      "Virtual Network\n",
      "Kubernetes\n",
      "Master nodes\n",
      "Scheduler\n",
      "API Server\n",
      "Controller\n",
      "Manager\n",
      "Request/Response\n",
      "---------------\n",
      "Building an OCR service | 653\n",
      "When creating an ASP.NET Core web application project using Visual Studio 2019, the \n",
      "Docker support option should be selected with either Windows or Linux as values. \n",
      "Depending on the chosen value, appropriate content will be generated in Dockerfile. \n",
      "The main difference in Dockerfile is the base image names. It uses different images for \n",
      "Linux compared to Windows.\n",
      "When installing Docker for Windows, it also installs a Linux virtual machine, and so it is \n",
      "important to turn on the Hyper-V hypervisor.\n",
      "In this example, instead of sending the data as a JSON string, the image is downloaded, \n",
      "and binary data is sent to the Cognitive Services endpoint.\n",
      "It has a function that accepts a string input for URL values. It then invokes Cognitive \n",
      "Services with appropriate header values and a body containing the URL. The header \n",
      "values should contain the key provided by Cognitive Services while provisioning the\n",
      "---------------\n",
      "service. The value in the body can contain vanilla string values in the form of JSON or it \n",
      "can contain binary image data itself. The content-type header property should be set \n",
      "accordingly.\n",
      "The code declares the URL and the key related to the Cognitive Services. This is shown \n",
      "for demonstration purposes only. The URL and key should be placed in configuration \n",
      "files.\n",
      "Using the HttpClient object, the image corresponding to the URL supplied by the user is \n",
      "downloaded and stored within the responseMessage variable. Another HttpClient object \n",
      "is instantiated and its headers are filled with Ocp-Apim-Subscription-Key and content-\n",
      "type keys. The value of the content-type header is application/octet-stream since \n",
      "binary data is being passed to the endpoint.\n",
      "A post request is made after extracting the content from the responseMessage variable \n",
      "and passing it as the body of a request to the cognitive service endpoint. \n",
      "The code for the controller action is shown next: \n",
      "        [HttpPost]\n",
      "---------------\n",
      "public async Task<string> Post([FromBody] string value)\n",
      "        {\n",
      "            string myurl = \" https://eastus.api.cognitive.microsoft.com/\n",
      "vision/v2.0/ocr?language=en&detectOrientation=true\n",
      "            string token = \"…………………………\";\n",
      "            using (HttpClient httpClient = new HttpClient())\n",
      "            {\n",
      "                var responseMessage = await httpClient.GetAsync(value);\n",
      "---------------\n",
      "654 | Architecting intelligent solutions\n",
      "                 using (var httpClient1 = new HttpClient())\n",
      "                {\n",
      "                     httpClient1.BaseAddress = new Uri(myurl);\n",
      "                   httpClient1.DefaultRequestHeaders.Add(\"Ocp-Apim-\n",
      "Subscription-Key\", token);\n",
      "                    HttpContent content = responseMessage.Content;\n",
      "                    content.Headers.ContentType = new \n",
      "MediaTypeWithQualityHeaderValue(\"application/octet-stream\");\n",
      "                    var response = await httpClient1.PostAsync(myurl, \n",
      "content);\n",
      "                    var responseContent = await response.Content.\n",
      "ReadAsByteArrayAsync();\n",
      "                    string ret = Encoding.ASCII.GetString(responseContent, \n",
      "0, responseContent.Length);\n",
      "                    dynamic image = JsonConvert.\n",
      "DeserializeObject<object>(ret);\n",
      "                    string temp = \"\";\n",
      "                    foreach (var regs in image.regions)\n",
      "                    {\n",
      "                        foreach (var lns in regs.lines)\n",
      "---------------\n",
      "{\n",
      "                            foreach (var wds in lns.words)\n",
      "                            {\n",
      "                                temp += wds.text + \" \";\n",
      "                            }\n",
      "                        }\n",
      "                    }\n",
      "                    return temp;\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "---------------\n",
      "Building a visual features service using the Cognitive Search .NET SDK | 655\n",
      "After the endpoint finishes its processing, it returns the response with a JSON payload. \n",
      "The context is extracted and deserialized into .NET objects. Multiple loops are coded to \n",
      "extract the text from the response.\n",
      "In this section, we created a simple application that uses Cognitive Services to provide \n",
      "word extractions from features using the OCR API and deployed it within Kubernetes \n",
      "Pods. This process and architecture can be used within any application that wants \n",
      "to consume Cognitive Services APIs. Next, we will take a look at another Cognitive \n",
      "Services API, known as visual features.\n",
      "Building a visual features service using the Cognitive Search .NET \n",
      "SDK\n",
      "The last section was about creating a service that uses an OCR cognitive endpoint to \n",
      "return text within images. In this section, a new service will be created that will return \n",
      "visual features within an image, such as descriptions, tags, and objects.\n",
      "---------------\n",
      "Using PowerShell\n",
      "The code in PowerShell is similar to the previous OCR example, so it is not repeated \n",
      "here. The URL is different from the previous code example:\n",
      "Figure 19.7: Request URL\n",
      "The request is made using a POST method, and the URL points to the endpoint in the \n",
      "East US Azure region. It also uses version 2 and consumes the Vision API.\n",
      "The Cognitive Services access key is part of the HTTP header named ocp-apim-\n",
      "subscription-key. The header also contains the header content-type with application/\n",
      "json as the value. This is because the body of the request contains a JSON value. The \n",
      "body has the URL of the image from which text should be extracted.\n",
      "---------------\n",
      "656 | Architecting intelligent solutions\n",
      "The response will be in JSON format containing the image content and a description.\n",
      "Using .NET\n",
      "This example is again an ASP.NET Core MVC application and has the Microsoft.Azure.\n",
      "CognitiveServices.Vision.ComputerVision NuGet package installed in it: \n",
      "Figure 19.8: ASP.NET Core MVC application with the Microsoft.Azure.CognitiveServices.Vision.\n",
      "ComputerVision NuGet package\n",
      "The code for the controller action is shown next. In this code, the cognitive service and \n",
      "key are declared. It also declares variables for the ComputerVisionClient and VisionType \n",
      "objects. It creates an instance of the ComputerVisionClient type, providing it the URL \n",
      "and the key.\n",
      "The VisionTypes list consists of multiple types of data sought from the image—tags, \n",
      "descriptions, and objects are added. Only these parameters will be extracted from the \n",
      "image.\n",
      "An HttpClient object is instantiated to download the image using the URL provided\n",
      "---------------\n",
      "by the user and sends this binary data to the Cognitive Services endpoint using the \n",
      "AnalyzeImageInStreamAsync function of type ComputerVisionClient:\n",
      "---------------\n",
      "Building a visual features service using the Cognitive Search .NET SDK | 657\n",
      "[HttpPost]\n",
      "        public string Post([FromBody] string value)\n",
      "        {\n",
      "        private string visionapiurl = \" https://\n",
      "eastus.api.cognitive.microsoft.com/vision/v2.0/\n",
      "analyze?visualFeaure=tags,description,objects&language=en\";\n",
      "        private string apikey = \"e55d36ac228f4d718d365f1fcddc0851\";\n",
      "        private ComputerVisionClient client;\n",
      "        private List<VisualFeatureTypes> visionType =  new \n",
      "List<VisualFeatureTypes>();\n",
      "client = new ComputerVisionClient(new ApiKeyServiceClientCredentials(apikey)) \n",
      "{ \n",
      "                Endpoint = visionapiurl\n",
      "            };\n",
      "            visionType.Add(VisualFeatureTypes.Description);\n",
      "            visionType.Add(VisualFeatureTypes.Tags);\n",
      "            visionType.Add(VisualFeatureTypes.Objects);\n",
      "            \n",
      "            string tags = \"\";\n",
      "            string descrip   = \"\";\n",
      "            string objprop = \"\";\n",
      "            using (HttpClient hc = new HttpClient()) {\n",
      "---------------\n",
      "var responseMessage =  hc.GetAsync(value).GetAwaiter().\n",
      "GetResult();\n",
      "                Stream streamData = responseMessage.Content.\n",
      "ReadAsStreamAsync().GetAwaiter().GetResult();\n",
      "                var result = client.AnalyzeImageInStreamAsync(streamData, \n",
      "visionType).GetAwaiter().GetResult();\n",
      "                foreach (var tag in result.Tags) {\n",
      "                    tags += tag.Name + \" \";\n",
      "                }\n",
      "                foreach (var caption in result.Description.Captions)\n",
      "---------------\n",
      "658 | Architecting intelligent solutions\n",
      "                {\n",
      "                    descrip += caption.Text + \" \";\n",
      "                }\n",
      "                foreach (var obj in result.Objects)\n",
      "                {\n",
      "                    objprop += obj.ObjectProperty + \" \";\n",
      "                }\n",
      "            }\n",
      " return tags;\n",
      " // return descrip or objprop\n",
      "            \n",
      "        }\n",
      " \n",
      "The results are looped through and tags are returned to the user. Similarly, descriptions \n",
      "and object properties can also be returned to the user. Now let's check out the ways we \n",
      "can safeguard the exposure of service keys.\n",
      "Safeguarding the Cognitive Services key\n",
      "There are multiple ways to safeguard the exposure of keys to other actors. This can \n",
      "be done using the API Management resource in Azure. It can also be done using Azure \n",
      "Functions Proxies.\n",
      "Using Azure Functions Proxies\n",
      "Azure Functions Proxies can refer to any URL, whether internal or external. When a\n",
      "---------------\n",
      "request reaches Azure Functions Proxies, it will use the URL of the cognitive service \n",
      "along with the key to invoke the cognitive endpoint, and it will also override the request \n",
      "parameters and add the incoming image URL and append it to the cognitive endpoint \n",
      "URL as POST data. When a response comes back from the service, it will override the \n",
      "response, remove the headers, and pass JSON data back to the user.\n",
      "---------------\n",
      "Consuming Cognitive Services | 659\n",
      "Consuming Cognitive Services\n",
      "Consuming Cognitive Services follows a consistent pattern. Each cognitive service is \n",
      "available as a REST API, with each API expecting different sets of parameters to work \n",
      "on. Clients invoking these URLs should check out the documentation for associate \n",
      "parameters and provide values for them. Consuming URLs is a relatively raw method \n",
      "of using Cognitive Services. Azure provides SDKs for each service and for multiple \n",
      "languages. Clients can use these SDKs to work with Cognitive Services.\n",
      "The LUIS (Language Understanding Intelligence Service) authoring API is available \n",
      "at https://{luis resource name}-authoring.cognitiveservices.azure.com/ and the \n",
      "production API is available at \n",
      "https://{azure region}.api.cognitive.microsoft.com/luis/prediction/v3.0/apps/\n",
      "{application id}/slots/production/predict?subscription-key={cognitive key} \n",
      "&verbose=true&show-all-intents=true&log=true&query=YOUR_QUERY_HERE.\n",
      "---------------\n",
      "Similarly, the Face API is available at https://{endpoint}/face/v1.0/\n",
      "detect[?returnFaceId][&returnFaceLandmarks][&returnFaceAttributes]\n",
      "[&recognitionModel][&returnRecognitionModel][&detectionModel].\n",
      "There are many Cognitive Services APIs, with each having multiple flavors in terms of \n",
      "URLs, and the best way to know about these URLs is to use the Azure documentation.\n",
      "Summary\n",
      "In this chapter, you gained an understanding of the deployment architecture and \n",
      "application architecture for creating intelligent applications in Azure. Azure provides \n",
      "Cognitive Services with numerous endpoints—each endpoint is responsible for \n",
      "executing an AI-related algorithm and providing outputs. Almost all Cognitive Services \n",
      "endpoints work in a similar manner with regard to HTTP requests and responses. These \n",
      "endpoints can also be invoked using SDKs provided by Azure for different languages, \n",
      "and you saw an example of obtaining visual features using them. There are more than\n",
      "---------------\n",
      "50 different endpoints, and you are advised to get an understanding of the nature of \n",
      "endpoints using the API console feature provided by Azure.\n",
      "---------------\n",
      "By developers,\n",
      "for developers\n",
      "Microsoft.Source newsletter\n",
      "Get technical articles, sample \n",
      "code, and information on \n",
      "upcoming events in \n",
      "Microsoft.Source, the \n",
      "curated monthly developer \n",
      "community newsletter.\n",
      "●  Keep up on the latest \n",
      " technologies\n",
      "●  Connect with your peers \n",
      " at community events\n",
      "●  Learn with \n",
      " hands-on resources\n",
      "Sign upSign up\n",
      "---------------\n",
      "About\n",
      "All major keywords used in this book are captured alphabetically in this section. Each one is \n",
      "accompanied by the page number of where they appear.\n",
      "Index\n",
      ">\n",
      "---------------\n",
      "A\n",
      "access: 3, 8, 10, 12-13, \n",
      "19, 71, 73-74, 76, 80, \n",
      "84-85, 87, 110, 113, 115, \n",
      "145-148, 156, 158-159, \n",
      "163-165, 168, 170-171, \n",
      "177-178, 184, 189-190, \n",
      "204-206, 208, 210, 213, \n",
      "221, 226-227, 230-232, \n",
      "234, 237-243, 245, \n",
      "247-252, 256, 265, 267, \n",
      "275, 287, 294, 297-298, \n",
      "319, 322, 358-359, \n",
      "361-363, 371, 395-396, \n",
      "399, 402, 442, 446, 458, \n",
      "479, 486, 503-504, 517, \n",
      "522, 545, 553, 565-566, \n",
      "569, 576, 604, 609, \n",
      "627, 632, 637, 645, 655\n",
      "account: 38, 52, 60, 64, \n",
      "87, 105-107, 109-111, 113, \n",
      "116-117, 119, 121, 132, 134, \n",
      "136-137, 139, 141, 151, \n",
      "157, 172, 174, 176-177, 179, \n",
      "184, 191, 194, 220-221, \n",
      "233, 248-252, 257, 274, \n",
      "279-284, 286-289, \n",
      "293, 296-297, 309-312, \n",
      "318-320, 329, 332, \n",
      "335-339, 347-348, \n",
      "358, 361-364, 366, \n",
      "369, 371-372, 375, 378, \n",
      "399-400, 405, 408, \n",
      "410-413, 415, 417, 443, \n",
      "458, 467-468, 483, 504, \n",
      "511, 516-519, 522, 524, \n",
      "526-529, 532, 535, 538, \n",
      "554, 559, 565-569\n",
      "adappname: 361\n",
      "addcontent: 374\n",
      "address: 29, 33, 35, 37, \n",
      "73, 75, 77, 93, 226, 233, \n",
      "236, 246, 250, 256,\n",
      "---------------\n",
      "268, 446, 458-459, \n",
      "471, 480, 485-486, \n",
      "501-503, 526, 528-529, \n",
      "560-561, 570, 572\n",
      "aggregates: 405, 407\n",
      "alerts: 27, 58, 61, 63-66, \n",
      "129, 133, 171, 198, \n",
      "229, 237, 247-248, \n",
      "254, 266, 333, 578\n",
      "algorithm: 50, 659\n",
      "analytics: 8, 50, 61-64, \n",
      "66-67, 70, 74, 193, 244, \n",
      "247-248, 265-266, 269, \n",
      "273, 276-277, 333, 347, \n",
      "351, 389-392, 403-408, \n",
      "411, 413-416, 418, 421, \n",
      "442-443, 462, 472, 534, \n",
      "578, 580, 582-588, \n",
      "598, 601-607, 609-614, \n",
      "616, 619-623, 625-627, \n",
      "631-633, 635-637, 644\n",
      "android: 222, 317\n",
      "ansible: 428, 430, 442\n",
      "apache: 7, 275-276, 298, \n",
      "392, 587, 606-607, 637\n",
      "api-based: 116\n",
      "apikey: 657\n",
      "apiversion: 151, 157, \n",
      "485, 488-490, 496, \n",
      "498-499, 511-512, 514, \n",
      "520, 524, 527-528, 530, \n",
      "533-539, 542-544, \n",
      "551-554, 568, 571-572\n",
      "appname: 485, \n",
      "488-491, 496\n",
      "architect: 20, 23-24, 67, \n",
      "71, 101, 162, 179, 184, \n",
      "190-191, 269, 343, \n",
      "355, 386, 603, 632\n",
      "artifacts: 106-107, 112, \n",
      "162-163, 230, 285-286, \n",
      "395, 429, 432, 434-436, \n",
      "441, 444, 459-460, 644\n",
      "assemblies: 265, \n",
      "432, 448, 460\n",
      "---------------\n",
      "attack: 228, 237, 248, 265\n",
      "audits: 187\n",
      "automate: 17-18, 20, 101, \n",
      "105, 109, 184, 428, 475, \n",
      "613, 619, 622, 633\n",
      "autoscale: 48, 53, 315\n",
      "azcopy: 634\n",
      "azureappid: 363, 367\n",
      "azureblob: 536\n",
      "azurecr: 485, 490-491\n",
      "azurerm: 119, 122-123\n",
      "azure-sql: 199, 213, 217\n",
      "B\n",
      "backend: 163, 310, \n",
      "315, 321, 505\n",
      "backup: 46, 201, 606, 632\n",
      "bacpac: 429\n",
      "balancer: 29, 32-35, \n",
      "38-39, 45, 50, 189, 245, \n",
      "458-459, 487-488, \n",
      "498, 500, 526\n",
      "bandwidth: 42, 70, 73, \n",
      "76, 79, 87, 215, 634\n",
      "binary: 272, 406, 439, \n",
      "645, 653, 656\n",
      "binding: 307-311, \n",
      "326, 330-332\n",
      "blobadded: 335, 340\n",
      "blobsource: 539\n",
      "browser: 17, 263, 295, \n",
      "470, 500, 576\n",
      "bundle: 518, 520\n",
      "---------------\n",
      "C\n",
      "caching: 58, 190, 612\n",
      "callback: 261\n",
      "canary: 651\n",
      "cassandra: 7, 219-221, 298\n",
      "certkey: 115\n",
      "cgroups: 476\n",
      "client: 16-17, 36-37, \n",
      "57-58, 85, 238, 241, \n",
      "245, 248, 254, 257, \n",
      "259-260, 276, 321, 380, \n",
      "463, 481, 580, 657\n",
      "clouds: 7, 67, 72, 136, 139, \n",
      "141, 145, 173, 633, 651\n",
      "cluster: 29, 200, 275-276, \n",
      "295, 478, 480-481, \n",
      "483-484, 486-487, \n",
      "490, 492-495, \n",
      "500-506, 604-605\n",
      "cmdlet: 18, 112, 116-118, 121, \n",
      "127, 137-139, 150, 159, \n",
      "340, 342, 358, 361, 368, \n",
      "469-470, 516, 519, 649\n",
      "cognitive: 8, 317, \n",
      "325, 347, 637, 640, \n",
      "643-646, 649-650, \n",
      "652-653, 655-659\n",
      "columnar: 199, 298, 610\n",
      "compile: 108, 430-431, 470\n",
      "concurrent: 195, 606, 617\n",
      "configure: 12, 19, 27, 40, \n",
      "62, 66, 130-131, 134, \n",
      "136-137, 162, 197, 218, \n",
      "222, 233, 259-261, 266, \n",
      "292, 307, 313, 333, 346, \n",
      "348, 350-352, 361, 371, \n",
      "412, 414-415, 429-430, \n",
      "433, 446, 457, 460, \n",
      "467-468, 470, 473, 495, \n",
      "513, 530, 567, 589, 594\n",
      "constraint: 79, 198, \n",
      "502, 625\n",
      "container: 4, 10, 15-16, 32, \n",
      "50, 60, 147, 178, 190, 211, \n",
      "220, 280, 282, 288-289,\n",
      "---------------\n",
      "297, 316, 318-321, 330, \n",
      "340, 395, 413-414, 417, \n",
      "438, 443, 462-463, 473, \n",
      "475, 477-479, 484-485, \n",
      "493, 496, 500-501, \n",
      "505-507, 522, 558, \n",
      "565-566, 569, 650-652\n",
      "cosmosdb: 330\n",
      "D\n",
      "dacpac: 448\n",
      "daemon: 16-17, 463\n",
      "dashboard: 61, 107, \n",
      "172-173, 176, 180, \n",
      "247, 373, 443\n",
      "database: 6-7, 16, 39-40, \n",
      "106, 111, 163, 178, 190, \n",
      "193-199, 201-202, 204, \n",
      "206, 208-215, 217-220, \n",
      "222, 231, 242-243, \n",
      "252-253, 255-256, 269, \n",
      "330-331, 405, 432, 442, \n",
      "445-446, 448, 534-536, \n",
      "557, 569, 577, 585-586, \n",
      "610, 612, 618, 634\n",
      "databricks: 272, 274, 276, \n",
      "293-295, 297, 302-303, \n",
      "510, 534, 586, 607\n",
      "datacenter: 7, 26-27, 29, \n",
      "31, 36, 38, 61, 70, 76, \n",
      "80-82, 86, 197, 230, \n",
      "251, 253, 392, 545, \n",
      "556-557, 565, 596, 613\n",
      "dataops: 619\n",
      "dataset: 179, 282, \n",
      "287-289, 292, 298, 406, \n",
      "536-538, 541, 581\n",
      "decryption: 33, \n",
      "227, 239-240\n",
      "deployment: 4-8, 10-12, \n",
      "14, 17, 19-20, 24-25, \n",
      "39, 42, 45, 49, 58-59, \n",
      "72-75, 78, 152, 163-164, \n",
      "179-180, 184, 188, 190, \n",
      "194-200, 211, 213, \n",
      "218-219, 222, 225-226, \n",
      "228, 230-231, 233,\n",
      "---------------\n",
      "235-236, 239, 244-245, \n",
      "247, 256, 268, 306, \n",
      "308, 314-315, 343, 358, \n",
      "425-427, 429, 431-437, \n",
      "440, 443, 445-446, \n",
      "460, 463, 466, 472-473, \n",
      "475, 478-479, 483, 485, \n",
      "488-490, 492, 498-500, \n",
      "506, 509, 512-513, \n",
      "516-521, 525-526, \n",
      "534, 545, 547-548, \n",
      "550-551, 554, 556, \n",
      "567, 569-570, 572-573, \n",
      "607, 642, 651, 659\n",
      "devops: 6, 11, 226-227, \n",
      "286, 303, 418, 421-427, \n",
      "430, 435-444, 457-458, \n",
      "461-462, 464-466, \n",
      "471-473, 479, 493\n",
      "dnsserver: 544\n",
      "downstream: 14, 274, \n",
      "410, 582, 614\n",
      "E\n",
      "ecosystem: 1, 6-8, 44, 303, \n",
      "390, 424, 443, 477, 484, \n",
      "585, 603, 612, 637, 651\n",
      "emulator: 327\n",
      "---------------\n",
      "encryption: 33, 70, 75, \n",
      "79, 85, 208-209, 227, \n",
      "231, 239, 251, 253-254, \n",
      "267-268, 358, 632\n",
      "endpoints: 4, 19, 29, \n",
      "35-38, 40, 125, 256-257, \n",
      "265, 268, 316, 321, \n",
      "325, 334, 486-487, \n",
      "588-589, 591-593, \n",
      "612, 643-647, 659\n",
      "enrolment: 168\n",
      "entities: 84, 87, \n",
      "220, 227, 586\n",
      "eventgrid: 336, \n",
      "342, 365, 381\n",
      "eventtype: 335, 341, 368\n",
      "extension: 36, 50, 240, \n",
      "307, 325, 336, 377-378, \n",
      "429, 463, 558, 564, 610\n",
      "F\n",
      "failover: 40, 78, 86, 89, 221\n",
      "filename: 115, 137, \n",
      "297, 497, 537\n",
      "filesystem: 208, \n",
      "237, 360-361\n",
      "firewall: 29, 33, 38, 40, 74, \n",
      "189, 204-205, 233-237, \n",
      "245-247, 252-253, 265, \n",
      "269, 446, 458, 483, \n",
      "493, 557, 570, 596\n",
      "formats: 256, 272-273, \n",
      "277, 298, 317, \n",
      "405-406, 583, 646\n",
      "framework: 9, 18, 46, \n",
      "222, 275, 307-308, \n",
      "423, 429, 468, 501\n",
      "frequency: 84, 184, 348, \n",
      "412, 537-538, 540, 581\n",
      "G\n",
      "github: 54, 293, 303, \n",
      "535, 545, 577\n",
      "gremlin: 219-221\n",
      "H\n",
      "hadoop: 7, 269, 272, \n",
      "274-275, 303, 389, \n",
      "586-587, 622\n",
      "header: 125-127, 252, \n",
      "297-298, 341-342, \n",
      "368-369, 406, 495, 592, \n",
      "644-646, 648, 653, 655\n",
      "---------------\n",
      "horizontal: 51, 212\n",
      "hosting: 1, 3, 14, 30, 33, \n",
      "38, 46, 74, 131, 133, 189, \n",
      "197-198, 226, 229, 238, \n",
      "285, 315, 343, 407, 414, \n",
      "445, 476, 493, 503, \n",
      "506, 526, 532, 584\n",
      "hostname: 485\n",
      "httpclient: 653-654, \n",
      "656-657\n",
      "httppost: 653, 657\n",
      "httpstart: 325-326\n",
      "I\n",
      "identifier: 117, 128, 241, \n",
      "289, 335, 375, 569\n",
      "images: 14, 17, 58, 84, 272, \n",
      "316, 443, 463, 479, 641, \n",
      "643-644, 651-653, 655\n",
      "import: 112, 122-123, \n",
      "137, 300, 362, \n",
      "364-365, 469, 636\n",
      "ingestion: 273, 277, \n",
      "391-392, 399, 403-404, \n",
      "578, 581, 584-585, \n",
      "587, 606-607, 612, \n",
      "633, 635, 641\n",
      "installer: 18\n",
      "instance: 12, 26, 30, 32-33, \n",
      "36, 46-47, 49, 54, \n",
      "56-57, 64, 70, 148, 152, \n",
      "182, 185, 194, 198-199, \n",
      "210, 213-214, 222, 245, \n",
      "272, 277, 279-280, 282, \n",
      "289, 346, 357-358, \n",
      "390-391, 393, 487, \n",
      "492-493, 501-503, \n",
      "519-520, 522-524, \n",
      "550, 581, 632, 656\n",
      "integrity: 76, 227, 625\n",
      "invoicing: 168, 176-177\n",
      "isolation: 4, 10, 15-16, \n",
      "73, 76, 83, 90, 195, \n",
      "220, 247, 432, 497, \n",
      "549, 596, 604-605\n",
      "J\n",
      "javascript: 19, 58, 265, \n",
      "307, 405, 407, 430\n",
      "jenkins: 6, 418, 422,\n",
      "---------------\n",
      "464-466\n",
      "K\n",
      "kernel: 15-16, 462, 476\n",
      "keyvault: 241, 365, \n",
      "521, 525, 569\n",
      "kubectl: 495-497, \n",
      "499-500, 504\n",
      "kubelet: 483, 493, 504-507\n",
      "L\n",
      "lambda: 307\n",
      "libraries: 17, 58, 265, \n",
      "432, 463, 640\n",
      "license: 188\n",
      "---------------\n",
      "localhost: 137, 139-140, 471\n",
      "locations: 14, 18, 58, \n",
      "81, 86, 153, 164, 221, \n",
      "519, 556, 651\n",
      "logging: 59-60, 74, 115, \n",
      "141, 238, 263, 265, \n",
      "358, 477, 479, 532\n",
      "lookup: 405\n",
      "M\n",
      "machine: 7, 25, 30, 57, \n",
      "65, 70-72, 74, 84, 99, \n",
      "127, 130, 145, 161, 180, \n",
      "185-186, 188-190, \n",
      "197-198, 213, 231-233, \n",
      "235, 238, 256, 268, 274, \n",
      "276, 279, 295, 315, 356, \n",
      "360, 389-390, 405, \n",
      "407, 429-430, 442-443, \n",
      "460, 462, 467, 471, 473, \n",
      "478-479, 526, 528-530, \n",
      "532, 541-545, 551, 556, \n",
      "558-560, 562-565, \n",
      "570, 572, 586-587, \n",
      "603-604, 606-607, \n",
      "622, 637, 650, 653\n",
      "mainframe: 1\n",
      "mariadb: 196\n",
      "master: 169, 213, 275-276, \n",
      "294, 343, 448, 478-484, \n",
      "492, 500, 535, 545, \n",
      "555, 557, 565-567, \n",
      "571-572, 651\n",
      "memory: 14, 44, 46-48, \n",
      "51, 186, 199, 204, \n",
      "217-218, 361, 408, 633\n",
      "metadata: 12-13, 51, 152, \n",
      "167, 309, 485, 488-491, \n",
      "496, 498-499, 564, \n",
      "591-592, 609-610, \n",
      "613, 619, 632\n",
      "migration: 11, 279-280, \n",
      "602, 611, 613-617, \n",
      "619-621, 623, 625, \n",
      "632-637, 641\n",
      "modeling: 228-229, 642\n",
      "monitoring: 11, 23, 32, 42, \n",
      "52, 58-59, 61-63, 66-67,\n",
      "---------------\n",
      "74, 187, 198, 210-211, \n",
      "222, 229, 231, 244, 265, \n",
      "267-269, 279, 313, 316, \n",
      "319, 389, 435, 443, 462, \n",
      "471-472, 477, 483, 577, \n",
      "588, 607, 610, 642\n",
      "N\n",
      "namespace: 15, 279, \n",
      "283, 393-396, 400, \n",
      "403, 409-410, 497, \n",
      "504, 512, 551\n",
      "native: 178, 213, 257, \n",
      "479, 501, 609-610\n",
      "nginx-lb: 499-500\n",
      "notebook: 295-296\n",
      "O\n",
      "offload: 33, 343, 637\n",
      "on-demand: 2-3, 6, 14, \n",
      "240, 306, 318, 603-604, \n",
      "606-608, 639, 641\n",
      "openid: 227, 257\n",
      "oracle: 7\n",
      "os-level: 30\n",
      "P\n",
      "package: 18, 243, 255, 324, \n",
      "429, 435, 446, 564, 656\n",
      "partition: 275, 398, \n",
      "400-402\n",
      "password: 112-116, 210, \n",
      "240-241, 359-361, \n",
      "364, 380, 386, 495, \n",
      "502, 536, 544\n",
      "pattern: 69, 86, 90-101, \n",
      "218, 265, 333, 415, \n",
      "466, 555, 584, \n",
      "592-593, 603, 659\n",
      "payload: 35, 66, 184, 227, \n",
      "308-309, 335, 341, 368, \n",
      "382, 495, 649, 655\n",
      "petabytes: 8, 85, 274, \n",
      "582, 588, 606\n",
      "pipeline: 278, 280, \n",
      "282-284, 286-287, 291, \n",
      "293, 431, 433-434, \n",
      "443, 446, 448, 452, \n",
      "457, 459-461, 463-464, \n",
      "472, 538, 541, 603, \n",
      "609, 612, 633, 650\n",
      "playbook: 247\n",
      "policy: 8, 13, 56, 145-148, \n",
      "152-153, 155-156,\n",
      "---------------\n",
      "162-164, 174, 278, \n",
      "361, 395-396, 399, \n",
      "522, 540, 565\n",
      "postgresql: 6, 196\n",
      "postman: 127, 328\n",
      "powershell: 8, 10, 17-20, \n",
      "50, 72, 108-109, 111-112, \n",
      "114-115, 117-118, 120-121, \n",
      "125, 127, 134, 148-150, \n",
      "152, 157-159, 163, 237, \n",
      "266, 316, 340, 356-357, \n",
      "362, 364, 366, 386, \n",
      "429, 440, 443, 458, \n",
      "466-468, 470, 472, \n",
      "493, 510, 512-513, 515, \n",
      "519, 521, 543-544, 564, \n",
      "567, 646, 649, 655\n",
      "premium: 30, 47-48, \n",
      "84, 87, 190, 200, \n",
      "212, 294, 315, 568\n",
      "---------------\n",
      "pricing: 141, 168, 178, \n",
      "184-185, 191, 214-215, \n",
      "217-220, 222, 247, 315, \n",
      "347, 370, 393-394, 409, \n",
      "594-595, 598, 608\n",
      "principal: 59, 107, 111, 113, \n",
      "115-118, 121, 161, 210, \n",
      "241, 252, 287, 358-362, \n",
      "367, 380-381, 516, 569\n",
      "protected: 66, 75, \n",
      "230-231, 236, 239, 247, \n",
      "251, 256-257, 265, 360, \n",
      "522, 553, 593, 645\n",
      "protocol: 29, 33, 35, \n",
      "84-85, 88, 227, \n",
      "232-233, 251, 257, 341, \n",
      "370, 392, 485, 488, \n",
      "490-491, 497, 579-580, \n",
      "584, 588-589, 593\n",
      "psgallery: 564\n",
      "publish: 333-334, 341-342, \n",
      "357, 368-369, 385, 586\n",
      "pyspark: 300\n",
      "python: 6, 109, 112, 307, \n",
      "316, 356, 466, 586, \n",
      "603-604, 606-607, 646\n",
      "R\n",
      "raspberry: 589\n",
      "rawdata: 282\n",
      "readable: 19, 202, 405, 430\n",
      "reboot: 27, 30, 564\n",
      "rebuild: 503\n",
      "recovery: 4, 7, 197-198, \n",
      "201, 221-222, 513, 545\n",
      "redundancy: 25-27, \n",
      "58, 86, 189, 200\n",
      "regions: 7, 12-14, 27-29, \n",
      "36-41, 67, 69-73, \n",
      "75, 79-80, 101, 106, \n",
      "163-164, 189, 191, 197, \n",
      "202-203, 219, 221, 230, \n",
      "246, 333, 512, 515, 519, \n",
      "545, 556, 649, 654\n",
      "registry: 15, 237, 316, \n",
      "479, 485, 650-652\n",
      "reliable: 82, 84, 88-89,\n",
      "---------------\n",
      "101, 103, 275-276, 426, \n",
      "578, 580, 597, 640\n",
      "remote: 81, 98, \n",
      "238, 246, 459\n",
      "replicas: 86, 221, 489, \n",
      "491, 498-499\n",
      "replicaset: 482-483, \n",
      "488, 490-493\n",
      "report: 176, 180, 182, 184, \n",
      "436, 442, 577, 606\n",
      "resilient: 29, 38, 98, \n",
      "274, 298, 436\n",
      "response: 33, 43, 99, \n",
      "127-128, 131, 182, 184, \n",
      "221-222, 233, 244, 247, \n",
      "315, 321, 493, 647-650, \n",
      "652, 654-656, 658\n",
      "retrieve: 19, 150, 159, \n",
      "182, 241, 277, 330, 332, \n",
      "341, 368, 651-652\n",
      "revoke: 250\n",
      "rgname: 569\n",
      "role-based: 8, 115, 145, \n",
      "242, 294, 504, 609, 633\n",
      "runbook: 105-106, 109, \n",
      "111, 118-121, 123-129, \n",
      "131-132, 134-136, 141, \n",
      "333, 356-357, 366-367, \n",
      "369, 379, 382, 385, 466\n",
      "S\n",
      "scaling: 3-4, 44-49, 51, \n",
      "53-54, 56-57, 197, 315, \n",
      "475, 477, 576, 594\n",
      "scenarios: 29, 53-54, 76, \n",
      "78-79, 96, 107, 109, \n",
      "136, 180, 184, 222, 277, \n",
      "301, 323, 392, 457, \n",
      "584-585, 603, 609, 614\n",
      "schedule: 53-54, 98, \n",
      "237, 276, 307, 346, \n",
      "369, 385, 425, 483, \n",
      "493, 505, 586, 622\n",
      "scopes: 170, 172, 237\n",
      "script: 107, 111, 115, \n",
      "118-121, 134, 136-138, \n",
      "152, 237, 251, 356,\n",
      "---------------\n",
      "512, 516-518, 521, 525, \n",
      "543, 558, 564-565\n",
      "secret: 239-241, 261-262, \n",
      "357, 359-360, 368-369, \n",
      "519-521, 523-524, 569\n",
      "sendgrid: 355, 357, \n",
      "370-373, 375, \n",
      "377-379, 386\n",
      "sentinel: 243-245, 247\n",
      "servicebus: 325, 395, 400\n",
      "servicenow: 66\n",
      "session: 15-16, 29, \n",
      "32-33, 35, 38, 40, \n",
      "263, 302, 392, 459\n",
      "setting: 49, 57, 85, 107, \n",
      "131, 136, 161, 211, 217, \n",
      "266, 285, 315-316, 366, \n",
      "371-373, 375, 378-379, \n",
      "522, 566, 592, 605, 607\n",
      "sharepoint: 233\n",
      "signature: 85, 113, 249, \n",
      "287, 395, 399, 522, 553\n",
      "skuname: 533\n",
      "slaves: 275\n",
      "---------------\n",
      "socket: 239, 392\n",
      "sqlazure: 534\n",
      "sqlclient: 636\n",
      "sqldataset: 538-540\n",
      "sql-query: 586\n",
      "standalone: 236, 242, 244\n",
      "stateless: 57, 142, 392\n",
      "static: 58, 65, 75, 190, \n",
      "312, 320, 373, 576\n",
      "streaming: 266, 391-392, \n",
      "403, 407-408, 414, \n",
      "418, 587, 637\n",
      "structured: 84, \n",
      "272-273, 641\n",
      "subnet: 37, 50, 73, 75, 207, \n",
      "232-233, 235, 238-239, \n",
      "501-503, 506-507, \n",
      "527-529, 562, 567\n",
      "subscribe: 333-334, \n",
      "338, 379\n",
      "subscribed: 333\n",
      "switch: 27, 172, 262, \n",
      "478, 495, 615\n",
      "synapse: 405, 586-587, \n",
      "598, 601-614, \n",
      "616, 619-637\n",
      "sysadmin: 567\n",
      "T\n",
      "target: 29, 33, 87, 113, \n",
      "242-243, 277-278, 289, \n",
      "292-293, 310, 315, 319, \n",
      "390, 407, 426, 443, \n",
      "468, 613, 619, 651\n",
      "template: 19-20, 56, 157, \n",
      "336, 348, 358, 446, \n",
      "489, 491, 498, 510-519, \n",
      "521-522, 524-527, 529, \n",
      "532, 534-535, 538, \n",
      "541, 547-561, 563, \n",
      "565-568, 570-573, 614\n",
      "tenant: 77, 96, 117, \n",
      "257, 358, 361, 363, \n",
      "367, 380, 569\n",
      "terraform: 428\n",
      "tokens: 85, 87, 182, 245, \n",
      "249-250, 252, 569, 593\n",
      "tracking: 63, 76, 178, \n",
      "247, 255, 437, 577\n",
      "traffic: 29, 33-38, 40, \n",
      "45-47, 72, 74, 77, 79,\n",
      "---------------\n",
      "82, 106, 142, 188-189, \n",
      "234, 237-238, 245, \n",
      "248, 390, 501, 589\n",
      "twilio: 355, 357, 371-373, \n",
      "375, 378-379, 386\n",
      "twitter: 257, 265, 269, \n",
      "307, 406, 411-413, 415\n",
      "U\n",
      "ubuntu: 235\n",
      "upgrade: 54-56, 489, 594\n",
      "V\n",
      "validation: 277, 431, \n",
      "433-434, 437, \n",
      "460, 471, 481\n",
      "vaults: 241, 268, 358, 366, \n",
      "368, 379, 525, 569\n",
      "version: 11-12, 19, 51, 56, \n",
      "274, 279, 284-285, 293, \n",
      "295, 308, 324, 430, \n",
      "438-439, 441, 444, 478, \n",
      "488-490, 510-513, 516, \n",
      "531, 592, 604, 607, 610, \n",
      "645, 647, 651, 655\n",
      "virtualize: 15, 462\n",
      "W\n",
      "warehouse: 277, \n",
      "586-587, 601-606, \n",
      "609-612, 614-619, \n",
      "622-623, 625-627, \n",
      "632-634, 636-637\n",
      "web-based: 33, 606\n",
      "webdeploy: 429, 432\n",
      "webhook: 66, 119, 125-127, \n",
      "129, 131, 333, 342\n",
      "webserver: 468, 470, 496\n",
      "webserver-: 496\n",
      "windows: 1, 6, 14-18, 50, \n",
      "63, 108, 188, 210, 234, \n",
      "236, 243, 297-298, 316, \n",
      "395, 400, 429-430, \n",
      "442, 462, 466, 476-477, \n",
      "503, 536, 552-553, \n",
      "558, 564-565, 596, \n",
      "642, 651-653\n",
      "workbooks: 247\n",
      "worker: 105-106, 111, \n",
      "119, 127, 134-136, 276, \n",
      "294-295, 478-481, \n",
      "483, 492, 500\n",
      "---------------\n",
      "workflows: 108-109, 278, \n",
      "321-322, 346, 350, 355, \n",
      "466, 586, 609, 622\n",
      "workloads: 26, 58, 211, \n",
      "226, 233, 235, 237, 317, \n",
      "587, 604-605, 613\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.page_content)\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of tokens in a text\n",
    "\n",
    "Like LLM models, Embedding models defines a `max input`. It is defined in number of `tokens`. The `max_input` for `text-embedding-3-large` is 8191 tokens. So we need to split the text into chunks of 8191 tokens or less. For that, you need to get the number of tokens in a text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name=\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(string, disallowed_special=()))\n",
    "    return num_tokens\n",
    "\n",
    "# Test the function\n",
    "num_tokens_from_string(\"tiktoken is great!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of tokens in each chunk. It should not exceed the max specified by the Embedding model (8191)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    num_tokens = num_tokens_from_string(chunk.page_content)\n",
    "    if num_tokens > 8191:\n",
    "        print(chunk.metadata[\"page\"])\n",
    "        print(num_tokens)\n",
    "        print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings\n",
    "\n",
    "`Vector search` is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\n",
    "\n",
    "`LangChain` supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-openai --quiet\n",
    "%pip install python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "if os.path.exists(\".env\"):\n",
    "    load_dotenv(override=True)\n",
    "\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_EMBEDDING_API_VERSION\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated vectors of length 3072\n",
      "\n",
      "[0.010515968315303326, -0.01905210129916668, -0.017833741381764412, 0.022737640887498856, 0.008901641704142094, -0.006392581854015589, -0.011467811651527882, 0.024169212207198143, -0.0178185123950243, 0.02282901667058468]\n",
      "[-0.002384420484304428, -0.014230713248252869, -0.017530273646116257, 0.012981644831597805, 0.021096978336572647, -0.03303893655538559, -0.021400220692157745, 0.027956021949648857, -0.031335003674030304, 0.007270587608218193]\n"
     ]
    }
   ],
   "source": [
    "vector_1 = embeddings.embed_query(chunks[0].page_content)\n",
    "vector_2 = embeddings.embed_query(chunks[1].page_content)\n",
    "\n",
    "print(f\"Generated vectors of length {len(vector_1)}\\n\")\n",
    "print(vector_1[:10])\n",
    "print(vector_2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\n",
    "\n",
    "## Vector stores for Azure AI Search\n",
    "\n",
    "`LangChain VectorStore` objects contain methods for adding text and `Document` objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\n",
    "\n",
    "`LangChain` includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as `Postgres`) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet azure-search-documents\n",
    "%pip install --upgrade --quiet azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings, AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings: AzureOpenAIEmbeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_EMBEDDING_API_VERSION\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create vector store instance\n",
    "\n",
    "Create instance of the `AzureSearch` class using the embeddings from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "# Specify additional properties for the Azure client such as the following https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/core/azure-core/README.md#configurations\n",
    "vector_store: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"],\n",
    "    azure_search_key=os.environ[\"AZURE_SEARCH_SERVICE_ADMIN_KEY\"],\n",
    "    index_name=os.environ[\"AZURE_SEARCH_SERVICE_INDEX\"],\n",
    "    embedding_function=embeddings.embed_query,\n",
    "    # Configure max retries for the Azure client\n",
    "    additional_search_client_options={\"retry_total\": 3},\n",
    "    relevance_score_fn=\"cosine\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert text and embeddings into vector store\n",
    "\n",
    "This step loads, chunks, and vectorizes the sample document, and then indexes the content into a search index on `Azure AI Search`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading doscuments from  0  to  300\n",
      "Uploading doscuments from  300  to  600\n",
      "Uploading doscuments from  600  to  900\n",
      "Uploading doscuments from  900  to  1200\n",
      "Uploading doscuments from  1200  to  1500\n"
     ]
    }
   ],
   "source": [
    "# vector_store.add_documents(documents=chunks)\n",
    "\n",
    "# You can import search documents into a specified index using HTTP POST. \n",
    "# For a large update, batching (up to 1000 documents per batch, or about 16 MB per batch) \n",
    "# is recommended and will significantly improve indexing performance.\n",
    "\n",
    "# upload documents per batch of 300\n",
    "\n",
    "for i in range(0, len(chunks), 300):\n",
    "    print(\"Uploading doscuments from \", i, \" to \", i+300)\n",
    "    vector_store.add_documents(documents=chunks[i:i+300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've instantiated a VectorStore that contains documents, we can query it. VectorStore includes methods for querying:\n",
    "\n",
    "* Synchronously and asynchronously;\n",
    "* By string query and by vector;\n",
    "* With and without returning similarity scores;\n",
    "* By similarity and maximum marginal relevance (to balance similarity with query to diversity in retrieved results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a vector similarity search with relevance scores\n",
    "\n",
    "Execute a pure vector similarity search using the `similarity_search_with_relevance_scores()` method. Queries that don't meet the threshold requirements are exluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(metadata={'id': 'NmMzNDQyMjUtZDAwMS00MTFmLTkyZmItOTIyOTYwMzAyZTc4', 'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.0 (Windows)', 'creationdate': '2021-06-17T13:34:27+05:30', 'author': 'Ritesh Modi', 'moddate': '2021-06-17T14:21:14+05:30', 'subject': 'Create secure, scalable, high-availability applications on the cloud', 'title': 'Azure for Architects, Third Edition', 'trapped': '/False', 'source': './example_data/azure-for-architects.pdf', 'total_pages': 701, 'page': 1, 'page_label': 'a', 'start_index': 0}, page_content='Ritesh Modi, Jack Lee, and Rithin Skaria\\nCreate secure, scalable, high-availability \\napplications on the cloud\\nAzure for Architects\\nThird Edition'),\n",
      "  0.77307135),\n",
      " (Document(metadata={'id': 'ZGE2NGJjMGUtOTA1Yy00MjNkLTk5M2QtM2I4NDc4OGUzZDM1', 'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.0 (Windows)', 'creationdate': '2021-06-17T13:34:27+05:30', 'author': 'Ritesh Modi', 'moddate': '2021-06-17T14:21:14+05:30', 'subject': 'Create secure, scalable, high-availability applications on the cloud', 'title': 'Azure for Architects, Third Edition', 'trapped': '/False', 'source': './example_data/azure-for-architects.pdf', 'total_pages': 701, 'page': 28, 'page_label': 'ii', 'start_index': 1872}, page_content='technical certifications. His hobbies are writing books, playing with his daughter, \\nwatching movies, and learning new technologies. He currently lives in Hyderabad, India. \\nYou can follow him on Twitter at @automationnext.\\nJack Lee is a senior Azure certified consultant and an Azure practice lead with a \\npassion for software development, cloud, and DevOps innovations. Jack has been \\nrecognized as a Microsoft MVP for his contributions to the tech community. He \\nhas presented at various user groups and conferences, including the Global Azure \\nBootcamp at Microsoft Canada. Jack is an experienced mentor and judge at hackathons \\nand is also the president of a user group that focuses on Azure, DevOps, and software \\ndevelopment. He is the co-author of Cloud Analytics with Microsoft Azure, published by \\nPackt Publishing. You can follow Jack on Twitter at @jlee_consulting.'),\n",
      "  0.7499432),\n",
      " (Document(metadata={'id': 'MGQ2YmY4ZGEtM2JlYS00MmZmLWI5MTItZDM0OTU3NTI2Y2Uz', 'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.0 (Windows)', 'creationdate': '2021-06-17T13:34:27+05:30', 'author': 'Ritesh Modi', 'moddate': '2021-06-17T14:21:14+05:30', 'subject': 'Create secure, scalable, high-availability applications on the cloud', 'title': 'Azure for Architects, Third Edition', 'trapped': '/False', 'source': './example_data/azure-for-architects.pdf', 'total_pages': 701, 'page': 29, 'page_label': 'iii', 'start_index': 0}, page_content='About Azure for Architects, Third Edition | iii\\nRithin Skaria is an open source evangelist with over 7 years of experience of managing \\nopen source workloads in Azure, AWS, and OpenStack. He is currently working \\nfor Microsoft and is a part of several open source community activities conducted \\nwithin Microsoft. He is a Microsoft Certified Trainer, Linux Foundation Certified \\nEngineer and Administrator, Kubernetes Application Developer and Administrator, \\nand also a Certified OpenStack Administrator. When it comes to Azure, he has four \\ncertifications (solution architecture, Azure administration, DevOps, and security), and \\nhe is also certified in Office 365 administration. He has played a vital role in several \\nopen source deployments, and the administration and migration of these workloads \\nto the cloud. He also co-authored Linux Administration on Azure, published by Packt \\nPublishing. Connect with him on LinkedIn at @rithin-skaria.\\nAbout the Reviewers'),\n",
      "  0.7482719),\n",
      " (Document(metadata={'id': 'ZmI2ODIwNGYtMjQwMi00MGU0LWJmYmMtOTkyZjJiY2E0Nzcz', 'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.0 (Windows)', 'creationdate': '2021-06-17T13:34:27+05:30', 'author': 'Ritesh Modi', 'moddate': '2021-06-17T14:21:14+05:30', 'subject': 'Create secure, scalable, high-availability applications on the cloud', 'title': 'Azure for Architects, Third Edition', 'trapped': '/False', 'source': './example_data/azure-for-architects.pdf', 'total_pages': 701, 'page': 2, 'page_label': 'b', 'start_index': 0}, page_content='Azure for Architects Third Edition\\nCopyright © 2020 Packt Publishing\\nAll rights reserved. No part of this book may be reproduced, stored in a retrieval system, \\nor transmitted in any form or by any means, without the prior written permission of the \\npublisher, except in the case of brief quotations embedded in critical articles or reviews.\\nEvery effort has been made in the preparation of this book to ensure the accuracy of \\nthe information presented. However, the information contained in this book is sold \\nwithout warranty, either express or implied. Neither the authors, nor Packt Publishing, \\nand its dealers and distributors will be held liable for any damages caused or alleged to \\nbe caused directly or indirectly by this book.\\nPackt Publishing has endeavored to provide trademark information about all of the \\ncompanies and products mentioned in this book by the appropriate use of capitals. \\nHowever, Packt Publishing cannot guarantee the accuracy of this information.'),\n",
      "  0.7238531)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "docs_and_scores = vector_store.similarity_search_with_relevance_scores(\n",
    "    query=\"Who are the authors of the book Azure for Architects ?\",\n",
    "    k=4,\n",
    "    score_threshold=0.70,\n",
    ")\n",
    "\n",
    "pprint(docs_and_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a hybrid search\n",
    "\n",
    "`Hybrid search` combines keyword and semantic similarity, marrying the benefits of both approaches.\n",
    "Execute hybrid search using the `search_type` or `hybrid_search()` method. Vector and nonvector text fields are queried in parallel, results are merged, and top matches of the unified result set are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'id': 'NmMzNDQyMjUtZDAwMS00MTFmLTkyZmItOTIyOTYwMzAyZTc4', 'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.0 (Windows)', 'creationdate': '2021-06-17T13:34:27+05:30', 'author': 'Ritesh Modi', 'moddate': '2021-06-17T14:21:14+05:30', 'subject': 'Create secure, scalable, high-availability applications on the cloud', 'title': 'Azure for Architects, Third Edition', 'trapped': '/False', 'source': './example_data/azure-for-architects.pdf', 'total_pages': 701, 'page': 1, 'page_label': 'a', 'start_index': 0}, page_content='Ritesh Modi, Jack Lee, and Rithin Skaria\\nCreate secure, scalable, high-availability \\napplications on the cloud\\nAzure for Architects\\nThird Edition'),\n",
      " Document(metadata={'id': 'MGQ2YmY4ZGEtM2JlYS00MmZmLWI5MTItZDM0OTU3NTI2Y2Uz', 'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.0 (Windows)', 'creationdate': '2021-06-17T13:34:27+05:30', 'author': 'Ritesh Modi', 'moddate': '2021-06-17T14:21:14+05:30', 'subject': 'Create secure, scalable, high-availability applications on the cloud', 'title': 'Azure for Architects, Third Edition', 'trapped': '/False', 'source': './example_data/azure-for-architects.pdf', 'total_pages': 701, 'page': 29, 'page_label': 'iii', 'start_index': 0}, page_content='About Azure for Architects, Third Edition | iii\\nRithin Skaria is an open source evangelist with over 7 years of experience of managing \\nopen source workloads in Azure, AWS, and OpenStack. He is currently working \\nfor Microsoft and is a part of several open source community activities conducted \\nwithin Microsoft. He is a Microsoft Certified Trainer, Linux Foundation Certified \\nEngineer and Administrator, Kubernetes Application Developer and Administrator, \\nand also a Certified OpenStack Administrator. When it comes to Azure, he has four \\ncertifications (solution architecture, Azure administration, DevOps, and security), and \\nhe is also certified in Office 365 administration. He has played a vital role in several \\nopen source deployments, and the administration and migration of these workloads \\nto the cloud. He also co-authored Linux Administration on Azure, published by Packt \\nPublishing. Connect with him on LinkedIn at @rithin-skaria.\\nAbout the Reviewers'),\n",
      " Document(metadata={'id': 'ZGE2NGJjMGUtOTA1Yy00MjNkLTk5M2QtM2I4NDc4OGUzZDM1', 'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.0 (Windows)', 'creationdate': '2021-06-17T13:34:27+05:30', 'author': 'Ritesh Modi', 'moddate': '2021-06-17T14:21:14+05:30', 'subject': 'Create secure, scalable, high-availability applications on the cloud', 'title': 'Azure for Architects, Third Edition', 'trapped': '/False', 'source': './example_data/azure-for-architects.pdf', 'total_pages': 701, 'page': 28, 'page_label': 'ii', 'start_index': 1872}, page_content='technical certifications. His hobbies are writing books, playing with his daughter, \\nwatching movies, and learning new technologies. He currently lives in Hyderabad, India. \\nYou can follow him on Twitter at @automationnext.\\nJack Lee is a senior Azure certified consultant and an Azure practice lead with a \\npassion for software development, cloud, and DevOps innovations. Jack has been \\nrecognized as a Microsoft MVP for his contributions to the tech community. He \\nhas presented at various user groups and conferences, including the Global Azure \\nBootcamp at Microsoft Canada. Jack is an experienced mentor and judge at hackathons \\nand is also the president of a user group that focuses on Azure, DevOps, and software \\ndevelopment. He is the co-author of Cloud Analytics with Microsoft Azure, published by \\nPackt Publishing. You can follow Jack on Twitter at @jlee_consulting.')]\n"
     ]
    }
   ],
   "source": [
    "# Perform a hybrid search using the search_type parameter\n",
    "docs = vector_store.similarity_search(\n",
    "    query=\"Who are the authors of the book Azure for Architects ?\",\n",
    "    k=3,\n",
    "    search_type=\"hybrid\",\n",
    ")\n",
    "pprint(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval and Generation using LangGraph\n",
    "\n",
    "Now let’s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n",
    "\n",
    "For generation, we will use the chat model selected at the start of the tutorial.\n",
    "\n",
    "We’ll use a prompt for `RAG` that is checked into the `LangChain` prompt hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain import hub\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hodellai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of *Azure for Architects, Third Edition* are Rithin Skaria and Ritesh Modi. It was published in 2020.\n"
     ]
    }
   ],
   "source": [
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "question = \"Who are the authors of the book Azure for Architects and when it was published ?\"\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(question)\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "prompt = prompt.invoke({\"question\": question, \"context\": docs_content})\n",
    "answer = llm.invoke(prompt)\n",
    "\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hodellai\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The authors of the book *Azure for Architects* (Third Edition), published in 2020, are Rithin Skaria and Ritesh Modi.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain import hub\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    ")\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "response = graph.invoke({\"question\": \"Who are the authors of the book Azure for Architects and when it was published ?\"})\n",
    "\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph also comes with built-in utilities for visualizing the control flow of your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
