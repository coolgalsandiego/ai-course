{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è OpenAI\n",
    "\n",
    "## Backend pool Load Balancing lab\n",
    "![flow](../../images/backend-pool-load-balancing.gif)\n",
    "\n",
    "Playground to try the built-in load balancing [backend pool functionality of APIM](https://learn.microsoft.com/en-us/azure/api-management/backends?tabs=bicep) to either a list of Azure OpenAI endpoints or mock servers.\n",
    "\n",
    "Notes:\n",
    "- The backend pool uses round-robin by default\n",
    "- But priority and weight based routing are also supported: Adjust the `priority` (the lower the number, the higher the priority) and `weight` parameters in the `openai_resources` variable\n",
    "- The `retry` API Management policy initiates a retry to an available backend if an HTTP 429 status code is encountered\n",
    "- Use the mock servers by cleaning the `openai_resources` variable and simulate custom behavior by changing the code in the [app.py](../../tools/mock-server/app.py) file \n",
    "\n",
    "### Result\n",
    "![result](result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sdk'></a>\n",
    "### üß™ Test the API using the Azure OpenAI Python SDK\n",
    "\n",
    "Repeat the same test using the Python SDK to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APIM Resource Gateway URL:  https://apim-external-300prod.azure-api.net\n",
      "APIM Subscription Key:  da0ffc371d324190b27991f4ac0e17e9\n"
     ]
    }
   ],
   "source": [
    "apim_resource_gateway_url = ! terraform output -raw apim_resource_gateway_url\n",
    "apim_resource_gateway_url = apim_resource_gateway_url[0]\n",
    "apim_subscription_key = ! terraform output -raw apim_subscription_key\n",
    "apim_subscription_key = apim_subscription_key[0]\n",
    "openai_api_version = \"2024-10-21\"\n",
    "openai_model_name = \"gpt-4o\"\n",
    "openai_deployment_name = \"gpt-4o\"\n",
    "\n",
    "print(\"APIM Resource Gateway URL: \", apim_resource_gateway_url)\n",
    "print(\"APIM Subscription Key: \", apim_subscription_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='requests'></a>\n",
    "### üß™ Test the API using a direct HTTP call\n",
    "Requests is an elegant and simple HTTP library for Python that will be used here to make raw API requests and inspect the responses. \n",
    "\n",
    "You will not see HTTP 429s returned as API Management's `retry` policy will select an available backend. If no backends are viable, an HTTP 503 will be returned.\n",
    "\n",
    "Tip: Use the [tracing tool](../../tools/tracing.ipynb) to track the behavior of the backend pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è Run: 1 / 20\n",
      "‚åö 5.58 seconds\n",
      "Response status: \u001b[1;32m200 - OK\u001b[0m\n",
      "Response headers: {'Content-Type': 'application/json', 'Date': 'Sat, 14 Dec 2024 11:23:12 GMT', 'Cache-Control': 'private', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Vary': 'Accept-Encoding', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': '6a7062b8-5db1-45a4-aaa8-2cd071cdc170', 'x-ratelimit-remaining-requests': '199', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': '2e2c1463-788c-435c-9705-e043003a7fc9', 'X-Content-Type-Options': 'nosniff', 'azureml-model-session': 'd059-20241120063825', 'x-ms-region': 'UK South', 'x-envoy-upstream-service-time': '1092', 'x-ms-client-request-id': 'Not-Set', 'x-ratelimit-remaining-tokens': '19340'}\n",
      "x-ms-region: \u001b[1;31mUK South\u001b[0m\n",
      "Token usage: {'completion_tokens': 62, 'prompt_tokens': 30, 'total_tokens': 92} \n",
      "\n",
      "üí¨  Oh sure, just let me check my imaginary watch and sync up with the non-existent satellites orbiting above me. You know, since I'm an AI and all, I don't actually have the ability to know the current time. Why not check a clock or your phone like the rest of us mere mortals? \n",
      "\n",
      "‚ñ∂Ô∏è Run: 2 / 20\n",
      "‚åö 0.94 seconds\n",
      "Response status: \u001b[1;32m200 - OK\u001b[0m\n",
      "Response headers: {'Content-Type': 'application/json', 'Date': 'Sat, 14 Dec 2024 11:23:13 GMT', 'Cache-Control': 'private', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Vary': 'Accept-Encoding', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': '0544e1b5-b068-405f-b4ea-8ece7f9d6107', 'x-ratelimit-remaining-requests': '198', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': '2ead0f7b-eb69-49d2-b736-117363dc26e2', 'X-Content-Type-Options': 'nosniff', 'azureml-model-session': 'd054-20241115143238', 'x-ms-region': 'UK South', 'x-envoy-upstream-service-time': '642', 'x-ms-client-request-id': 'Not-Set', 'x-ratelimit-remaining-tokens': '18680'}\n",
      "x-ms-region: \u001b[1;31mUK South\u001b[0m\n",
      "Token usage: {'completion_tokens': 46, 'prompt_tokens': 30, 'total_tokens': 76} \n",
      "\n",
      "üí¨  Oh sure, let me just check my non-existent watch... Right, it's exactly half-past 'use a clock'. Why not use that device in your hand that you used to talk to me? It might just tell you! \n",
      "\n",
      "‚ñ∂Ô∏è Run: 3 / 20\n",
      "‚åö 1.32 seconds\n",
      "Response status: \u001b[1;32m200 - OK\u001b[0m\n",
      "Response headers: {'Content-Type': 'application/json', 'Date': 'Sat, 14 Dec 2024 11:23:14 GMT', 'Cache-Control': 'private', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Vary': 'Accept-Encoding', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': 'be24e970-a2fc-42e4-a073-a2beef1c8403', 'x-ratelimit-remaining-requests': '197', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': 'c503a908-6597-4dd8-aac5-f9a4a87241f6', 'X-Content-Type-Options': 'nosniff', 'azureml-model-session': 'd057-20241120063825', 'x-ms-region': 'UK South', 'x-envoy-upstream-service-time': '1055', 'x-ms-client-request-id': 'Not-Set', 'x-ratelimit-remaining-tokens': '18020'}\n",
      "x-ms-region: \u001b[1;31mUK South\u001b[0m\n",
      "Token usage: {'completion_tokens': 52, 'prompt_tokens': 30, 'total_tokens': 82} \n",
      "\n",
      "üí¨  Oh sure, let me just magically access the current time for you! Because, you know, that‚Äôs totally what a text-based AI can do. Maybe try looking at a clock or your phone? They're pretty good at telling time, or so I hear. \n",
      "\n",
      "‚ñ∂Ô∏è Run: 4 / 20\n",
      "‚åö 1.02 seconds\n",
      "Response status: \u001b[1;32m200 - OK\u001b[0m\n",
      "Response headers: {'Content-Type': 'application/json', 'Date': 'Sat, 14 Dec 2024 11:23:15 GMT', 'Cache-Control': 'private', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Vary': 'Accept-Encoding', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': '076ac447-bc4f-4853-bf52-98f86fafc3ee', 'x-ratelimit-remaining-requests': '196', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': '4ec4546d-6abe-40f0-9c39-334928a6086d', 'X-Content-Type-Options': 'nosniff', 'azureml-model-session': 'd058-20241120063825', 'x-ms-region': 'UK South', 'x-envoy-upstream-service-time': '612', 'x-ms-client-request-id': 'Not-Set', 'x-ratelimit-remaining-tokens': '17360'}\n",
      "x-ms-region: \u001b[1;31mUK South\u001b[0m\n",
      "Token usage: {'completion_tokens': 27, 'prompt_tokens': 30, 'total_tokens': 57} \n",
      "\n",
      "üí¨  Oh, sure! Let me just check my invisible watch... and voil√†, it's \"Time You Got a Real Clock\" o'clock! \n",
      "\n",
      "‚ñ∂Ô∏è Run: 5 / 20\n",
      "‚åö 0.74 seconds\n",
      "Response status: \u001b[1;32m200 - OK\u001b[0m\n",
      "Response headers: {'Content-Type': 'application/json', 'Date': 'Sat, 14 Dec 2024 11:23:15 GMT', 'Cache-Control': 'private', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Vary': 'Accept-Encoding', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': '5ae08a18-c740-46b0-8712-f168b320f4d0', 'x-ratelimit-remaining-requests': '195', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': 'f0a7a919-b397-4934-8b2f-b6bf4cb7bc8d', 'X-Content-Type-Options': 'nosniff', 'azureml-model-session': 'd065-20241120171340', 'x-ms-region': 'UK South', 'x-envoy-upstream-service-time': '466', 'x-ms-client-request-id': 'Not-Set', 'x-ratelimit-remaining-tokens': '16700'}\n",
      "x-ms-region: \u001b[1;31mUK South\u001b[0m\n",
      "Token usage: {'completion_tokens': 25, 'prompt_tokens': 30, 'total_tokens': 55} \n",
      "\n",
      "üí¨  Oh sure, let me just peer through the time-space continuum. Or maybe just check a clock, that might work too! \n",
      "\n",
      "‚ñ∂Ô∏è Run: 6 / 20\n",
      "‚åö 1.25 seconds\n",
      "Response status: \u001b[1;32m200 - OK\u001b[0m\n",
      "Response headers: {'Content-Type': 'application/json', 'Date': 'Sat, 14 Dec 2024 11:23:18 GMT', 'Cache-Control': 'private', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Vary': 'Accept-Encoding', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': 'a7bc824b-8a56-4d60-bf46-ef58956678b7', 'x-ratelimit-remaining-requests': '194', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': '61d2c473-9695-427d-bd91-1c8c9de5699b', 'X-Content-Type-Options': 'nosniff', 'azureml-model-session': 'd060-20241120063825', 'x-ms-region': 'UK South', 'x-envoy-upstream-service-time': '789', 'x-ms-client-request-id': 'Not-Set', 'x-ratelimit-remaining-tokens': '16040'}\n",
      "x-ms-region: \u001b[1;31mUK South\u001b[0m\n",
      "Token usage: {'completion_tokens': 40, 'prompt_tokens': 30, 'total_tokens': 70} \n",
      "\n",
      "üí¨  Oh, absolutely! Let me just check my imaginary watch that always conveniently breaks down when someone asks for the time. Maybe try looking at a clock or your phone‚Äîthose tend to be pretty accurate! \n",
      "\n",
      "‚ñ∂Ô∏è Run: 7 / 20\n",
      "‚åö 0.83 seconds\n",
      "Response status: \u001b[1;32m200 - OK\u001b[0m\n",
      "Response headers: {'Content-Type': 'application/json', 'Date': 'Sat, 14 Dec 2024 11:23:19 GMT', 'Cache-Control': 'private', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Vary': 'Accept-Encoding', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': '21a9f293-b762-4262-8d32-96c0e8dfc794', 'x-ratelimit-remaining-requests': '193', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': 'cb540a60-a268-4918-9690-9dcd28c18311', 'X-Content-Type-Options': 'nosniff', 'azureml-model-session': 'd057-20241120063825', 'x-ms-region': 'UK South', 'x-envoy-upstream-service-time': '555', 'x-ms-client-request-id': 'Not-Set', 'x-ratelimit-remaining-tokens': '15380'}\n",
      "x-ms-region: \u001b[1;31mUK South\u001b[0m\n",
      "Token usage: {'completion_tokens': 34, 'prompt_tokens': 30, 'total_tokens': 64} \n",
      "\n",
      "üí¨  Oh, absolutely! I'm just a digital assistant with zero ability to know the current time, but I'm sure you have a clock somewhere around you. Give it a glance! \n",
      "\n",
      "‚ñ∂Ô∏è Run: 8 / 20\n",
      "‚åö 0.77 seconds\n",
      "Response status: \u001b[1;32m200 - OK\u001b[0m\n",
      "Response headers: {'Content-Type': 'application/json', 'Date': 'Sat, 14 Dec 2024 11:23:19 GMT', 'Cache-Control': 'private', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Vary': 'Accept-Encoding', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': 'cc04e3c7-ae59-49e8-8d89-0482b021cd7b', 'x-ratelimit-remaining-requests': '192', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': 'b461c7e6-3f4f-4a9f-99ec-61093d738faf', 'X-Content-Type-Options': 'nosniff', 'azureml-model-session': 'd053-20241115105508', 'x-ms-region': 'UK South', 'x-envoy-upstream-service-time': '484', 'x-ms-client-request-id': 'Not-Set', 'x-ratelimit-remaining-tokens': '14720'}\n",
      "x-ms-region: \u001b[1;31mUK South\u001b[0m\n",
      "Token usage: {'completion_tokens': 41, 'prompt_tokens': 30, 'total_tokens': 71} \n",
      "\n",
      "üí¨  Oh sure, let me just check my imaginary watch! But seriously, I can't tell you the current time. Why not try looking at a clock or your phone? They usually have that sort of information. \n",
      "\n",
      "‚ñ∂Ô∏è Run: 9 / 20\n",
      "‚åö 1.53 seconds\n",
      "Response status: \u001b[1;32m200 - OK\u001b[0m\n",
      "Response headers: {'Content-Type': 'application/json', 'Date': 'Sat, 14 Dec 2024 11:23:21 GMT', 'Cache-Control': 'private', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Vary': 'Accept-Encoding', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': '88a2fc91-83a9-417b-819e-a3ef4727ffd3', 'x-ratelimit-remaining-requests': '199', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': '229cc3e1-dd4e-4725-a7b7-1bfac3f606c3', 'X-Content-Type-Options': 'nosniff', 'azureml-model-session': 'd066-20241120171340', 'x-ms-region': 'France Central', 'x-envoy-upstream-service-time': '858', 'x-ms-client-request-id': 'Not-Set', 'x-ratelimit-remaining-tokens': '19340'}\n",
      "x-ms-region: \u001b[1;31mFrance Central\u001b[0m\n",
      "Token usage: {'completion_tokens': 45, 'prompt_tokens': 30, 'total_tokens': 75} \n",
      "\n",
      "üí¨  Oh sure, let me just check my non-existent watch... Beep boop, it's precisely 'who knows what time it is'! Maybe try looking at your phone or a clock, those tend to be rather punctual! \n",
      "\n",
      "‚ñ∂Ô∏è Run: 10 / 20\n",
      "‚åö 1.08 seconds\n",
      "Response status: \u001b[1;32m200 - OK\u001b[0m\n",
      "Response headers: {'Content-Type': 'application/json', 'Date': 'Sat, 14 Dec 2024 11:23:23 GMT', 'Cache-Control': 'private', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Vary': 'Accept-Encoding', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': '4333ffa8-5aa6-4f60-999f-6ae7a7bb8b44', 'x-ratelimit-remaining-requests': '199', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': '355a1263-4229-436d-b1f7-5bc6ff9bb589', 'X-Content-Type-Options': 'nosniff', 'azureml-model-session': 'd197-20241121102057', 'x-ms-region': 'Sweden Central', 'x-envoy-upstream-service-time': '569', 'x-ms-client-request-id': 'Not-Set', 'x-ratelimit-remaining-tokens': '19340'}\n",
      "x-ms-region: \u001b[1;31mSweden Central\u001b[0m\n",
      "Token usage: {'completion_tokens': 42, 'prompt_tokens': 30, 'total_tokens': 72} \n",
      "\n",
      "üí¨  Sure, let me just check my invisible watch... Oh wait, never mind. Why don‚Äôt you use one of those fancy devices you probably have on you? You know, like a phone or a clock. \n",
      "\n",
      "‚ñ∂Ô∏è Run: 11 / 20\n",
      "‚åö 1.39 seconds\n",
      "Response status: \u001b[1;32m200 - OK\u001b[0m\n",
      "Response headers: {'Content-Type': 'application/json', 'Date': 'Sat, 14 Dec 2024 11:23:24 GMT', 'Cache-Control': 'private', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Vary': 'Accept-Encoding', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': 'b444e802-33ea-4fa2-9186-4271eb6caaa6', 'x-ratelimit-remaining-requests': '191', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': '451d0ce3-67ff-4753-a8f0-56b3f9d08ac7', 'X-Content-Type-Options': 'nosniff', 'azureml-model-session': 'd051-20241115090622', 'x-ms-region': 'UK South', 'x-envoy-upstream-service-time': '794', 'x-ms-client-request-id': 'Not-Set', 'x-ratelimit-remaining-tokens': '14060'}\n",
      "x-ms-region: \u001b[1;31mUK South\u001b[0m\n",
      "Token usage: {'completion_tokens': 45, 'prompt_tokens': 30, 'total_tokens': 75} \n",
      "\n",
      "üí¨  Oh sure, because I'm totally a magical time-keeping device now! Unfortunately, I can't give you the current time, but you can always try checking a clock or looking at one of those fancy digital devices in your pocket. \n",
      "\n",
      "‚ñ∂Ô∏è Run: 12 / 20\n",
      "‚åö 0.81 seconds\n",
      "Response status: \u001b[1;32m200 - OK\u001b[0m\n",
      "Response headers: {'Content-Type': 'application/json', 'Date': 'Sat, 14 Dec 2024 11:23:25 GMT', 'Cache-Control': 'private', 'Content-Encoding': 'gzip', 'Transfer-Encoding': 'chunked', 'Vary': 'Accept-Encoding', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'apim-request-id': 'fffcc988-064f-4a7a-ab40-46ceb35db37c', 'x-ratelimit-remaining-requests': '190', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'X-Request-ID': '0c290e8b-7de1-4cd5-96b3-ef9eae956f79', 'X-Content-Type-Options': 'nosniff', 'azureml-model-session': 'd058-20241120063825', 'x-ms-region': 'UK South', 'x-envoy-upstream-service-time': '536', 'x-ms-client-request-id': 'Not-Set', 'x-ratelimit-remaining-tokens': '13400'}\n",
      "x-ms-region: \u001b[1;31mUK South\u001b[0m\n",
      "Token usage: {'completion_tokens': 42, 'prompt_tokens': 30, 'total_tokens': 72} \n",
      "\n",
      "üí¨  Oh sure, let me just look up at the non-existent clock in my imagination. Actually, why don't you use that magical device in your pocket called a smartphone? It's known for doing exactly that and more! \n",
      "\n",
      "‚ñ∂Ô∏è Run: 13 / 20\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import requests\n",
    "\n",
    "runs = 20\n",
    "sleep_time_ms = 200\n",
    "url = apim_resource_gateway_url + \"/openai/deployments/\" + openai_deployment_name + \"/chat/completions?api-version=\" + openai_api_version\n",
    "api_runs = []\n",
    "\n",
    "for i in range(runs):\n",
    "    print(\"‚ñ∂Ô∏è Run:\", i+1, \"/\", runs)\n",
    "    \n",
    "\n",
    "    messages={\"messages\":[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic unhelpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "    ]}\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers = {'api-key':apim_subscription_key}, json = messages)\n",
    "    response_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚åö {response_time:.2f} seconds\")\n",
    "    # Check the response status code and apply formatting\n",
    "    if 200 <= response.status_code < 300:\n",
    "        status_code_str = '\\x1b[1;32m' + str(response.status_code) + \" - \" + response.reason + '\\x1b[0m'  # Bold and green\n",
    "    elif response.status_code >= 400:\n",
    "        status_code_str = '\\x1b[1;31m' + str(response.status_code) + \" - \" + response.reason + '\\x1b[0m'  # Bold and red\n",
    "    else:\n",
    "        status_code_str = str(response.status_code)  # No formatting\n",
    "\n",
    "    # Print the response status with the appropriate formatting\n",
    "    print(\"Response status:\", status_code_str)\n",
    "    \n",
    "    print(\"Response headers:\", response.headers)\n",
    "    \n",
    "    if \"x-ms-region\" in response.headers:\n",
    "        print(\"x-ms-region:\", '\\x1b[1;31m'+response.headers.get(\"x-ms-region\")+'\\x1b[0m') # this header is useful to determine the region of the backend that served the request\n",
    "        api_runs.append((response_time, response.headers.get(\"x-ms-region\")))\n",
    "    \n",
    "    if (response.status_code == 200):\n",
    "        data = json.loads(response.text)\n",
    "        print(\"Token usage:\", data.get(\"usage\"), \"\\n\")\n",
    "        print(\"üí¨ \", data.get(\"choices\")[0].get(\"message\").get(\"content\"), \"\\n\")\n",
    "    else:\n",
    "        print(response.text)   \n",
    "\n",
    "    time.sleep(sleep_time_ms/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂Ô∏è Run:  1\n",
      "‚åö 1.67 seconds\n",
      "üí¨  Oh, sure thing! Let me just consult my imaginary watch. Unfortunately, I can't actually tell you the time because I'm not connected to any real-world clock. But hey, time flies when you're sarcastic, doesn't it?\n",
      "‚ñ∂Ô∏è Run:  2\n",
      "‚åö 1.08 seconds\n",
      "üí¨  Oh sure, let me just check my imaginary watch with its imaginary hands and its imaginary numbers... done! The current time is probably somewhere between too late and too early. Hope that helps!\n",
      "‚ñ∂Ô∏è Run:  3\n",
      "‚åö 1.20 seconds\n",
      "üí¨  Of course! Just hold on while I consult my vast collection of clocks. Oh wait, I can‚Äôt actually do that. Maybe try looking at a device that does tell the time? Like a phone or a watch.\n",
      "‚ñ∂Ô∏è Run:  4\n",
      "‚åö 1.06 seconds\n",
      "üí¨  Oh, sure, let me just check my imaginary watch... oh wait, I forgot I can‚Äôt tell time! Why don't you check your phone or a clock nearby?\n",
      "‚ñ∂Ô∏è Run:  5\n",
      "‚åö 1.04 seconds\n",
      "üí¨  Oh, sure! Let me just reach into my magical database of current times that I definitely have access to and... Oh wait, that's right, I can't actually tell you the time. Maybe try a clock, or that fancy smartphone you carry around everywhere?\n",
      "‚ñ∂Ô∏è Run:  6\n",
      "‚åö 1.10 seconds\n",
      "üí¨  Oh sure, let me just check my wristwatch, calendar, and sundial simultaneously. Actually, I can‚Äôt tell you the time. Maybe you should try looking at a clock?\n",
      "‚ñ∂Ô∏è Run:  7\n",
      "‚åö 1.56 seconds\n",
      "üí¨  Oh sure, let me just check my imaginary watch... Yep, it's adventure o'clock! You should definitely find a clock or use your phone for the real time though.\n",
      "‚ñ∂Ô∏è Run:  8\n",
      "‚åö 0.98 seconds\n",
      "üí¨  Oh sure, let me just peek out my window and consult the timeless stars in the sky. Or, you know, you could just check the device you're using to talk to me.\n",
      "‚ñ∂Ô∏è Run:  9\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'code': 'ResourceNotFound', 'message': 'Subdomain does not map to a resource.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m\n\u001b[0;32m     15\u001b[0m client \u001b[38;5;241m=\u001b[39m AzureOpenAI(\n\u001b[0;32m     16\u001b[0m     azure_endpoint\u001b[38;5;241m=\u001b[39mapim_resource_gateway_url,\n\u001b[0;32m     17\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapim_subscription_key,\n\u001b[0;32m     18\u001b[0m     api_version\u001b[38;5;241m=\u001b[39mopenai_api_version\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 23\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenai_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m response_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚åö \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\resources\\chat\\completions.py:668\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    667\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\_base_client.py:1260\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1248\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1257\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1258\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1259\u001b[0m     )\n\u001b[1;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\_base_client.py:937\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    930\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    935\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    936\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\openai\\_base_client.py:1041\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1038\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1040\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1044\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1045\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1049\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[0;32m   1050\u001b[0m )\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'code': 'ResourceNotFound', 'message': 'Subdomain does not map to a resource.'}}"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "runs = 9\n",
    "sleep_time_ms = 0\n",
    "\n",
    "for i in range(runs):\n",
    "    print(\"‚ñ∂Ô∏è Run: \", i+1)\n",
    "\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a sarcastic unhelpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you tell me the time, please?\"}\n",
    "    ]\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=apim_resource_gateway_url,\n",
    "        api_key=apim_subscription_key,\n",
    "        api_version=openai_api_version\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = client.chat.completions.create(model=openai_model_name, messages=messages)\n",
    "    \n",
    "    response_time = time.time() - start_time\n",
    "    print(f\"‚åö {response_time:.2f} seconds\")\n",
    "    print(\"üí¨ \", response.choices[0].message.content)\n",
    "    time.sleep(sleep_time_ms/1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "### üóëÔ∏è Clean up resources\n",
    "\n",
    "When you're finished with the lab, you should remove all your deployed resources from Azure to avoid extra charges and keep your Azure subscription uncluttered.\n",
    "Use the [clean-up-resources notebook](clean-up-resources.ipynb) for that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
