{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) with Azure AI Search and OpenAI\n",
    "\n",
    "This code demonstrates how to work with RAG to give more context to the LLM/SLM models to get a more accurate answer. The code uses Azure AI Search to index the documents and Azure OpenAI's embedding model to generate embeddings/vectors for the documents.\n",
    "\n",
    "+ Create an index schema\n",
    "+ Load the sample data from a local folder\n",
    "+ Embed the documents in-memory using Azure OpenAI's text-embedding-ada-002 model\n",
    "+ Index the vector and non-vector fields on Azure AI Search\n",
    "+ Run a series of vector and hybrid queries, including metadata filtering and hybrid (text + vectors) search. \n",
    "\n",
    "The code uses Azure OpenAI to generate embeddings for title and content fields. You'll need access to Azure OpenAI to run this demo.\n",
    "\n",
    "## Create the resources\n",
    "\n",
    "Refer to the `README.md` file in the root folder to create the resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-dotenv in c:\\python312\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in c:\\python312\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\python312\\lib\\site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\python312\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: azure-search-documents in c:\\python312\\lib\\site-packages (11.6.0b3)\n",
      "Requirement already satisfied: azure-core>=1.28.0 in c:\\python312\\lib\\site-packages (from azure-search-documents) (1.30.2)\n",
      "Requirement already satisfied: azure-common>=1.1 in c:\\python312\\lib\\site-packages (from azure-search-documents) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\python312\\lib\\site-packages (from azure-search-documents) (0.6.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\python312\\lib\\site-packages (from azure-core>=1.28.0->azure-search-documents) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\hodellai\\appdata\\roaming\\python\\python312\\site-packages (from azure-core>=1.28.0->azure-search-documents) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\python312\\lib\\site-packages (from azure-core>=1.28.0->azure-search-documents) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-search-documents) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: azure-identity in c:\\users\\hodellai\\appdata\\roaming\\python\\python312\\site-packages (1.15.0)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.23.0 in c:\\python312\\lib\\site-packages (from azure-identity) (1.30.2)\n",
      "Requirement already satisfied: cryptography>=2.5 in c:\\python312\\lib\\site-packages (from azure-identity) (42.0.8)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.24.0 in c:\\python312\\lib\\site-packages (from azure-identity) (1.28.1)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in c:\\python312\\lib\\site-packages (from azure-identity) (1.1.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\python312\\lib\\site-packages (from azure-core<2.0.0,>=1.23.0->azure-identity) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\hodellai\\appdata\\roaming\\python\\python312\\site-packages (from azure-core<2.0.0,>=1.23.0->azure-identity) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\python312\\lib\\site-packages (from azure-core<2.0.0,>=1.23.0->azure-identity) (4.12.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\python312\\lib\\site-packages (from cryptography>=2.5->azure-identity) (1.16.0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\python312\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2.0.0,>=1.24.0->azure-identity) (2.8.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hodellai\\appdata\\roaming\\python\\python312\\site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity) (24.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.6 in c:\\python312\\lib\\site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity) (2.8.2)\n",
      "Requirement already satisfied: pycparser in c:\\python312\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.22)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\hodellai\\appdata\\roaming\\python\\python312\\site-packages (from portalocker<3,>=1.6->msal-extensions<2.0.0,>=0.3.0->azure-identity) (306)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.23.0->azure-identity) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.23.0->azure-identity) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.23.0->azure-identity) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.23.0->azure-identity) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\hodellai\\appdata\\roaming\\python\\python312\\site-packages (1.41.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\python312\\lib\\site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\python312\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\python312\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\hodellai\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\hodellai\\appdata\\roaming\\python\\python312\\site-packages (from openai) (2.9.1)\n",
      "Requirement already satisfied: sniffio in c:\\python312\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\python312\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\python312\\lib\\site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\python312\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in c:\\users\\hodellai\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install tiktoken\n",
    "%pip install azure-search-documents\n",
    "%pip install azure-identity\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the Azure AI Search and OpenAI\n",
    "\n",
    "Load environment variables from the `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-AyK8L6Eks2IdCkqK6tQAsLfpHyYuV\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"I am an AI language model created by OpenAI, known as ChatGPT. I'm here to help answer your questions and provide information on a wide range of topics. How can I assist you today?\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1738940437,\n",
      "  \"model\": \"gpt-4o-2024-08-06\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": \"fp_f3927aa00d\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 40,\n",
      "    \"prompt_tokens\": 21,\n",
      "    \"total_tokens\": 61,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    }\n",
      "  },\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"jailbreak\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "if os.path.exists(\".env\"):\n",
    "    load_dotenv(override=True)\n",
    "    config = dotenv_values(\".env\")\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_openai_chat_completions_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_COMPLETIONS_DEPLOYMENT_NAME\")\n",
    "\n",
    "azure_openai_embedding_model = os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL\")\n",
    "embedding_vector_dimensions = os.getenv(\"EMBEDDING_VECTOR_DIMENSIONS\")\n",
    "\n",
    "azure_search_service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "azure_search_service_admin_key = os.getenv(\"AZURE_SEARCH_SERVICE_ADMIN_KEY\")\n",
    "search_index_name = os.getenv(\"SEARCH_INDEX_NAME\")\n",
    "\n",
    "openai_client = AzureOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_key=azure_openai_api_key,\n",
    "    api_version=\"2024-06-01\"\n",
    ")\n",
    "\n",
    "# Test connection to OpenAI ChatGPT\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model=azure_openai_chat_completions_deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who are you ?\"}\n",
    "    ])\n",
    "print(completion.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of tokens in a text\n",
    "\n",
    "Like LLM models, Embedding models defines a `max input`. It is defined in number of `tokens`. The `max_input` for `text-embedding-3-large` is 8191 tokens. So we need to split the text into chunks of 8191 tokens or less. For that, you need to get the number of tokens in a text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name=\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(string, disallowed_special=()))\n",
    "    return num_tokens\n",
    "\n",
    "# Test the function\n",
    "num_tokens_from_string(\"tiktoken is great!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI embedding model `text-embedding-3-large` has a limit of `8191` tokens per request.\n",
    "Before sending the files to the model, we need to split the text into chunks of less than `8191` tokens.\n",
    "Count the number of tokens in the sample files and show the files with more than `8191` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = './data/azure-ai-docs/'\n",
    "i=0\n",
    "\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.md'):\n",
    "        with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            tokens = num_tokens_from_string(content)\n",
    "            if tokens > 8191:\n",
    "                print(f'File {filename} has {tokens} tokens which is more than 8191 (max) tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming/cleaning the documents\n",
    "\n",
    "Later in this lab, we will proceed with markdown `.md` files. We will need to remove all special characters and markdown syntax from the files. The function `clean_markdown_content()` will help us with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_markdown_content(content):\n",
    "    # Remove links\n",
    "    link_pattern = r'\\[([^\\[]+)\\]\\(([^\\)]+)\\)'\n",
    "    content = re.sub(link_pattern, r'\\1', content)\n",
    "\n",
    "    # Remove images\n",
    "    image_pattern = r'\\!\\[([^\\[]*)\\]\\(([^\\)]+)\\)'\n",
    "    content = re.sub(image_pattern, '', content)\n",
    "\n",
    "    # Remove all occurrences of **\n",
    "    content = content.replace('**', '')\n",
    "    content = content.replace('\\n', '')\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the vector embedding for an input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_vector(text):\n",
    "\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=azure_openai_embedding_model,\n",
    "    )\n",
    "\n",
    "    embedding = response.data[0].embedding\n",
    "\n",
    "    return embedding\n",
    "\n",
    "# Test the function\n",
    "vector = get_embeddings_vector(\"Sample text\")\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create file chunks\n",
    "\n",
    "This is where we split the markdown files in folder `./data/azure-ai-docs` into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "input_directory = './data/azure-ai-docs/'\n",
    "output_directory = './data/chunks/'\n",
    "# create output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "chunk_index=0\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    # Check if the file is a markdown file\n",
    "    if filename.endswith('.md'):\n",
    "        # Open the file\n",
    "        with open(os.path.join(input_directory, filename), 'r', encoding='utf-8') as file:\n",
    "            print(filename)\n",
    "            # Read the file content\n",
    "            content = file.read()\n",
    "            \n",
    "            # break if content doesn't contain title, description, ms.date and '##'\n",
    "            if 'title:' not in content or 'description:' not in content or 'ms.date:' not in content or '##' not in content:\n",
    "                print(f'File {filename} does not contain title, description, ms.date or ##')\n",
    "                continue\n",
    "\n",
    "            # Extract the title, description, and date\n",
    "            page_title = re.search(r'title: (.*)', content).group(1).replace('\"', '')\n",
    "            page_description = re.search(r'description: (.*)', content).group(1)\n",
    "            page_date = re.search(r'ms.date: (.*)', content).group(1)\n",
    "            \n",
    "            # Split the content into chunks based on '##'\n",
    "            chunks = content.split('\\n## ')[1:]  # Skip the first chunk as it contains the title, description, and date\n",
    "            \n",
    "            # Add the chunks to the list along with the title, description, and date\n",
    "            for chunk in chunks:\n",
    "                chunk_index=chunk_index + 1\n",
    "                chunk_content = clean_markdown_content(chunk.strip())\n",
    "                \n",
    "                if (num_tokens_from_string(chunk_content) > 8191):\n",
    "                    print(f'Chunk {chunk_index} in file {filename} has more than 8191 tokens')\n",
    "                    break\n",
    "\n",
    "                vector = get_embeddings_vector(chunk_content)\n",
    "                \n",
    "                chunk = {\n",
    "                    \"id\": str(uuid.uuid4()),\n",
    "                    'page_title': page_title,\n",
    "                    'page_description': page_description,\n",
    "                    'page_date': page_date,\n",
    "                    'chunk_title': chunk.split('\\n')[0],  # The first line after '##' is the title of the chunk\n",
    "                    'chunk_content': chunk_content,  # Remove leading and trailing whitespaces\n",
    "                    'vector': vector\n",
    "                }\n",
    "                \n",
    "                chunk_file_name = f'chunk_{chunk_index}_{page_title}.json'.replace('?', '').replace(':', '').replace(\"'\", '').replace('|', '').replace('/', '').replace('\\\\', '')\n",
    "\n",
    "                # write chunk into JSON file into output directory\n",
    "                with open(f'{output_directory}/{chunk_file_name}', 'w') as f:\n",
    "                    json.dump(chunk, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the length of the embedding vector will be `1536` for `text-embedding-3-small` or `3072` for `text-embedding-3-large`. You can reduce the dimensions of the embedding by passing in the dimensions parameter without the embedding losing its concept-representing properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index in Azure AI Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ComplexField,\n",
    "    CorsOptions,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    ScoringProfile,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticSearch,\n",
    "    SemanticField\n",
    ")\n",
    "\n",
    "credential = AzureKeyCredential(azure_search_service_admin_key)\n",
    "\n",
    "search_index_client = SearchIndexClient(\n",
    "    endpoint=azure_search_service_endpoint, \n",
    "    index_name=search_index_name, \n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# create search index\n",
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        sortable=True,\n",
    "        filterable=True,\n",
    "        facetable=True,\n",
    "    ),\n",
    "    SearchableField(name=\"page_title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"page_description\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"page_date\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"chunk_title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"chunk_content\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=3072, #1536,\n",
    "        vector_search_profile_name=\"myHnswProfile\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\"\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"page_title\"),\n",
    "        # keywords_fields=[SemanticField(field_name=\"category\")],\n",
    "        content_fields=[SemanticField(field_name=\"chunk_content\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "# Create the search index with the semantic settings\n",
    "search_index = SearchIndex(name=search_index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_search=semantic_search)\n",
    "result = search_index_client.create_or_update_index(search_index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you need to delete an index, you can use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete index\n",
    "search_index_client.delete_index(search_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload chunks/documents to Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "search_client = SearchClient(endpoint=azure_search_service_endpoint, index_name=search_index_name, credential=credential)\n",
    "\n",
    "# for each json file in ./data/chunks/ folder, load the json document and upload it to the search index\n",
    "\n",
    "for filename in os.listdir(output_directory):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(output_directory, filename), 'r') as file:\n",
    "            document = json.load(file)\n",
    "\n",
    "            result = search_client.upload_documents(documents=document)\n",
    "            print(f\"Upload of {filename} succeeded: { result[0].succeeded }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a vector similarity search\n",
    "\n",
    "This example shows a pure vector search using the vectorizable text query, all you need to do is pass in text and your vectorizer will handle the query vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "# Pure Vector Search\n",
    "query = \"How to use Azure AI ?\"  \n",
    "\n",
    "embedding = get_embeddings_vector(query)\n",
    "\n",
    "vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=3, fields=\"vector\")\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=None,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"page_title\", \"page_date\", \"chunk_title\", \"chunk_content\"],\n",
    ")  \n",
    "  \n",
    "for result in results:\n",
    "    print(f\"-------------------------------------------\")\n",
    "    print(f\"Page Date: {result['page_date']}\")  \n",
    "    print(f\"Page Title: {result['page_title']}\")  \n",
    "    print(f\"Chunk Title: {result['chunk_title']}\")  \n",
    "    print(f\"Chunk Content: {result['chunk_content']}\")\n",
    "    print(f\"Score: {result['@search.score']}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate a user query\n",
    "\n",
    "This is where we will use the Azure AI Search to search for documents similar to the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai_client.chat.completions.create(\n",
    "    model=azure_openai_chat_completions_deployment_name,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant for an AI learner.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What are the supported llms in Azure ?\"}\n",
    "    ],\n",
    "    extra_body={\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": azure_search_service_endpoint,\n",
    "                    \"index_name\": search_index_name,\n",
    "                    \"authentication\": {\n",
    "                        \"type\": \"api_key\",\n",
    "                        \"key\": azure_search_service_admin_key,\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
